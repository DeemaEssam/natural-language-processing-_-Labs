{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "topic modeling is a technique to find hidden topics in a collection of texts,it helps in simplifying complex text data into meaningful topics.\n",
        "\n"
      ],
      "metadata": {
        "id": "e1gT6jHSKsnc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cz_dKYtdwaH_"
      },
      "source": [
        "## Introduction\n",
        "##### How to get started with topic modeling using LDA in Python\n",
        "** **\n",
        "Topic Models, in a nutshell, are a type of statistical language models used for uncovering hidden structure in a collection of texts. In a practical and more intuitively, you can think of it as a task of:\n",
        "\n",
        "- **Dimensionality Reduction**, where rather than representing a text T in its feature space as {Word_i: count(Word_i, T) for Word_i in Vocabulary}, you can represent it in a topic space as {Topic_i: Weight(Topic_i, T) for Topic_i in Topics}\n",
        "- **Unsupervised Learning**, where it can be compared to clustering, as in the case of clustering, the number of topics, like the number of clusters, is an output parameter. By doing topic modeling, we build clusters of words rather than clusters of texts. A text is thus a mixture of all the topics, each having a specific weight\n",
        "- **Tagging**, abstract “topics” that occur in a collection of documents that best represents the information in them.\n",
        "\n",
        "There are several existing algorithms you can use to perform the topic modeling. The most common of it are, Latent Semantic Analysis (LSA/LSI), Probabilistic Latent Semantic Analysis (pLSA), and Latent Dirichlet Allocation (LDA)\n",
        "\n",
        "In this tutorial, we’ll take a closer look at LDA, and implement our first topic model using the sklearn implementation in python 2.7\n",
        "\n",
        "### Theoretical Overview\n",
        "LDA is a generative probabilistic model that assumes each topic is a mixture over an underlying set of words, and each document is a mixture of over a set of topic probabilities.\n",
        "\n",
        "![LDA_Model](https://github.com/chdoig/pytexas2015-topic-modeling/blob/master/images/lda-4.png?raw=true)\n",
        "\n",
        "We can describe the generative process of LDA as, given the M number of documents, N number of words, and prior K number of topics, the model trains to output:\n",
        "\n",
        "- `psi`, the distribution of words for each topic K\n",
        "- `phi`, the distribution of topics for each document i\n",
        "\n",
        "#### Parameters of LDA\n",
        "\n",
        "- `Alpha parameter` is Dirichlet prior concentration parameter that represents document-topic density — with a higher alpha, documents are assumed to be made up of more topics and result in more specific topic distribution per document.\n",
        "- `Beta parameter` is the same prior concentration parameter that represents topic-word density — with high beta, topics are assumed to made of up most of the words and result in a more specific word distribution per topic.\n",
        "\n",
        "**To read more: https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jelnCLDtwaIA"
      },
      "source": [
        "** **\n",
        "### LDA Implementation\n",
        "\n",
        "1. [Loading data](#load_data)\n",
        "2. [Data cleaning](#clean_data)\n",
        "3. [Exploratory analysis](#eda)\n",
        "4. [Prepare data for LDA analysis](#data_preparation)\n",
        "5. [LDA model training](#train_model)\n",
        "6. [Analyzing LDA model results](#results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4gbKaO2waIB"
      },
      "source": [
        "** **\n",
        "For this tutorial, we’ll use the dataset of papers published in NeurIPS (NIPS) conference which is one of the most prestigious yearly events in the machine learning community. The CSV data file contains information on the different NeurIPS papers that were published from 1987 until 2016 (29 years!). These papers discuss a wide variety of topics in machine learning, from neural networks to optimization methods, and many more.\n",
        "\n",
        "<img src=\"https://s3.amazonaws.com/assets.datacamp.com/production/project_158/img/nips_logo.png\" alt=\"The logo of NIPS (Neural Information Processing Systems)\">\n",
        "\n",
        "Let’s start by looking at the content of the file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5E1vZuawaIB"
      },
      "source": [
        "** **\n",
        "#### Step 1: Loading Data <a class=\"anchor\\\" id=\"load_data\"></a>\n",
        "** **\n",
        "For this tutorial, we’ll use the dataset of papers published in NeurIPS (NIPS) conference which is one of the most prestigious yearly events in the machine learning community. The CSV data file contains information on the different NeurIPS papers that were published from 1987 until 2016 (29 years!). These papers discuss a wide variety of topics in machine learning, from neural networks to optimization methods, and many more.\n",
        "\n",
        "Let’s start by looking at the content of the file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "wIxmuphywaIB",
        "outputId": "951054b6-424f-481c-97c0-af914b0fac8f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     id  year                                              title event_type  \\\n",
              "0     1  1987  Self-Organization of Associative Database and ...        NaN   \n",
              "1    10  1987  A Mean Field Theory of Layer IV of Visual Cort...        NaN   \n",
              "2   100  1988  Storing Covariance by the Associative Long-Ter...        NaN   \n",
              "3  1000  1994  Bayesian Query Construction for Neural Network...        NaN   \n",
              "4  1001  1994  Neural Network Ensembles, Cross Validation, an...        NaN   \n",
              "\n",
              "                                            pdf_name          abstract  \\\n",
              "0  1-self-organization-of-associative-database-an...  Abstract Missing   \n",
              "1  10-a-mean-field-theory-of-layer-iv-of-visual-c...  Abstract Missing   \n",
              "2  100-storing-covariance-by-the-associative-long...  Abstract Missing   \n",
              "3  1000-bayesian-query-construction-for-neural-ne...  Abstract Missing   \n",
              "4  1001-neural-network-ensembles-cross-validation...  Abstract Missing   \n",
              "\n",
              "                                          paper_text  \n",
              "0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
              "1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
              "2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
              "3  Bayesian Query Construction for Neural\\nNetwor...  \n",
              "4  Neural Network Ensembles, Cross\\nValidation, a...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9116f726-9c6e-4d9f-9814-3bc749949d39\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>year</th>\n",
              "      <th>title</th>\n",
              "      <th>event_type</th>\n",
              "      <th>pdf_name</th>\n",
              "      <th>abstract</th>\n",
              "      <th>paper_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1987</td>\n",
              "      <td>Self-Organization of Associative Database and ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1-self-organization-of-associative-database-an...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>1987</td>\n",
              "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100</td>\n",
              "      <td>1988</td>\n",
              "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100-storing-covariance-by-the-associative-long...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000</td>\n",
              "      <td>1994</td>\n",
              "      <td>Bayesian Query Construction for Neural Network...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1001</td>\n",
              "      <td>1994</td>\n",
              "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1001-neural-network-ensembles-cross-validation...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9116f726-9c6e-4d9f-9814-3bc749949d39')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9116f726-9c6e-4d9f-9814-3bc749949d39 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9116f726-9c6e-4d9f-9814-3bc749949d39');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-25d8f9c9-c410-4767-b733-c4ad6213ba73\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-25d8f9c9-c410-4767-b733-c4ad6213ba73')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-25d8f9c9-c410-4767-b733-c4ad6213ba73 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "papers",
              "summary": "{\n  \"name\": \"papers\",\n  \"rows\": 6560,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1901,\n        \"min\": 1,\n        \"max\": 6603,\n        \"num_unique_values\": 6560,\n        \"samples\": [\n          3087,\n          78,\n          5412\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 1987,\n        \"max\": 2016,\n        \"num_unique_values\": 30,\n        \"samples\": [\n          1992,\n          1990,\n          2012\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6560,\n        \"samples\": [\n          \"Natural Actor-Critic for Road Traffic Optimisation\",\n          \"Learning Representations by Recirculation\",\n          \"Quantized Kernel Learning for Feature Matching\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"event_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Oral\",\n          \"Spotlight\",\n          \"Poster\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pdf_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6560,\n        \"samples\": [\n          \"3087-natural-actor-critic-for-road-traffic-optimisation.pdf\",\n          \"78-learning-representations-by-recirculation.pdf\",\n          \"5412-quantized-kernel-learning-for-feature-matching.pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3244,\n        \"samples\": [\n          \"Tensor CANDECOMP/PARAFAC (CP) decomposition has wide applications in statistical learning of latent variable models and in data mining. In this paper, we propose fast and randomized tensor CP decomposition algorithms based on sketching. We build on the idea of count sketches, but introduce many novel ideas which are unique to tensors. We develop novel methods for randomized com- putation of tensor contractions via FFTs, without explicitly forming the tensors. Such tensor contractions are encountered in decomposition methods such as ten- sor power iterations and alternating least squares. We also design novel colliding hashes for symmetric tensors to further save time in computing the sketches. We then combine these sketching ideas with existing whitening and tensor power iter- ative techniques to obtain the fastest algorithm on both sparse and dense tensors. The quality of approximation under our method does not depend on properties such as sparsity, uniformity of elements, etc. We apply the method for topic mod- eling and obtain competitive results.\",\n          \"Many spectral unmixing methods rely on the non-negative decomposition of spectral data onto a dictionary of spectral templates. In particular, state-of-the-art music transcription systems decompose the spectrogram of the input signal onto a dictionary of representative note spectra. The typical measures of fit used to quantify the adequacy of the decomposition compare the data and template entries frequency-wise. As such, small displacements of energy from a frequency bin to another as well as variations of timber can disproportionally harm the fit. We address these issues by means of optimal transportation and propose a new measure of fit that treats the frequency distributions of energy holistically as opposed to frequency-wise. Building on the harmonic nature of sound, the new measure is invariant to shifts of energy to harmonically-related frequencies, as well as to small and local displacements of energy. Equipped with this new measure of fit, the dictionary of note templates can be considerably simplified to a set of Dirac vectors located at the target fundamental frequencies (musical pitch values). This in turns gives ground to a very fast and simple decomposition algorithm that achieves state-of-the-art performance on real musical data.\",\n          \"The problem of  multiclass boosting is considered. A new framework,based on multi-dimensional codewords and predictors is introduced. The optimal set of codewords is derived, and a margin enforcing loss proposed. The resulting risk is minimized by gradient descent on a multidimensional functional space. Two algorithms are proposed: 1) CD-MCBoost, based on coordinate descent, updates one predictor component at a time, 2) GD-MCBoost, based on gradient descent, updates all components jointly. The algorithms differ in the weak learners that they support but are both shown to be 1) Bayes consistent, 2) margin enforcing, and 3) convergent to the global minimum of the risk. They also reduce to AdaBoost when there are only two classes. Experiments show that both methods outperform previous multiclass boosting approaches on a number of datasets.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"paper_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6553,\n        \"samples\": [\n          \"550\\n\\nAckley and Littman\\n\\nGeneralization and scaling in reinforcement\\nlearning\\nDavid H. Ackley\\nMichael L. Littman\\nCognitive Science Research Group\\nBellcore\\nMorristown, NJ 07960\\n\\nABSTRACT\\nIn associative reinforcement learning, an environment generates input\\nvectors, a learning system generates possible output vectors, and a reinforcement function computes feedback signals from the input-output\\npairs. The task is to discover and remember input-output pairs that\\ngenerate rewards. Especially difficult cases occur when rewards are\\nrare, since the expected time for any algorithm can grow exponentially\\nwith the size of the problem. Nonetheless, if a reinforcement function\\npossesses regularities, and a learning algorithm exploits them, learning\\ntime can be reduced below that of non-generalizing algorithms. This\\npaper describes a neural network algorithm called complementary reinforcement back-propagation (CRBP), and reports simulation results\\non problems designed to offer differing opportunities for generalization.\\n\\n1\\n\\nREINFORCEMENT LEARNING REQUIRES SEARCH\\n\\nReinforcement learning (Sutton, 1984; Barto & Anandan, 1985; Ackley, 1988; Allen,\\n1989) requires more from a learner than does the more familiar supervised learning\\nparadigm. Supervised learning supplies the correct answers to the learner, whereas\\nreinforcement learning requires the learner to discover the correct outputs before\\nthey can be stored. The reinforcement paradigm divides neatly into search and\\nlearning aspects: When rewarded the system makes internal adjustments to learn\\nthe discovered input-output pair; when punished the system makes internal adjustments to search elsewhere.\\n\\n\\fGeneralization and Scaling in Reinforcement Learning\\n1.1\\n\\nMAKING REINFORCEMENT INTO ERROR\\n\\nFollowing work by Anderson (1986) and Williams (1988), we extend the backpropagation algorithm to associative reinforcement learning. Start with a \\\"garden variety\\\" backpropagation network: A vector i of n binary input units propagates\\nthrough zero or more layers of hidden units, ultimately reaching a vector 8 of m\\nsigmoid units, each taking continuous values in the range (0,1). Interpret each 8j\\nas the probability that an associated random bit OJ takes on value 1. Let us call\\nthe continuous, deterministic vector 8 the search vector to distinguish it from the\\nstochastic binary output vector o.\\nGiven an input vector, we forward propagate to produce a search vector 8, and\\nthen perform m independent Bernoulli trials to produce an output vector o. The\\ni - 0 pair is evaluated by the reinforcement function and reward or punishment\\nensues. Suppose reward occurs. We therefore want to make 0 more likely given i.\\nBackpropagation will do just that if we take 0 as the desired target to produce an\\nerror vector (0 - 8) and adjust weights normally.\\nNow suppose punishment occurs, indicating 0 does not correspond with i. By choice\\nof error vector, backpropagation allows us to push the search vector in any direction;\\nwhich way should we go? In absence of problem-specific information, we cannot pick\\nan appropriate direction with certainty. Any decision will involve assumptions. A\\nvery minimal \\\"don't be like 0\\\" assumption-employed in Anderson (1986), Williams\\n(1988), and Ackley (1989)-pushes s directly away from 0 by taking (8 - 0) as the\\nerror vector. A slightly stronger \\\"be like not-o\\\" assumption-employed in Barto &\\nAnandan (1985) and Ackley (1987)-pushes s directly toward the complement of 0\\nby taking ((1 - 0) - 8) as the error vector. Although the two approaches always\\nagree on the signs of the error terms, they differ in magnitudes. In this work,\\nwe explore the second possibility, embodied in an algorithm called complementary\\nreinforcement back-propagation ( CRBP).\\nFigure 1 summarizes the CRBP algorithm. The algorithm in the figure reflects three\\nmodifications to the basic approach just sketched. First, in step 2, instead of using\\nthe 8j'S directly as probabilities, we found it advantageous to \\\"stretch\\\" the values\\nusing a parameter v. When v < 1, it is not necessary for the 8i'S to reach zero or\\none to produce a deterministic output. Second, in step 6, we found it important\\nto use a smaller learning rate for punishment compared to reward. Third, consider\\nstep 7: Another forward propagation is performed, another stochastic binary output vector 0* is generated (using the procedure from step 2), and 0* is compared\\nto o. If they are identical and punishment occurred, or if they are different and\\nreward occurred, then another error vector is generated and another weight update\\nis performed. This loop continues until a different output is generated (in the case\\nof failure) or until the original output is regenerated (in the case of success). This\\nmodification improved performance significantly, and added only a small percentage\\nto the total number of weight updates performed.\\n\\n551\\n\\n\\f552\\n\\nAckley and Littman\\n\\nO. Build a back propagation network with input dimensionality n and output\\ndimensionality m. Let t = 0 and te = O.\\n1. Pick random i E 2n and forward propagate to produce a/s.\\n2. Generate a binary output vector o. Given a uniform random variable ~ E [0,1]\\nand parameter 0 < v < 1,\\nOJ\\n\\n=\\n\\n{1,\\n\\n0,\\n\\nif(sj - !)/v+! ~ ~j\\notherwise.\\n\\n3. Compute reinforcement r = f(i,o). Increment t. If r < 0, let te = t.\\n4. Generate output errors ej. If r > 0, let tj = OJ, otherwise let tj = 1- OJ. Let\\nej = (tj - sj)sj(l- Sj).\\n5. Backpropagate errors.\\n6. Update weights. 1:::..Wjk = 1]ekSj, using 1] = 1]+ if r ~ 0, and 1] = 1]- otherwise,\\nwith parameters 1]+,1]- > o.\\n7. Forward propagate again to produce new Sj's. Generate temporary output\\nvector 0*. If (r > 0 and 0* #- 0) or (r < 0 and 0* = 0), go to 4.\\n8. If te ~ t, exit returning te, else go to 1.\\n\\nFigure 1: Complementary Reinforcement Back Propagation-CRBP\\n\\n2\\n\\nON-LINE GENERALIZATION\\n\\nWhen there are many possible outputs and correct pairings are rare, the computational cost associated with the search for the correct answers can be profound.\\nThe search for correct pairings will be accelerated if the search strategy can effectively generalize the reinforcement received on one input to others. The speed of\\nan algorithm on a given problem relative to non-generalizing algorithms provides a\\nmeasure of generalization that we call on-line generalization.\\nO. Let z be an array of length 2n. Set the z[i] to random numbers from 0 to\\n2m - 1. Let t = te = O.\\n1. Pick a random input i E 2n.\\n2. Compute reinforcement r = f(i, z[i]). Increment t.\\n3. If r < 0 let z[i] = (z[i] + 1) mod 2m , and let te = t.\\n4. If te <t:: t exit returning t e, else go to 1.\\n\\nFigure 2: The Table Lookup Reference Algorithm Tref(f, n, m)\\nConsider the table-lookup algorithm Tref(f, n, m) summarized in Figure 2. In this\\nalgorithm, a separate storage location is used for each possible input. This prevents\\nthe memorization of one i - 0 pair from interfering with any other. Similarly,\\nthe selection of a candidate output vector depends only on the slot of the table\\ncorresponding to the given input. The learning speed of T ref depends only on the\\ninput and output dimensionalities and the number of correct outputs associated\\n\\n\\fGeneralization and Scaling in Reinforcement Learning\\n\\nwith each input. When a problem possesses n input bits and n output bits, and\\nthere is only one correct output vector for each input vector, Tre{ runs in about 4n\\ntime (counting each input-output judgment as one.) In such cases one expects to\\ntake at least 2n - 1 just to find one correct i - 0 pair, so exponential time cannot be\\navoided without a priori information. How does a generalizing algorithm such as\\nCRBP compare to Trer?\\n\\n3\\n\\nSIMULATIONS ON SCALABLE PROBLEMS\\n\\nWe have tested CRBP on several simple problems designed to offer varying degrees\\nand types of generalization. In all of the simulations in this section, the following\\ndetails apply: Input and output bit counts are equal (n). Parameters are dependent\\non n but independent of the reinforcement function f. '7+ is hand-picked for each\\nn,l 11- = 11+/10 and II = 0.5. All data points are medians of five runs. The stopping\\ncriterion te ~ t is interpreted as te +max(2000, 2n+l) < t. The fit lines in the figures\\nare least squares solutions to a x bn , to two significant digits.\\nAs a notational convenience, let c = ~\\n\\n3.1\\n\\nn\\n\\nE ij\\n\\n;=1\\n\\n-\\n\\nthe fraction of ones in the input.\\n\\nn-MAJORlTY\\n\\nConsider this \\\"majority rules\\\" problem: [if c > ~ then 0 = In else 0 = on]. The i-o\\nmapping is many-to-l. This problem provides an opportunity for what Anderson\\n(1986) called \\\"output generalization\\\": since there are only two correct output states,\\nevery pair of output bits are completely correlated in the cases when reward occurs.\\n\\nG)\\n\\n'iii\\nu\\nrn\\n\\nC)\\n\\n0\\n\\n::::.\\nG)\\n\\nE\\n\\n;\\n\\n10 7\\n10 6\\n10 5\\n10 4\\n\\nx\\n\\nTable\\n\\nD\\n\\nCRBP n-n-n\\n\\n+ CRBP n-n\\n\\n10 3\\n10 2\\n10 1\\n10 0\\n0\\n\\n1\\n\\n2\\n\\n3\\n\\n456\\n\\n78\\n\\n91011121314\\n\\nn\\nFigure 3: The n-majority problem\\n\\nFigure 3 displays the simulation results. Note that although Trer is faster than\\nCRBP at small values of n, CRBP's slower growth rate (1.6n vs 4.2n ) allows it to\\ncross over and begin outperforming Trer at about 6 bits. Note also--in violation of\\n1 For n = 1 to 12. we used '1+\\n0.219. 0.170. 0.121}.\\n\\n= {2.000. 1.550. 1.130.0.979.0.783.0.709.0.623.0.525.0.280.\\n\\n553\\n\\n\\f554\\n\\nAckley and Littman\\n\\nsome conventional wisdom-that although n-majority is a linearly separable problem, the performance of CRBP with hidden units is better than without. Hidden\\nunits can be helpful--even on linearly separable problems-when there are opportunities for output generalization.\\n\\n3.2\\n\\nn-COPY AND THE 2k -ATTRACTORS FAMILY\\n\\nAs a second example, consider the n-copy problem: [0 = i]. The i-o mapping is now\\n1-1, and the values of output bits in rewarding states are completely uncorrelated,\\nbut the value of each output bit is completely correlated with the value of the\\ncorresponding input bit. Figure 4 displays the simulation results. Once again, at\\n\\nG)\\n\\n'ii\\n\\ntA\\nQ\\n0\\n\\n::::.\\nG)\\n\\n-\\n\\n.5\\n\\n10 7\\n10 6\\n10 5\\n10 4\\n\\nx\\n150*2.0I\\\\n\\n\\nD\\n\\n10 3\\n10 2\\n\\n12*2.2I\\\\n\\n\\n+\\n\\nTable\\nCRBP n-n-n\\nCRBP n-n\\n\\n10 1\\n10 0\\n0\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\n5\\n\\n6\\n\\n7\\n\\n8\\n\\n9\\n\\n10 1112\\n\\nn\\nFigure 4: The n-copy problem\\nlow values of n, Trer is faster, but CRBP rapidly overtakes Trer as n increases. In\\nn-copy, unlike n-majority, CRBP performs better without hidden units.\\nThe n-majority and n-copy problems are extreme cases of a spectrum. n-majority\\ncan be viewed as a \\\"2-attractors\\\" problem in that there are only two correct\\noutputs-all zeros and all ones-and the correct output is the one that i is closer\\nto in hamming distance. By dividing the input and output bits into two groups\\nand performing the majority function independently on each group, one generates\\na \\\"4-aUractors\\\" problem. In general, by dividing the input and output bits into\\n1 ~ Ie ~ n groups, one generates a \\\"2i:-attractors\\\" problem. When Ie = 1, nmajority results, and when Ie n, n-copy results.\\n\\n=\\n\\nFigure 5 displays simulation results on the n = 8-bit problems generated when Ie is\\nvaried from 1 to n. The advantage of hidden units for low values of Ie is evident,\\nas is the advantage of \\\"shortcut connections\\\" (direct input-to-output weights) for\\nlarger values of Ie. Note also that combination of both hidden units and shortcut\\nconnections performs better than either alone.\\n\\n\\fGeneralization and Scaling in Reinforcement Learning\\n\\n105~--------------------------------~\\n\\nCASP 8-10-8\\n-+- CASP 8-8\\n.... CASP 8-10-Sls\\n-0-\\n\\n... Table\\n\\n3\\n\\n2\\n\\n1\\n\\n5\\n\\n4\\n\\n7\\n\\n6\\n\\n8\\n\\nk\\n\\nFigure 5: The 21:- attractors family at n = 8\\n\\n3.3\\n\\nn-EXCLUDED MIDDLE\\n\\nAll of the functions considered so far have been linearly separable. Consider this\\n\\\"folded majority\\\" function: [if\\n< c < then 0 on else 0 In]. Now, like\\nn-majority, there are only two rewarding output states, but the determination of\\nwhich output state is correct is not linearly separable in the input space. When\\nn = 2, the n-excluded middle problem yields the EQV (i.e., the complement of\\nXOR) function, but whereas functions such as n-parity [if nc is even then 0\\non\\nelse 0 = In] get more non-linear with increasing n, n-excluded middle does not.\\n\\ni\\n\\ni\\n\\n=\\n\\n=\\n\\n=\\n\\n107~------------------------------~~\\n\\n-\\n\\n10 6\\n10 5\\n\\nD)\\n\\n10 4\\n10 3\\n\\nI)\\n\\n'ii\\nu\\nf)\\n\\n.2\\n\\nI)\\n\\nE\\n\\n:::\\n\\nx\\nc\\n\\n17oo*1.6\\\"n\\n\\nTable\\n\\nCRSP n-n-n/s\\n\\n10 2\\n10 1\\n10 0\\n0\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\n5\\n\\n6\\n\\n7\\n\\n8\\n\\n9\\n\\n10 1112\\n\\nn\\nFigure 6: The n-excluded middle problem\\nFigure 6 displays the simulation results. CRBP is slowed somewhat compared to\\nthe linearly separable problems, yielding a higher \\\"cross over point\\\" of about 8 bits.\\n\\n555\\n\\n\\f556\\n\\nAckley and Littman\\n\\n4\\n\\nSTRUCTURING DEGENERATE OUTPUT SPACES\\n\\nAll of the scaling problems in the previous section are designed so that there is\\na single correct output for each possible input. This allows for difficult problems\\neven at small sizes, but it rules out an important aspect of generalizing algorithms\\nfor associative reinforcement learning: If there are multiple satisfactory outputs\\nfor given inputs, a generalizing algorithm may impose structure on the mapping it\\nproduces.\\nWe have two demonstrations of this effect, \\\"Bit Count\\\" and \\\"Inverse Arithmetic.\\\"\\nThe Bit Count problem simply states that the number of I-bits in the output should\\nequal the number of I-bits in the input. When n = 9, Tref rapidly finds solutions\\ninvolving hundreds of different output patterns. CRBP is slower--especially with\\nrelatively few hidden units-but it regularly finds solutions involving just 10 output\\npatterns that form a sequence from 09 to 19 with one bit changing per step.\\n0+Ox4=0\\n1+0x4=1\\n2+0x4=2\\n3+0x4=3\\n\\n0+2x4=8\\n1+2x4=9\\n2 + 2 x 4 = 10\\n3+2x4=11\\n\\n4+0x4=4 4+ 2 x 4 =\\n5+0x4=5 5 + 2 x 4 =\\n6+0x4=6 6 + 2 x 4 =\\n7+0x4=7 7 + 2 x 4 =\\n\\n12\\n13\\n14\\n15\\n\\n2+2-4=0 2+2+4=8\\n3+2-4=1 3+2+4=9\\n2+2+4=2 2 + 2 x 4 = 10\\n3+2+4=3 3+2x4=1l\\n6+2-4=4\\n7+2-4=5\\n6+2+4=6\\n7+2-.;-4=7\\n\\n6+\\n7+\\n6+\\n7+\\n\\n2+ 4 =\\n2+ 4 =\\n2x4=\\n2x4=\\n\\n0+4 x 4 = 16 0+6 x 4 =\\n1+4x4=17 1 + 6 x 4 =\\n2 + 4 x 4 = 18 2 + 6 x 4 =\\n3 +4 x 4 = 19 3 + 6 x 4 =\\n\\n24\\n25\\n26\\n27\\n\\n4+4\\n5+ 4\\n6+ 4\\n7+ 4\\n\\n=\\n=\\n=\\n=\\n\\n28\\n29\\n30\\n31\\n24\\n25\\n26\\n27\\n\\nx\\nx\\nx\\nx\\n\\n4=\\n4=\\n4=\\n4=\\n\\n6+ 6 + 4 =\\n7+6+4=\\n2+ 4 x 4 =\\n3+ 4 x 4=\\n\\n12 4 x 4 +\\n13 5 + 4 x\\n14 6 + 4 x\\n15 7 +4 x\\n\\n4=\\n4=\\n4\\n4=\\n\\n=\\n\\n20 4 + 6 x\\n21 5 + 6 x\\n22 6 + 6 x\\n23 7 + 6 x\\n\\n4\\n4\\n4\\n4\\n\\n16\\n17\\n18\\n19\\n\\n0+6 x\\n1+ 6 x\\n2+ 6x\\n3+ 6x\\n\\n4=\\n4=\\n4=\\n4=\\n\\n20\\n21\\n22\\n23\\n\\n4+\\n5+\\n6+\\n7+\\n\\n4 = 28\\n4 = 29\\n4 30\\n4 = 31\\n\\n6\\n6\\n6\\n6\\n\\nx\\nx\\nx\\nx\\n\\n=\\n\\nFigure 7: Sample CRBP solutions to Inverse Arithmetic\\n\\nThe Inverse Arithmetic problem can be summarized as follows: Given i E 25 , find\\n:1:, y, z E 23 and 0, <> E {+(OO)' -(01)' X (10)' +(11)} such that :I: oy<>z = i. In all there are\\n13 bits of output, interpreted as three 3-bit binary numbers and two 2-bit operators,\\nand the task is to pick an output that evaluates to the given 5-bit binary input\\nunder the usual rules: operator precedence, left-right evaluation, integer division,\\nand division by zero fails.\\nAs shown in Figure 7, CRBP sometimes solves this problem essentially by discovering positional notation, and sometimes produces less-globally structured solutions,\\nparticularly as outputs for lower-valued i's, which have a wider range of solutions.\\n\\n\\fGeneralization and Scaling in Reinforcement Learning\\n\\n5\\n\\nCONCLUSIONS\\n\\nSome basic concepts of supervised learning appear in different guises when the\\nparadigm of reinforcement learning is applied to large output spaces. Rather than\\na \\\"learning phase\\\" followed by a \\\"generalization test,\\\" in reinforcement learning\\nthe search problem is a generalization test, performed simultaneously with learning.\\nInformation is put to work as soon as it is acquired.\\nThe problem of of \\\"overfitting\\\" or \\\"learning the noise\\\" seems to be less of an issue,\\nsince learning stops automatically when consistent success is reached. In experiments not reported here we gradually increased the number of hidden units on\\nthe 8-bit copy problem from 8 to 25 without observing the performance decline\\nassociated with \\\"too many free parameters.\\\"\\nThe 2 k -attractors (and 2 k -folds-generalizing Excluded Middle) families provide\\na starter set of sample problems with easily understood and distinctly different\\nextreme cases.\\nIn degenerate output spaces, generalization decisions can be seen directly in the\\ndiscovered mapping. Network analysis is not required to \\\"see how the net does it.\\\"\\nThe possibility of ultimately generating useful new knowledge via reinforcement\\nlearning algorithms cannot be ruled out.\\nReferences\\nAckley, D.H. (1987) A connectionist machine for genetic hillclimbing. Boston, MA: Kluwer\\nAcademic Press.\\nAckley, D.H. (1989) Associative learning via inhibitory search. In D.S. Touretzky (ed.),\\nAdvances in Neural Information Processing Systems 1, 20-28. San Mateo, CA: Morgan\\nKaufmann.\\nAllen, R.B. (1989) Developing agent models with a neural reinforcement technique. IEEE\\nSystems, Man, and Cybernetics Conference. Cambridge, MA.\\nAnderson, C.W. (1986) Learning and problem solving with multilayer connectionist systems. University of Mass. Ph.D. dissertation. COINS TR 86-50. Amherst, MA.\\nBarto, A.G. (1985) Learning by statistical cooperation of self-interested neuron-like computing elements. Human Neurobiology, 4:229-256.\\nBarto, A.G., & Anandan, P. (1985) Pattern recognizing stochastic learning automata.\\nIEEE Transactions on Systems, Man, and Cybernetics, 15, 360-374.\\nRumelhart, D.E., Hinton, G.E., & Williams, R.J. (1986) Learning representations by backpropagating errors. Nature, 323, 533-536.\\nSutton, R.S. (1984) Temporal credit assignment in reinforcement learning. University of\\nMass. Ph.D. dissertation. COINS TR 84-2. Amherst, MA.\\nWilliams, R.J. (1988) Toward a theory of reinforcement-learning connectionist systems.\\nCollege of Computer Science of Northeastern University Technical Report NU-CCS-88-3.\\nBoston, MA.\\n\\n557\\n\\n\\f\",\n          \"Dynamics of Supervised Learning with\\nRestricted Training Sets and Noisy Teachers\\n\\nA.C.C. Coolen\\nDept of Mathematics\\nKing's College London\\nThe Strand, London WC2R 2LS, UK\\ntcoolen@mth.kc1.ac.uk\\n\\nC.W.H.Mace\\nDept of Mathematics\\nKing's College London\\nThe Strand, London WC2R 2LS, UK\\ncmace@mth.kc1.ac.uk\\n\\nAbstract\\nWe generalize a recent formalism to describe the dynamics of supervised\\nlearning in layered neural networks, in the regime where data recycling\\nis inevitable, to the case of noisy teachers. Our theory generates reliable\\npredictions for the evolution in time of training- and generalization errors, and extends the class of mathematically solvable learning processes\\nin large neural networks to those situations where overfitting can occur.\\n\\n1 Introduction\\nTools from statistical mechanics have been used successfully over the last decade to study\\nthe dynamics of learning in layered neural networks (for reviews see e.g. [1] or [2]). The\\nsimplest theories result upon assuming the data set to be much larger than the number\\nof weight updates made, which rules out recycling and ensures that any distribution of\\nrelevance will be Gaussian. Unfortunately, both in terms of applications and in terms of\\nmathematical interest, this regime is not the most relevant one. Most complications and\\npeculiarities in the dynamics of learning arise precisely due to data recycling, which creates\\nfor the system the possibility to improve performance by memorizing answers rather than\\nby learning an underlying rule. The dynamics of learning with restricted training sets was\\nfirst studied analytically in [3] (linear learning rules) and [4] (systems with binary weights).\\nThe latter studies were ahead of their time, and did not get the attention they deserved just\\nbecause at that stage even the simpler learning dynamics without data recycling had not\\nyet been studied. More recently attention has moved back to the dynamics of learning\\nin the recycling regime. Some studies aimed at developing a general theory [5, 6, 7],\\nsome at finding exact solutions for special cases [8]. All general theories published so far\\nhave in common that they as yet considered realizable scenario's: the rule to be learned\\nwas implementable by the student, and overfitting could not yet occur. The next hurdle is\\nthat where restricted training sets are combined with unrealizable rules. Again some have\\nturned to non-typical but solvable cases, involving Hebbian rules and noisy [9] or 'reverse\\nwedge' teachers [10]. More recently the cavity method has been used to build a general\\ntheory [11] (as yet for batch learning only). In this paper we generalize the general theory\\nlaunched in [6,5,7], which applies to arbitrary learning rules, to the case of noisy teachers.\\nWe will mirror closely the presentation in [6] (dealing with the simpler case of noise-free\\nteachers), and we refer to [5, 7] for background reading on the ideas behind the formalism.\\n\\n\\fA. C. C. Coolen and C. W. H. Mace\\n\\n238\\n\\n2 Definitions\\nAs in [6, 5] we restrict ourselves for simplicity to perceptrons. A student perceptron operates a linear separation, parametrised by a weight vector J E iRN :\\nS:{-I,I}N -t{-I,I}\\n\\nS(e) = sgn[J?e]\\n\\nIt aims to emulate a teacher o~erating a similar rule, which, however, is characterized by a\\nvariable weight vector BE iR ,drawn at random from a distribution P(B) such as\\nP(B) = >'6[B+B*]\\n\\noutput noise:\\n\\n+ (1->')6[B-B*]\\n\\n(1)\\n\\nP(B) = [~~/NrN e- tN (B-B')2/E2\\n(2)\\nThe parameters>. and ~ control the amount of teacher noise, with the noise-free teacher\\nB = B* recovered in the limits>. -t 0 and ~ -t O. The student modifies J iteratively, using\\nexamples of input vectors which are drawn at random from a fixed (randomly composed)\\nE {-I, I}N with a> 0, and the corresponding\\ntraining set containing p = aN vectors\\nvalues of the teacher outputs. We choose the teacher noise to be consistent, i.e. the answer\\nwill remain the same when that particular question\\ngiven by the teacher to a question\\nre-appears during the learning process. Thus T(e?) = sgn[BJL . e], with p teacher weight\\nvectors BJL, drawn randomly and independently from P(B), and we generalize the training\\nl , B l ), . .. , (e, BP)}. Consistency of teacher noise is natural\\nset accordingly to jj =\\nin terms of applications, and a prerequisite for overfitting phenomena. Averages over the\\ntraining set will be denoted as ( ... ) b; averages over all possible input vectors E {-I, I}N\\nas ( ... )e. We analyze two classes of learning rules, of the form J (? + 1) = J (?) + f).J (?):\\n\\nGaussian weight noise:\\n\\ne\\n\\ne\\n\\ne\\n\\nHe\\n\\ne\\n\\n= 11 {e(?) 9 [J(?)?e(?), B(?)?e(?)] - ,J(?) }\\nf).J(?) = 11 {(e 9 [J(?)?e, B?eDl> - ,J(m) }\\n\\non-line:\\n\\nf).J(?)\\n\\nbatch :\\n\\n(3)\\n\\nIn on-line learning one draws at each step ? a question/answer pair (e (?), B (?)) at random from the training set. In batch learning one iterates a deterministic map which is an\\naverage over all data in the training set. Our performance measures are the training- and\\ngeneralization errors, defined as follows (with the step function O[x > 0] = 1, O[x < 0] = 0):\\nEt(J)\\n\\n= (O[-(J ?e)(B ?em b\\n\\nEg(J)\\n\\n= (O[-(J ?e)(B* ?e)])e\\n\\n(4)\\n\\nWe introduce macroscopic observables, taylored to the present problem, generalizing [5, 6]:\\nQ[J]=J 2,\\nR[J]=J?B*,\\nP[x,y,z;J]=(6[x-J?e]6[y-B*?e]6[z-B?eDl> (5)\\nAs in [5, 6] we eliminate technical subtleties by assuming the number of arguments (x, y, z)\\nfor which P[x, y, z; J] is evaluated to go to infinity after the limit N -t 00 has been taken.\\n\\n3 Derivation of Macroscopic Laws\\nUpon generalizing the calculations in [6, 5], one finds for on-line learning:\\n\\n!\\n!\\n\\nQ = 2'f} !dXdydZ P[x, y, z] xg[x, z] - 2'f},Q + 'f}2!dXdYdZ P[x, y, z] g2[x, z]\\n\\n(6)\\n\\nR = 'f} !dXdydZ P[x, y, z] y9[x, z]- 'f},R\\n\\n(7)\\n\\n:t\\n\\nP[x, y, z] =\\n\\n~\\n\\n!\\n\\ndx' P[x', y, z] {6[x-x' -'f}G[x', z]] -6[x-x']}\\n\\n-'f}! / dx'dy'dz' / dx'dy'dz'9[x', z]A[x, y, z; x',y', z']\\n\\n1\\n+'i'f}2\\n\\n!\\n\\n+ 'f}, :x\\n\\nEP2P[x, y, z]\\ndx'dy'dz' P[x', y', z']92[x', z'] 8x\\n\\n{xP[x , y, z]}\\n\\n(8)\\n\\n\\fSupervised Learning with Restricted Training Sets\\n\\n239\\n\\nThe complexity of the problem is concentrated in a Green's function:\\nA[x, y, Zj x', y', z'] = lim\\nN-+oo\\n\\n(( ([1-6ee , ]6[x-J?e]6[y-B*?e]6[z-B?e] (e?e')6[x' -J?e']6[y' - B*?e']6[y' - B?e'])i?i> )QW;t\\n\\nJ\\n\\nIt involves a conditional average of the form (K[J])QW;t = dJ Pt(JIQ,R,P)K[J], with\\nPt(J) 6[Q-Q[J]]6[R- R[J]] nXYZ 6[P[x, y, z] -P[x, y, Zj J]]\\nPt(JIQ,R,P)\\nJdJ Pt(J) 6[Q - Q[J]]6[R- R[J]] nXYZ 6[P[x, y, z] - P[x, y, z; J]]\\n\\n=\\n\\nin which Pt (J) is the weight probability density at time t. The solution of (6,7,8) can be\\nused to generate the N -+ 00 performance measures (4) at any time:\\nEt\\n\\n=/\\n\\ndxdydz P[x, y, z]O[-xz]\\n\\nEg\\n\\n= 11\\\"-1 arccos[RIVQ]\\n\\n(9)\\n\\nExpansion of these equations in powers of\\\"\\\" and retaining only the terms linear in \\\"\\\" gives\\nthe corresponding equations describing batch learning. So far this analysis is exact.\\n\\n4\\n\\nClosure of Macroscopic Laws\\n\\nAs in [6, 5] we close our macroscopic laws (6,7,8) by making the two key assumptions\\nunderlying dynamical replica theory:\\n(i) For N -+ 00 our macroscopic observables obey closed dynamic equations.\\n(ii) These equations are self-averaging with respect to the specific realization of D.\\n\\n(i) implies that probability variations within {Q, R, P} subshells are either absent or irrelevant to the macroscopic laws. We may thus make the simplest choice for Pt (J IQ, R, P):\\nPt(JIQ,R,P) -+ 6[Q-Q[J]] 6[R-R[J]]\\n\\nII 6[P[x,y,z]-P[x,y,ZjJ]]\\n\\n(10)\\n\\nxyz\\n\\nThe procedure (10) leads to exact laws if our observables {Q, R, P} indeed obey closed\\nequations for N -+ 00. It is a maximum entropy approximation if not. (ii) allows us\\nto average the macroscopic laws over all training sets; it is observed in simulations, and\\nproven using the formalism of [4]. Our assumptions (10) result in the closure of (6,7,8),\\nsince now the Green's function can be written in terms of {Q, R, Pl. The final ingredient\\nof dynamical replica theory is doing the average of fractions with the replica identity\\n\\n/ JdJ W[JID]GIJID])\\n\\n\\\\\\n\\nJdJ W[JID]\\n\\n= lim\\nsets\\n\\n/dJ I\\n\\n???\\n\\ndJn (G[J 1 ID]\\n\\nn-+O\\n\\nIT\\n\\nW[JO<ID])sets\\n\\na=1\\n\\nOur problem has been reduced to calculating (non-trivial) integrals and averages. One\\nfinds that P[x, y, z] P[x, zly]P[y] with Ply] (211\\\")-!exp[-!y 21With the short-hands\\nDy = P[y]dy and (f(x, y, z)) = Dydxdz P[x, zly]f(x, y, z) we can write the resulting\\nmacroscopic laws, for the case of output noise (1), in the following compact way:\\n\\n=\\n\\nd\\n\\ndt Q = 2\\\",(V - ,Q)\\n\\n[)\\n\\n[)tP[x,zly] =\\n\\n=\\n\\nJ\\n\\n+ rJ2 Z\\n\\nd\\n\\ndtR = \\\",(W - ,R)\\n\\n(11)\\n\\n1 [)x[)22P[x,zIY]\\na1/dx'P[x',zly] {6[x-x'-\\\",G[x',z]]-6[x-x'] }+2\\\",2Z\\n\\n-\\\",:x {P[x,zly]\\n\\n[U(x-RY)+Wy-,x+[V-RW-(Q-R2)U]~[x,y,z])}\\n\\n(12)\\n\\nwith\\n\\nU = (~[x, y, z]9[x, z]),\\n\\nv = (x9[x, z]),\\n\\nW = (y9[x, z]),\\n\\nZ = (9 2[x, z])\\n\\nThe solution of (12) is at any time of the following form:\\n\\nP[x,zly]\\n\\n= (1-,x)6[y-z]P+[xly] + ,x6[y+z]P-[xly]\\n\\n(13)\\n\\n\\fA. C. C. Coolen and C. W. H. Mace\\n\\n240\\n\\nFinding the function <I> [x, y, z] (in replica symmetric ansatz) requires solving a saddle-point\\nproblem for a scalar observable q and two functions M?[xly]. Upon introducing\\n\\nB = . . :. V. .,. .q.,-Q___R,-2\\nQ(I-q)\\n(with Jdx M?[xly]\\n\\nJdx M?[xly]eBxs J[x, y]\\nJdx M?[xly]eBxs\\n\\n(f[x, y])? =\\n*\\n\\n= 1 for all y) the saddle-point equations acquire the fonn\\np?[Xly] =\\n\\nfor all X, y :\\n\\n((x-Ry)2) + (qQ-R 2)[I-!:.]\\na\\n\\n!\\n\\nDs (O[X -xl);\\n\\n2 !DYDS S[(I-A)(X); + A(X);]\\n= qQ+Q-2R\\n..jqQ_R2\\n\\n(14)\\n(15)\\n\\nThe equations (14) which detennine M?[xly] have the same structure as the corresponding\\n(single) equation in [5, 6], so the proofs in [5, 6] again apply, and the solutions M?[xly],\\ngiven a q in the physical range q E [R2/Q, 1], are unique. The function <I> [x, y, z] is then\\ngiven by\\n<I> [X,\\n\\ny, z]\\n\\n=!\\n\\nDs s\\n{(I-A)O[Z-y](o[X -x)); + AO[Z+Y](o[X -xl);}\\n..jqQ_R2 P[X, zly]\\n(16)\\n\\nWorking out predictions from these equations is generally CPU-intensive, mainly due to\\nthe functional saddle-point equation (14) to be solved at each time step. However, as in [7]\\none can construct useful approximations of the theory, with increasing complexity:\\n\\n(i) Large a approximation (giving the simplest theory, without saddle-point equations)\\n(ii) Conditionally Gaussian approximation for M[xly] (with y-dependent moments)\\n(iii) Annealed approximation of the functional saddle-point equation\\n\\n5 Benchmark Tests: The Limits a --+ 00 and ,\\\\ --+ 0\\nWe first show that in the limit a --+ 00 our theory reduces to the simple (Q, R) formalism\\nof infinite training sets, as worked out for noisy teachers in [12]. Upon making the ansatz\\n\\np?[xly] = P[xly] = [27r(Q-R 2)]-t e- t [x- Rv]2/(Q-R 2)\\n\\n(17)\\n\\none finds\\n\\n<I>[x,y,Z] = (x-Ry)/(Q-R 2)\\n\\nM?[xly] = P[xly],\\n\\nInsertion of our ansatz into (12), followed by rearranging of terms and usage of the above\\nexpression for <I> [x, y, z], shows that (12) is satisfied. The remaining equations (11) involve\\nonly averages over the Gaussian distribution (17), and indeed reduce to those of [12]:\\n\\n~! Q =\\n\\n(I-A) { 2(x9[x, y))\\n1 d\\n--d R\\n1} t\\n\\n+ 1}{92[x, y)) } + A {2(x9[x,-y)) + 1}(92[x,-y)) } - 2,Q\\n\\n= (I-A)(y9[x,y)) + A(y9[x,-yl) -,R\\n\\nNext we turn to the limit A --+ 0 (restricted training sets & noise-free teachers) and show that\\nhere our theory reproduces the fonnalism of [6,5]. Now we make the following ansatz:\\n\\nP+[xly] = P[xly],\\n\\nP[x, zly]\\n\\n= o[z-y]P[xIY]\\n\\n(18)\\n\\nInsertion shows that for A = 0 solutions of this fonn indeed solve our equations, giving\\n<p[x, y, z]--+ <I> [x, y] and M+[xly]\\nM[xly), and leaving us exactly with the fonnalism\\nof [6, 5] describing the case of noise-free teachers and restricted training sets (apart from\\nsome new tenns due to the presence of weight decay, which was absent in [6, 5]).\\n\\n=\\n\\n\\f241\\n\\nSupervised Learning with Restricted Training Sets\\n0. , r------~--__,\\n\\n0..4\\n\\n~-------_____I\\n\\n0..4\\n\\n11>=0.'\\n\\n0..3\\n\\na=4\\n\\n0. ,\\n\\n0..0.\\n\\n--\\n\\n, 0.\\n\\n0.2\\n\\n_ __ ___ _____ _\\n\\na= 1\\n\\n0;=1\\n\\n------- ---- -- --- -\\n\\n0.\\n\\n0;=2\\n\\n=-=\\n-\\n\\n0;=2\\n\\n- - ----- -\\n\\na=4\\na=4\\n\\n= =-=\\n--=-=--=-=--=-=-=-- -=-=-_oed\\n\\na=4\\n\\n,\\n\\n0;=2\\n\\n':::::========:::j\\n\\n0..3\\n\\n-- - ----\\n\\n0;=1\\n\\n:::---- - -----1\\n\\n0;=2\\n\\n0..2\\n\\n11>=0.'\\n\\n~-------~\\n\\n0;=1\\n\\n0.,\\n\\n11>=0,\\n\\n\\\"\\n\\n,\\n\\nno. I\\n\\n0.\\n\\n, 0.\\n\\n\\\"\\n\\nFigure 1: On-line Hebbian learning: conditionally Gaussian approximation versus exact\\nsolution in [9] (.,., = 1, ,X = 0.2). Left: \\\"I = 0.1, right: \\\"I = 0.5. Solid lines: approximated\\ntheory, dashed lines: exact result. Upper curves: Eg as functions of time (here the two\\ntheories agree), lower curves: E t as functions of time.\\n\\n6\\n\\nBenchmark Tests: Hebbian Learning\\n\\nThe special case of Hebbian learning, i.e. Q[x, z] = sgn(z), can be solved exactly at any\\ntime, for arbitrary {a, ,x, \\\"I} [9], providing yet another excellent benchmark for our theory.\\nFor batch execution of Hebbian learning the macroscopic laws are obtained upon expanding\\n(11,12) and retaining only those terms which are linear in.,.,. All integrations can now be\\ndone and all equations solved explicitly, resulting in U =0, Z = 1, W = (I-2,X)J2/7r, and\\n\\nQ\\n\\n= Qo e-2rryt +\\n\\n2Ro(I-2'x) e-17\\\"Yt[I_e-rrrt]\\n\\\"I\\n\\nf{ + [~(I-2,X)2+.!.]\\n\\nV:;\\n\\n7r\\n\\na\\n\\n[I-e- 17 \\\"Y tF\\n\\\"12\\n\\nR = Ro e- 17\\\"Y t +(I-2'x)J2/7r[I-e- 17\\\"Y t ]/\\\"I\\nq = [aR2+(I_e- 17\\\"Yt)2 i'l]/aQ\\np?[xIY] = [27r(Q-R2)] -t e-tlz-RH sgn(y)[1-e-\\\"..,t]/a\\\"Y]2/(Q-R2)\\n(19)\\nFrom these results, in tum, follow the performance measures Eg = 7r- 1 arccos[ R/ JQ) and\\n\\nE = ! - !(1-,X)!D\\n2\\n\\nt\\n\\n2\\n\\nerf[IYIR+[I-e- 77\\\"Y t ]/a\\\"l] + !,X!D erf[IYIR-[I-e- 17\\\"Y t ]/a\\\"l]\\nY\\nJ2(Q-R2)\\n2\\ny\\nJ2(Q-R2)\\n\\nComparison with the exact solution, calculated along the lines of [9] or, equivalently, obtained upon putting t ?\\nin [9], shows that the above expressions are all exact.\\n\\n.,.,-2\\n\\nFor on-line execution we cannot (yet) solve the functional saddle-point equation in general.\\nHowever, some analytical predictions can still be extracted from (11,12,13):\\n\\nQ = Qo e-217\\\"Yt + 2Ro(I-2,X) e-77\\\"Yt[I_e-17\\\"Yt]\\n\\\"I\\n\\nR = Ro e- 17\\\"Y t + (I-2,X)J2/7r[I-e- 17\\\"Y t ]/\\\"I\\n\\nJ\\n\\nf{ + [~(I-2,X)2+.!.]\\n\\nV:;\\n\\n7r\\n\\na\\n\\n[I_e- 17\\\"Y t ]2\\n\\\"12\\n\\n+ !L[I_e- 217\\\"Y t ]\\n2\\\"1\\n\\ndx xP?[xIY] = Ry ? sgn(y)[I-e- 17\\\"Y t ]/a\\\"l\\n\\nwith U =0, W = (I-2,X)J2/7r, V = W R+[I-e- 17\\\"Y t ]/a\\\"l, and Z = 1. Comparison with the\\nresults in [9] shows that the above expressions, and thus also that of E g , are all fully exact,\\nat any time. Observables involving P[x, y, z] (including the training error) are not as easily\\nsolved from our equations. Instead we used the conditionally Gaussian approximation\\n(found to be adequate for the noiseless Hebbian case [5, 6, 7]). The result is shown in\\nfigure 1. The agreement is reasonable, but significantly less than that in [6]; apparently\\nteacher noise adds to the deformation of the field distribution away from a Gaussian shape.\\n\\n\\f242\\n\\nA. C. C. Coolen and C. W H. Mac\\n\\n~\\n\\n0.6\\n\\n000000\\n\\n0.4\\n\\n0.4\\n\\nE\\n\\n~\\n\\n0.2\\n\\nI\\ni\\n0.0\\n\\n0\\n\\n4\\n\\n2\\n\\n6\\n\\n10\\n\\n0.0\\n\\n-3\\n\\n-2\\n\\n-I\\n\\n0\\nX\\n\\n0.6\\n\\nf\\n\\n0.4\\n\\n0.4 [\\n\\nE\\n0.2\\n\\n0.2\\n\\n0.0\\n\\nL-o!i6iIII.\\\"\\\"\\\"\\\"\\\"',-\\\"--~_~~_ _--'\\n\\n-3\\n\\n-2\\n\\n-I\\n\\n0\\n\\n2\\n\\n3\\n\\nX\\n\\n,=\\n\\nFigure 2: Large a approximation versus numerical simulations (with N = 10,000), for\\n0 and A = 0.2. Top row: Perceptron rule, with.,., = ~. Bottom row: Adatron rule,\\nwith.,., = ~. Left: training errors E t and generalisation errors Eg as functions of time, for\\naE {~, 1, 2}. Lines: approximated theory, markers: simulations (circles: E t , squares: Eg) .\\nRight: joint distributions for student field and teacher noise p?[x] = dy P[x, y, z = ?y]\\n(upper: P+[x], lower: P-[x]). Histograms: simulations, lines: approximated theory.\\n\\nJ\\n\\n7\\n\\nNon-Linear Learning Rules: Theory versus Simulations\\n\\nIn the case of non-linear learning rules no exact solution is known against which to test our\\nformalism, leaving numerical simulations as the yardstick. We have evaluated numerically\\nthe large a approximation of our theory for Perceptron learning, 9[x, z] = sgn(z)O[-xz],\\nand for Adatron learning, 9[x, z] = sgn(z)lzIO[-xz]. This approximation leads to the\\nfollowing fully explicit equation for the field distributions:\\n\\n1/\\n\\nd\\n-p?[xly]\\n= dt\\na\\n.\\n\\nWith\\n\\nU=\\n\\n' +1\\n\\ndx' p?[x'ly]{o[x-x'-.,.,.1'[x', ?y]] -o[x-x]}\\n\\n_ ~ {P[ I ] [W _\\n.,., 8\\nx y\\ny\\n\\nJ\\n\\nX\\n\\n~ p?[xly]\\n\\n_.,.,2 Z!:I 2\\n2\\nuX\\n\\n,X + U[X?(y)-RY]+(V-RW)[X-X?(y)]]}\\nQ _ R2\\n\\nDydx {(I-A)P+[xly][x-P(y)]9[x,Y]+AP-[xly][x-x-(y)]9[x,-y])\\nV =\\nW=\\nZ=\\n\\n!\\n1\\n1\\n\\nDydx x {(I-A)P+[xly]9[x, Y]+AP-[xly]9[x,-y])\\nDydx y {(1-A)P+[xly]9[x, Y]+AP-[xly]9[x,-y])\\n\\nDydx {(I-A)P+[xly]92[x, Y]+AP-[xly]9 2[x,-yJ)\\n\\n\\fSupervised Learning with Restricted Training Sets\\n\\n243\\n\\nJ\\n\\nand with the short-hands X?(y) = dx xP?[xly). The result of our comparison is shown\\nin figure 2. Note: E t increases monotonically with a, and Eg decreases monotonically\\nwith a, at any t. As in the noise-free formalism [7], the large a approximation appears to\\ncapture the dominant terms both for a -7 00 and for a -7 O. The predicting power of our\\ntheory is mainly limited by numerical constraints. For instance, the Adatron learning rule\\ngenerates singularities at x = 0 in the distributions P?[xly) (especially for small \\\"I) which,\\nalthough predicted by our theory, are almost impossible to capture in numerical solutions.\\n\\n8 Discussion\\nWe have shown how a recent theory to describe the dynamics of supervised learning with\\nrestricted training sets (designed to apply in the data recycling regime, and for arbitrary online and batch learning rules) [5, 6, 7] in large layered neural networks can be generalized\\nsuccessfully in order to deal also with noisy teachers. In our generalized approach the joint\\ndistribution P[x, y, z) for the fields of student, 'clean' teacher, and noisy teacher is taken to\\nbe a dynamical order parameter, in addition to the conventional observables Q and R. From\\nthe order parameter set {Q, R, P} we derive the generalization error Eg and the training\\nerror E t . Following the prescriptions of dynamical replica theory one finds a diffusion\\nequation for P[x, y, z], which we have evaluated by making the replica-symmetric ansatz.\\nWe have carried out several orthogonal benchmark tests of our theory: (i) for a -7 00 (no\\ndata recycling) our theory is exact, (ii) for A -7 0 (no teacher noise) our theory reduces\\nto that of [5, 6, 7], and (iii) for batch Hebbian learning our theory is exact. For on-line\\nHebbian learning our theory is exact with regard to the predictions for Q, R, Eg and the\\ny-dependent conditional averages Jdx xP?[xly), at any time, and a crude approximation\\nof our equations already gives reasonable agreement with the exact results [9] for E t . For\\nnon-linear learning rules (Perceptron and Adatron) we have compared numerical solution\\nof a simple large a aproximation of our equations to numerical simulations, and found\\nsatisfactory agreement. This paper is a preliminary presentation of results obtained in the\\nsecond stage of a research programme aimed at extending our theoretical tools in the arena\\nof learning dynamics, building on [5, 6, 7]. Ongoing work is aimed at systematic application of our theory and its approximations to various types of non-linear learning rules, and\\nat generalization of the theory to multi-layer networks.\\n\\nReferences\\n[1]\\n[2]\\n[3]\\n[4]\\n[5]\\n[6]\\n[7]\\n[8]\\n[9]\\n[10]\\n[11]\\n[12]\\n\\nMace C.W.H. and Coolen AC.C (1998), Statistics and Computing 8, 55\\nSaad D. (ed.) (1998), On-Line Learning in Neural Networks (Cambridge: CUP)\\nHertz J.A., Krogh A and Thorgersson G.I. (1989), J. Phys. A 22, 2133\\nHomerH. (1992a), Z. Phys. B 86, 291 and Homer H. (1992b), Z. Phys. B 87,371\\nCoolen A.C.C. and Saad D. (1998), in On-Line Learning in Neural Networks, Saad\\nD. (ed.), (Cambridge: CUP)\\nCoolen AC.C. and Saad D. (1999), in Advances in Neural Information Processing\\nSystems 11, Kearns D., Solla S.A., Cohn D.A (eds.), (MIT press)\\nCoolen A.C.C. and Saad D. (1999), preprints KCL-MTH-99-32 & KCL-MTH-99-33\\nRae H.C., Sollich P. and Coolen AC.C. (1999), in Advances in Neural Information\\nProcessing Systems 11, Kearns D., Solla S.A., Cohn D.A. (eds.), (MIT press)\\nRae H.C., Sollich P. and Coolen AC.C. (1999),J. Phys. A 32, 3321\\nInoue J.I. (1999) private communication\\nWong K.YM., Li S. and Tong YW. (1999),preprint cond-mat19909004\\nBiehl M., Riegler P. and Stechert M. (1995), Phys. Rev. E 52, 4624\\n\\n\\f\",\n          \"Predicting Action Content On-Line and in\\nReal Time before Action Onset ? an\\nIntracranial Human Study\\n\\nShengxuan Ye\\nCalifornia Institute of Technology\\nPasadena, CA\\nsye@caltech.edu\\n\\nUri Maoz\\nCalifornia Institute of Technology\\nPasadena, CA\\nurim@caltech.edu\\nIan Ross\\nHuntington Hospital\\nPasadena, CA\\nianrossmd@aol.com\\n\\nAdam Mamelak\\nCedars-Sinai Medical Center\\nLos Angeles, CA\\nadam.mamelak@cshs.org\\n\\nChristof Koch\\nCalifornia Institute of Technology\\nPasadena, CA\\nAllen Institute for Brain Science\\nSeattle, WA\\nkoch@klab.caltech.edu\\n\\nAbstract\\nThe ability to predict action content from neural signals in real time before the action occurs has been long sought in the neuroscientific study of decision-making,\\nagency and volition. On-line real-time (ORT) prediction is important for understanding the relation between neural correlates of decision-making and conscious,\\nvoluntary action as well as for brain-machine interfaces. Here, epilepsy patients,\\nimplanted with intracranial depth microelectrodes or subdural grid electrodes for\\nclinical purposes, participated in a ?matching-pennies? game against an opponent.\\nIn each trial, subjects were given a 5 s countdown, after which they had to raise\\ntheir left or right hand immediately as the ?go? signal appeared on a computer\\nscreen. They won a fixed amount of money if they raised a different hand than\\ntheir opponent and lost that amount otherwise. The question we here studied was\\nthe extent to which neural precursors of the subjects? decisions can be detected in\\nintracranial local field potentials (LFP) prior to the onset of the action.\\nWe found that combined low-frequency (0.1?5 Hz) LFP signals from 10 electrodes\\nwere predictive of the intended left-/right-hand movements before the onset of the\\ngo signal. Our ORT system predicted which hand the patient would raise 0.5 s\\nbefore the go signal with 68?3% accuracy in two patients. Based on these results,\\nwe constructed an ORT system that tracked up to 30 electrodes simultaneously,\\nand tested it on retrospective data from 7 patients. On average, we could predict\\nthe correct hand choice in 83% of the trials, which rose to 92% if we let the system\\ndrop 3/10 of the trials on which it was less confident. Our system demonstrates?\\nfor the first time?the feasibility of accurately predicting a binary action on single\\ntrials in real time for patients with intracranial recordings, well before the action\\noccurs.\\n\\n1\\n\\n\\f1\\n\\nIntroduction\\n\\nThe work of Benjamin Libet [1, 2] and others [3, 4] has challenged our intuitive notions of the relation between decision making and conscious voluntary action. Using electrocorticography (EEG),\\nthese experiments measured brain potentials from subjects that were instructed to flex their wrist at a\\ntime of their choice and note the position of a rotating dot on a clock when they felt the urge to move.\\nThe results suggested that a slow cortical wave measured over motor areas?termed ?readiness potential? [5], and known to precede voluntary movement [6]?begins a few hundred milliseconds before the average reported time of the subjective ?urge? to move. This suggested that action onset and\\ncontents could be decoded from preparatory motor signals in the brain before the subject becomes\\naware of an intention to move and of the contents of the action. However, the readiness potential\\nwas computed by averaging over 40 or more trials aligned to movement onset after the fact. More\\nrecently, it was shown that action contents can be decoded using functional magnetic-resonance\\nimaging (fMRI) several seconds before movement onset [7]. But, while done on a single-trial basis,\\ndecoding the neural signals took place off-line, after the experiment was concluded, as the sluggish\\nnature of fMRI hemodynamic signals precluded real-time analysis. Moreover, the above studies\\nfocused on arbitrary and meaningless action?purposelessly raising the left or right hand?while\\nwe wanted to investigate prediction of reasoned action in more realistic, everyday situations with\\nconsequences for the subject.\\nIntracranial recordings are good candidates for single-trial, ORT analysis of action onset and contents [8, 9], because of the tight temporal pairing of LFP to the underlying neuronal signals. Moreover, such recordings are known to be cleaner and more robust, with signal-to-noise ratios up to\\n100 times larger than surface recordings like EEG [10, 11]. We therefore took advantage of a rare\\nopportunity to work with epilepsy patients implanted with intracranial electrodes for clinical purposes. Our ORT system (Fig. 1) predicts, with far above chance accuracy, which one of two future\\nactions is about to occur on this one trial and feeds the prediction back to the experimenter, all\\nbefore the onset of the go signal that triggers the patient?s movement (see Experimental Methods).\\nWe achieve relatively high prediction performance using only part of the data?learning from brain\\nactivity in past trials only (Fig. 2) to predict future ones (Fig. 3)?while still running the analysis\\nquickly enough to act upon the prediction before the subject moved.\\n\\n2\\n2.1\\n\\nExperimental Methods\\nSubjects\\n\\nSubjects in this experiment were 8 consenting intractable epilepsy patients that were implanted with\\nintracranial electrodes as part of their presurgical clinical evaluation (ages 18?60, 3 males). They\\nwere inpatients in the neuro-telemetry ward at the Cedars Sinai Medical Center or the Huntington\\nMemorial Hospital, and are designated with CS or HMH after their patient numbers, respectively. Six\\nof them?P12CS, P15CS, P22CS and P29?31HMH were implanted with intracortical depth electrodes targeting their bilateral anterior-cingulate cortex, amygdala, hippocampus and orbitofrontal\\ncortex. These electrodes had eight 40 ?m microwires at their tips, 7 for recording and 1 serving as\\na local ground. Two patients, P15CS and P22CS, had additional microwires in the supplementary\\nmotor area. We utilized the LFP recorded from the microwires in this study. Two other patients,\\nP16CS and P19CS, were implanted with an 8?8 subdural grid (64 electrodes) over parts of their\\ntemporal and prefrontal dorsolateral cortices. The data of one patient?P31HMH?was excluded\\nbecause microwire signals were too noisy for meaningful analysis. The institutional review boards\\nof Cedars Sinai Medical Center, the Huntington Memorial Hospital and the California Institute of\\nTechnology approved the experiments.\\nDuring the experiment, the subject sat in a hospital bed in a semi-inclined ?lounge chair? position.\\nThe stimulus/analysis computer (bottom left of Fig. 4) displaying the game screen (bottom right\\ninset of Fig. 4) was positioned to be easily viewable for the subject. When playing against the\\nexperimenter, the latter sat beside the bed. The response box was placed within easy reach of the\\nsubject (Fig. 4).\\n2\\n\\n\\f2.2\\n\\nExperiment Design\\n\\nAs part of our focus on purposeful, reasoned action, we had the subjects play a matching-pennies\\ngame?a 2-choice version of ?rock paper scissors??either against the experimenter or against a\\ncomputer. The subjects pressed down a button with their left hand and another with their right on a\\nresponse box. Then, in each trial, there was a 5 s countdown followed by a go signal, after which\\nthey had to immediately lift one of their hands. It was agreed beforehand that the patient would win\\nthe trial if she lifted a different hand than her opponent, and lose if she raised the same hand as her\\nopponent. Both players started off with a fixed amount of money, $5, and in each trial $0.10 was\\ndeducted from the loser and awarded to the winner. If a player lifted her hand before the go signal,\\ndid not lift her hand within 500 ms of the go signal, or lifted no hand or both hands at the go signal?\\nan error trial?she lost $0.10 without her opponent gaining any money. The subjects were shown the\\ncountdown, the go signal, the overall score, and various instructions on a stimulus computer placed\\nbefore them (Fig. 4). Each game consisted of 50 trials. If, at the end of the game, the subject had\\nmore money than her opponent, she received that money in cash from the experimenter.\\nBefore the experimental session began, the experimenter explained the rules of the game to the subject, and she could practice playing the game until she was familiar with it. Consequently, patients\\nusually made only few errors during the games (<6% of the trials). Following the tutorial, the subject played 1?3 games against the computer and then once against the experimenter, depending on\\ntheir availability and clinical circumstances. The first 2 games of P12CS were removed because\\nthe subject tended to constantly raise the right hand regardless of winning or losing. Two patients,\\nP15CS and P19CS, were tested in actual ORT conditions. In such sessions?3 games each?the\\nsubjects always played against the experimenter. These ORT games were different from the other\\ngames in two respects. First, a computer screen was placed behind the patient, in a location where\\nshe could not see it. Second, the experimenter was wearing earphones (Fig. 1,4). Half a second before go-signal onset, an arrow pointing towards the hand that the system predicted the experimenter\\nhad to raise to win the trial was displayed on that screen. Simultaneously, a monophonic tone was\\nplayed in the experimenter?s earphone ipsilateral to that hand. The experimenter then lifted that hand\\nat the go signal (see Supplemental Movie).\\n\\nCheetah Machine\\nCollect\\nand save\\ndata\\n\\nPatient\\nwith intracranial electrodes\\n\\nDown\\nsampling\\n\\nBuffer\\n\\n1Gbps\\nRouter\\n\\nTTL Signal\\n\\nThe winner is\\nPlayer 1\\nPLAYER 1 PLAYER 2\\nSCORE 1\\n\\nAnalysis/stimulus machine\\n\\nSCORE 2\\n\\nResponse Box Game Screen\\n\\n/\\nExperimenter\\n\\nResult\\nInterpreta\\ntion\\n\\nAnalysis\\n\\nFiltering\\n\\nDisplay/Sound\\n\\nFigure 1: A schematic diagram of the on-line real-time (ORT) system. Neural signals flow from\\nthe patient through the Cheetah machine to the analysis/stimulus computer, which controls the input\\nand output of the game and computes the prediction of the hand the patient would raise at the go\\nsignal. It displays it on a screen behind the patient and informs the experimenter which hand to raise\\nby playing a tone in his ipsilateral ear using earphones.\\n\\n3\\n\\n\\f3\\n3.1\\n\\nThe real-time system\\nHardware and software overview\\n\\n?V\\n\\n?V\\n\\n?V\\n\\nNeural data from the intracranial electrodes were transferred to a recording system (Neuralynx,\\nDigital Lynx), where it was collected and saved to the local Cheetah machine, down sampled\\nfrom 32 kHz to 2 kHz and buffered. The data were then transferred, through a dedicated 1 Gbps\\nlocal-area network, to the analysis/stimulus machine. This computer first band-pass-filtered the\\ndata to the 0.1?5 Hz range (delta and lower theta bands) using a second-order zero-lag elliptic\\nfilter with an attenuation of 40 dB (cf. Figs. 2a and 2b). We found that this frequency range?\\ngenerally comparable to that of the readiness potential?resulted in optimal prediction performance.\\nIt then ran the analysis algorithm (see below) on the filtered data. This computer also controlled\\nthe game screen, displaying the names of the players, their current scores and various instructions.\\nThe analysis/stimulus computer further\\ncontrolled the response box, which con- (a)\\n800\\nsisted of 4 LED-lit buttons. The buttons of the subject and her opponent\\n600\\nflashed red or blue whenever she or her\\n?5\\n?4\\n?3\\n?2\\n?1\\n0\\nopponent won, respectively. Addition(b)100\\nally, the analysis/stimulus computer sent\\n0\\na unique transistor-transistor logic (TTL)\\n?100\\n?200\\npulse whenever the game screen changed\\n?5\\n?4\\n?3\\n?2\\n?1\\n0\\nor a button was pressed on the response\\nbox, which synchronized the timing of (c) 100\\n0\\nthese events with the LFP recordings.\\n?100\\nIn real-time game sessions, the analy?200\\n?5\\n?4\\n?3\\n?2\\n?1\\n0\\nsis/stimulus computer also displayed the\\nappropriate arrow on the computer screen (d) 1\\nbehind the subject and played the tone\\n0\\nto the appropriate ear of the experimenter\\n?1\\n0.5 s before go-signal onset (Figs. 1,4).\\n?5\\n?4\\n?3\\n?2\\n?1\\n0\\nThe analysis software was based on a\\nmachine-learning algorithm that trained\\non past-trials data to predict the current\\ntrial and is detailed below. The training phase included the first 70% of the\\ntrials, with the prediction carried out on\\nthe remaining 30% using the trained parameters, together with an online weighting system (see below). The system examined only neural activity, and had no\\naccess to the subject?s left/right-choice\\nhistory. After filtering all the training\\ntrials (Fig. 2b), the system found the\\nmean and standard error over all leftward\\nand rightward training trials, separately\\n(Fig. 2c, left designated in red). It then\\nfound the electrodes and time windows\\nwhere the left/right separation was high\\n(Fig. 2d,e; see below), and trained the classifiers on these time windows (Fig. 2f?g).\\nThe best electrode/time-window/classifier\\n(ETC) combinations were then used to\\npredict the current trial in the prediction\\nphase (Fig. 3). The number of ETCs that\\ncan be actively monitored is currently limited to 10 due to the computational power\\nof the real-time system.\\n\\nEl 49?T1\\n\\n(e)\\n\\nEl 49?T2\\n\\nEl 49?T3\\n\\n1\\n0\\n?1\\n?5\\n\\n?4\\n\\n?3\\n?2\\n?1\\nCountdown to go signal at t=0 (seconds)\\n\\n0\\n\\n(f)\\nClassifier\\nCf1\\n\\nClassifier\\nCf2\\n\\n...\\n\\nClassifier\\nCf6\\n\\nEl 49?T1?Cf1\\nEl 49?T1?Cf2\\nEl 49?T1?Cf6\\n...\\nEl 49?T2?Cf1\\nEl 49?T2?Cf2\\nEl 49?T2?Cf6\\nEl 49?T3?Cf1\\nEl 49?T3?Cf2\\nEl 49?T3?Cf6\\n\\n(g)\\nCombination\\nEl49-T1-Cf2\\n\\nCombination\\nEl49-T2-Cf2\\n\\n...\\n\\nCombination\\nEl49-T2-Cf6\\n\\nFigure 2: The ORT-system?s training phase. Left (in\\nred) and right (in blue) raw signals (a) are low-pass filtered (b). Mean?standard errors of signals preceeding left- and right-hand movments (c) are used to compute a left/right separability index (d), from which time\\nwindows with good separation are found (e). Seven\\nclassifiers are then applied to all the time windows (f)\\nand the best electrode/time-window/classifier combinations are selected (g) and used in the prediction phase\\n(Fig. 3).\\n\\n4\\n\\n\\f?V\\n\\n100\\n0\\n?100\\n?200\\n?5\\n\\n?4\\n\\n?3\\n\\n?2\\n\\n?1\\n\\n0\\n\\nTrained classifiers\\n\\nCombination\\nE l 49?T1?Cf2\\n\\nCombination\\nE l 49?T2?Cf2\\n\\nWeight = 1\\n\\nWeight = 1\\n\\nCombination\\nE l 49?T2?Cf6\\n\\n&\\n\\nWeight = 1\\n\\nPredicted result\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nR\\n\\nL\\n\\n&\\n\\nR\\n\\nL\\nReal result\\n\\nAdjust the weights\\n\\nL\\n\\n==\\n\\nFigure 3: The ORT-system?s prediction phase. A new signal?from 5 to 0.5 seconds before the\\ngo signal?is received in real time, and each electrode/time-window/classifier combination (ETC)\\nclassifies it as resulting in left- or right-hand movement. These predictions are then compared to the\\nactual hand movement, with the weights associated with ETCs that correctly (incorrectly) predicted\\nincreasing (decreasing).\\n\\n3.2\\n\\nComputing optimal left/right-separating time windows\\n\\nThe algorithm focused on finding the time windows with the best left/right separation for the different recording electrodes over the training set (Fig. 2c?e). That is, we wanted to predict whether\\nthe signal aN (t) on trial N will result in a leftward or rightward movement?i.e., whether the label of the N th trial will be Lt or Rt, respectively. For each electrode, we looked at the N ? 1\\nprevious trials a1 (t), a2 (t), . . . , aN ?1 (t), and their associated labels as l1 , l2 , . . . , lN ?1 . Now, let\\nN ?1\\n?1\\nL(t) = {ai (t) | li = Lt}N\\ni=1 and R(t) = {ai (t) | li = Rt}i=1 be the set of previous leftward and\\nrightward trials in the training set, respectively. Furthermore, let Lm (t) (Rm (t)) and Ls (t) (Rs (t))\\nbe the mean and standard error of L(t) (R(t)), respectively. We can now define the normalized\\nrelative left/right separation for each electrode at time t (see Fig. 2d):\\n?\\n[Lm (t) ? Ls (t)] ? [Rm (t) + Rs (t)]\\n?\\n?\\nif [Lm (t) ? Ls (t)] ? [Rm (t) + Rs (t)] > 0\\n?\\n?\\nLm (t) ? Rm (t)\\n?\\n?\\n?\\n?\\n?\\n[Rm (t) ? Rs (t)] ? [Lm (t) + Ls (t)]\\n?(t) =\\n?\\nif [Rm (t) ? Rs (t)] ? [Lm (t) + Ls (t)] > 0\\n?\\n?\\n?\\nRm (t) ? Lm (t)\\n?\\n?\\n?\\n?\\n?\\n?\\n0\\notherwise\\nThus, ?(t) > 0 (?(t) < 0) means that the leftward trials tend to be considerably higher (lower)\\nthan rightward trials for that electrode at time t, while ?(t) = 0 suggests no left/right separation at\\ntime t. We define a consecutive time period of |?(t)| > 0 for t < prediction time (the time before\\nthe go signal when we want the system to output a prediction; -0.5 s for the ORT trials) as a time\\nwindow (Fig. 2e). After all time windows are found for all electrodes, time windows lessRthan M ms\\nt\\napart are combined into one. Then, for each time window from t1 to t2 we define a = t12 |?(t)|dt.\\nWe then eliminate all time windows satisfying a < A. We found the values M = 200 ms and\\nA = 4, 500 ?V ? ms to be optimal for real-time analysis. This resulted in 20?30 time windows over\\nall 64 electrodes that we monitored.\\n5\\n\\n\\f1\\n$4.80\\n\\n$5.20\\n\\nP15CS\\n\\nUri\\n\\nFigure 4: The experimental setup in the clinic. At 400 ms before the go signal, the patient and\\nexperimenter are watching the game screen (inset on bottom right) on the analysis/stimulus computer\\n(bottom left) and still pressing down the buttons of the response box. The realtime system already\\ncomputed a prediction, and thus displays an arrow on the screen behind the patient and plays a tone\\nin the experimenter?s ear ipsilateral to the hand it predicts he should raise to beat the patient (see\\nSupplemental Movie).\\n3.3\\n\\nClassifiers selection and ETC determination\\n\\nWe used ensemble learning with 7 types of relatively simple binary classifiers (due to real-time\\nprocessing considerations) on every electrode?s time windows (Fig. 2f). Classifiers A to G would\\nclassify aN (t) as Lt if:\\nP\\nP\\nP\\n(A) Defining aN,M , Lm,M and Rm,M as aN (t), Lm (t) and Rm (t) over time window M ,\\n\\u0001\\n\\u0001\\n\\u0001\\n(i) sign Rm,M 6= sign aN,M = sign Lm,M , or\\n\\f\\n\\f\\n\\f \\f\\n\\u0001\\n\\u0001\\n\\u0001\\n(ii) sign Rm,M = sign aN,M = sign Lm,M and \\fLm,M \\f > \\fRm,M \\f, or\\n\\f\\n\\f\\n\\f \\f\\n\\u0001\\n\\u0001\\n\\u0001\\n(iii) sign Rm (t) 6= sign SN,M 6= sign Lm (t) and \\fLm,M \\f < \\fRm,M \\f;\\n\\f\\n\\u0001\\n\\u0001\\f \\f\\n\\u0001\\n\\u0001\\f\\n(B) \\fmean aN (t) ? mean Lm (t) \\f < \\fmean aN (t) ? mean Rm (t) \\f;\\n\\f\\n\\f\\n\\u0001\\n\\u0001\\f\\n\\u0001\\n\\u0001\\f\\n(C) \\fmedian aN (t) ? median Lm (t) \\f < \\fmedian aN (t) ? median Rm (t) \\f over the time\\nwindow;\\n\\f\\n\\f\\n\\f\\n\\f\\n\\f\\n(D) aN (t) ? Lm (t)\\fL2 < \\faN (t) ? Rm (t)\\fL2 over the time window;\\n(E) aN (t) is convex/concave like Lm (t) while Rm (t) is concave/convex, respectively;\\n(F) Linear support-vector machine (SVM) designates it as so; and\\n(G) k-nearest neighbors (KNN) with Euclidean distance designates it as so.\\nEach classifier is optimized for certain types of features. To estimate how well its classification\\nwould generalize from the training to the test set, we trained and tested it using a 70/30 crossvalidation procedure within the training set. We tested each classifier on every time window of every\\nelectrode, discarding those with accuracy <0.68, which left 12.0 ? 1.6% of the original 232 ? 18\\nETCs, on average (?standard error). The training phase therefore ultimately output a set of S binary\\nETC combinations (Fig. 2g) that were used in the prediction phase (Fig. 3).\\n3.4\\n\\nThe prediction-phase weighting system\\n\\nIn the prediction phase, each of the overall S binary ETCs calculates a prediction, ci ? {?1, 1} (for\\nright and left, respectively), independently at the desired prediction time. All classifiers are initially\\n6\\n\\n\\fPS\\ngiven the same weight, w1 = w2 = ? ? ? = wS = 1. We then calculate ? = i=1 wi ? ci and predict\\nleft (right) if ? > d (? < ?d), or declare it an undetermined trial if ?d < ? < d. Here d is the\\ndrop-off threshold for the prediction. Thus the larger d is, the more confident the system needs to be\\nto make a prediction, and the larger the proportion of trials on which the system abstains?the dropoff rate. Weight wi associated with ETCi is increased (decreased) by 0.1 whenever ETCi predicts\\nthe hand movement correctly (incorrectly). A constantly erring ETC would therefore be associated\\nwith an increasingly small and then increasingly negative weight.\\n3.5\\n\\nImplementation\\n\\nThe algorithm was implemented in MATLAB 2011a (MathWorks, Natick, MA) as well as in C++\\non Visual Studio 2008 (Microsoft, Redmond, WA) for enhanced performance. The neural signals\\nwere collected by the Digital Lynx S system using Cheetah 5.4.0 (Neuralynx, Redmond, WA). The\\nsimulated-ORT system was also implemented in MATLAB 2011a. The simulated-ORT analyses\\ncarried out in this paper used real patient data saved on the Digital Lynx system.\\n1\\n\\n0.9\\n\\nDrop rate:\\nNone\\n0.18\\n0\\u0011\\u0016\\u0013\\n\\nPrediction accuracy\\n\\n0.8\\n\\n0.7\\nSignificant accuracy\\n(p=0.05)\\n0.6\\n\\n0.5\\n\\n?5\\n\\n?4.5\\n\\n?4\\n\\n?3.5\\n\\n?3\\n\\n?2.5\\nTime (s)\\n\\n?2\\n\\n?1.5\\n\\n?1\\n\\n?0.5\\n\\n0\\nGo-signal\\nonset\\n\\nFigure 5: Across-subjects average of the prediction accuracy of simulated-ORT versus time before\\nthe go signal. The mean accuracies over time when the system predicts on every trial, is allowed\\nto drop 19% or 30% of the trials, are depicted in blue, green and red, respectively (?standard error\\nshaded). Values above the dashed horizontal line are significant at p = 0.05.\\n\\n4\\n\\nResults\\n\\nWe tested our prediction system in actual real time on 2 patients?P15CS and P19CS (a depth\\nand grid patient, respectively), with a prediction time of 0.5 s before the go signal (see Supplementary Movie). Because of computational limitations, the ORT system could only track 10\\nelectrodes with just 1 ETC per electrode in real time. For P15CS, we achieved an accuracy of\\n72?2% (?standard error; accuracy = number of accurately predicted trials / [total number of trials - number of dropped trials]; p = 10?8 , binomial test) without modifying the weights online during the prediction (see Section 3.4). For P19CS we did not run patient-specific training of the ORT system, and used parameter values that were good on average over previous patients instead. The prediction accuracy was significantly above chance 63?2% (?standard error; p = 7 ? 10?4 , binomial test). To understand how much we could improve our accuracy\\nwith optimized hardware/software, we ran the simulated-ORT at various prediction times along\\n7\\n\\n\\fAccuracy\\n\\nthe 5 s countdown leading to the go signal. We further tested 3 drop-off rates?0, 0.19 and\\n0.30 (Fig. 5; drop-off rate = number of dropped trials / total number of trials; these resulted\\nfrom 3 drop-off thresholds?0, 0.1 and 0.2?respectively, see Section 3.4:). Running offline,\\nwe were able to track 20?30 ETCs, which resulted in considerably higher accuracies (Figs. 5,6).\\nAveraged over all subjects, the accuracy rose from about 65% more than\\n1\\n4 s before the go signal to 83?92%\\nclose to go-signal onset, depending\\n0.9\\non the allowed drop-off rate. In particular, we found that for a predic0.8\\ntion time of 0.5 s before go-signal\\nonset, we could achieve accuracies\\n0.7\\nof 81?5% and 90?3% (?standard\\nerror) for P15CS and P19CS, re0.6\\nspectively, with no drop off (Fig. 6).\\nPatients:\\nP12CS\\nWe also analyzed the weights that\\nP15CS\\nour weighting system assigned to the\\n0.5\\nP16CS\\nP19CS\\ndifferent ETCs. We found that the\\nP22CS\\nempirical distribution of weights to\\nP29HMH\\n0.4\\nP30HMH\\nETCs associated with classifiers A to\\nG was, on average: 0.15, 0.12, 0.16,\\n?5 ?4.5 ?4 ?3.5 ?3 ?2.5 ?2 ?1.5 ?1 ?0.5 0\\n0.22, 0.01, 0.26 and 0.07, respecTime before go signal (at t=0) (seconds)\\ntively. This suggests that the linear\\nSVM and L2-norm comparisons (of\\naN to Lm and Rm ) together make up Figure 6: Simulated-ORT accuracy over time for individual\\nnearly half of the overall weights at- patients with no drop off.\\ntributed to the classifiers, while the\\ncurrent concave/convex measure is of\\nlittle use as a classifier.\\n\\n5\\n\\nDiscussion\\n\\nWe constructed an ORT system that, based on intracranial recordings, predicted which hand a person would raise well before movement onset at accuracies much greater than chance in a competitive environment. We further tested this system off-line, which suggested that with optimized\\nhardware/software, such action contents would be predictable in real time at relatively high accuracies already several seconds before movement onset. Both our prediction accuracy and drop-off\\nrates close to movement onset are superior to those achieved before movement onset with noninvasive methods like EEG and fMRI [7, 12?14]. Importantly, our subjects played a matching pennies game?a 2-choice version of rock-paper-scissors [15]?to keep their task realistic, with minor\\nthough real consequences, unlike the Libet-type paradigms whose outcome bears no consequences\\nfor the subjects. It was suggested that accurate online, real-time prediction before movement onset\\nis key to investigating the relation between the neural correlates of decisions, their awareness, and\\nvoluntary action [16, 17]. Such prediction capabilities would facilitate many types of experiments\\nthat are currently infeasible. For example, it would make it possible to study decision reversals on\\na single-trial basis, or to test whether subjects can guess above chance which of their action contents are predictable from their current brain activity, potentially before having consciously made up\\ntheir mind [16, 18]. Accurately decoding these preparatory motor signals may also result in earlier\\nand improved classification for brain-computer interfaces [13, 19, 20]. The work we present here\\nsuggests that such ORT analysis might well be possible.\\nAcknowledgements\\nWe thank Ueli Rutishauser, Regan Blythe Towel, Liad Mudrik and Ralph Adolphs for meaningful\\ndiscussions. This research was supported by the Ralph Schlaeger Charitable Foundation, Florida\\nState University?s ?Big Questions in Free Will? initiative and the G. Harold & Leila Y. Mathers\\nCharitable Foundation.\\n8\\n\\n\\fReferences\\n[1] B. Libet, C. Gleason, E. Wright, and D. Pearl. Time of conscious intention to act in relation to\\nonset of cerebral activity (readiness-potential): The unconscious initiation of a freely voluntary\\nact. Brain, 106:623, 1983.\\n[2] B. Libet. Unconscious cerebral initiative and the role of conscious will in voluntary action.\\nBehavioral and brain sciences, 8:529?539, 1985.\\n[3] P. Haggard and M. Eimer. On the relation between brain potentials and the awareness of\\nvoluntary movements. Experimental Brain Research, 126:128?133, 1999.\\n[4] A. Sirigu, E. Daprati, S. Ciancia, P. Giraux, N. Nighoghossian, A. Posada, and P. Haggard.\\nAltered awareness of voluntary action after damage to the parietal cortex. Nature Neuroscience,\\n7:80?84, 2003.\\n[5] H. Kornhuber and L. Deecke. Hirnpotenti?alanderungen bei Willk?urbewegungen und passiven\\nBewegungen des Menschen: Bereitschaftspotential und reafferente Potentiale. Pfl?ugers Archiv\\nEuropean Journal of Physiology, 284:1?17, 1965.\\n[6] H. Shibasaki and M. Hallett. What is the Bereitschaftspotential? Clinical Neurophysiology,\\n117:2341?2356, 2006.\\n[7] C. Soon, M. Brass, H. Heinze, and J. Haynes. Unconscious determinants of free decisions in\\nthe human brain. Nature Neuroscience, 11:543?545, 2008.\\n[8] I. Fried, R. Mukamel, and G. Kreiman. Internally generated preactivation of single neurons in\\nhuman medial frontal cortex predicts volition. Neuron, 69:548?562, 2011.\\n[9] M. Cerf, N. Thiruvengadam, F. Mormann, A. Kraskov, R. Quian Quiorga, C. Koch, and\\nI. Fried. On-line, voluntary control of human temporal lobe neurons. Nature, 467:1104?1108,\\n2010.\\n[10] T. Ball, M. Kern, I. Mutschler, A. Aertsen, and A. Schulze-Bonhage. Signal quality of simultaneously recorded invasive and non-invasive EEG. Neuroimage, 46:708?716, 2009.\\n[11] G. Schalk, J. Kubanek, K. Miller, N. Anderson, E. Leuthardt, J. Ojemann, D. Limbrick,\\nD. Moran, L. Gerhardt, and J. Wolpaw. Decoding two-dimensional movement trajectories\\nusing electrocorticographic signals in humans. Journal of Neural engineering, 4:264, 2007.\\n[12] O. Bai, V. Rathi, P. Lin, D. Huang, H. Battapady, D. Y. Fei, L. Schneider, E. Houdayer, X. Chen,\\nand M. Hallett. Prediction of human voluntary movement before it occurs. Clinical Neurophysiology, 122:364?372, 2011.\\n[13] O. Bai, P. Lin, S. Vorbach, J. Li, S. Furlani, and M. Hallett. Exploration of computational\\nmethods for classification of movement intention during human voluntary movement from\\nsingle trial EEG. Clinical Neurophysiology, 118:2637?2655, 2007.\\n[14] U. Maoz, A. Arieli, S. Ullman, and C. Koch. Using single-trial EEG data to predict laterality\\nof voluntary motor decisions. Society for Neuroscience, 38:289.6, 2008.\\n[15] C. Camerer. Behavioral game theory: Experiments in strategic interaction. Princeton University Press, 2003.\\n[16] J. D. Haynes. Decoding and predicting intentions. Annals of the New York Academy of Sciences, 1224:9?21, 2011.\\n[17] P. Haggard. Decision time for free will. Neuron, 69:404?406, 2011.\\n[18] J. D. Haynes. Beyond libet. In W. Sinnott-Armstrong and L. Nadel, editors, Conscious will\\nand responsibility, pages 85?96. Oxford University Press, 2011.\\n[19] A. Muralidharan, J. Chae, and D. M. Taylor. Extracting attempted hand movements from EEGs\\nin people with complete hand paralysis following stroke. Frontiers in neuroscience, 5, 2011.\\n[20] E. Lew, R. Chavarriaga, S. Silvoni, and J. R. Milln. Detection of self-paced reaching movement\\nintention from EEG signals. Frontiers in Neuroengineering, 5:13, 2012.\\n\\n9\\n\\n\\f\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "import zipfile\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Open the zip file\n",
        "with zipfile.ZipFile(\"NIPS Papers.zip\", \"r\") as zip_ref:\n",
        "    # Extract the file to a temporary directory\n",
        "    zip_ref.extractall(\"temp\")\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "papers = pd.read_csv(\"papers.csv\")\n",
        "\n",
        "# Print head\n",
        "papers.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLn0HvULwaIC"
      },
      "source": [
        "** **\n",
        "#### Step 2: Data Cleaning <a class=\"anchor\\\" id=\"clean_data\"></a>\n",
        "** **\n",
        "\n",
        "Since the goal of this analysis is to perform topic modeling, let's focus only on the text data from each paper, and drop other metadata columns. Also, for the demonstration, we'll only look at 100 papers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "my note:\n",
        "\n",
        "Keeps only the main text content.\n",
        "\n",
        "Removes punctuation and converts text to lowercase."
      ],
      "metadata": {
        "id": "XgUln7p9K6QP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "7SmWSxQIwaIC",
        "outputId": "ad2435a4-116d-4f73-efa6-3c33e6a7943b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      year                                              title  \\\n",
              "6095  2016  Contextual semibandits via supervised learning...   \n",
              "5650  2016            Deep Learning without Poor Local Minima   \n",
              "4825  2014  Conditional Swap Regret and Conditional Correl...   \n",
              "261   1996                 Support Vector Regression Machines   \n",
              "5835  1992  Unsupervised Discrimination of Clustered Data ...   \n",
              "\n",
              "                                               abstract  \\\n",
              "6095  We study an online decision making problem whe...   \n",
              "5650  In this paper, we prove a conjecture published...   \n",
              "4825  We introduce a natural extension of the notion...   \n",
              "261                                    Abstract Missing   \n",
              "5835                                   Abstract Missing   \n",
              "\n",
              "                                             paper_text  \n",
              "6095  Contextual semibandits via supervised learning...  \n",
              "5650  Deep Learning without Poor Local Minima\\n\\nKen...  \n",
              "4825  Conditional Swap Regret and\\nConditional Corre...  \n",
              "261   Support Vector Regression Machines\\n\\nHarris D...  \n",
              "5835  Unsupervised Discrimination of Clustered Data\\...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c45fff69-f3e5-487d-8b6f-93d2cf7a4abe\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>paper_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6095</th>\n",
              "      <td>2016</td>\n",
              "      <td>Contextual semibandits via supervised learning...</td>\n",
              "      <td>We study an online decision making problem whe...</td>\n",
              "      <td>Contextual semibandits via supervised learning...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5650</th>\n",
              "      <td>2016</td>\n",
              "      <td>Deep Learning without Poor Local Minima</td>\n",
              "      <td>In this paper, we prove a conjecture published...</td>\n",
              "      <td>Deep Learning without Poor Local Minima\\n\\nKen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4825</th>\n",
              "      <td>2014</td>\n",
              "      <td>Conditional Swap Regret and Conditional Correl...</td>\n",
              "      <td>We introduce a natural extension of the notion...</td>\n",
              "      <td>Conditional Swap Regret and\\nConditional Corre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>261</th>\n",
              "      <td>1996</td>\n",
              "      <td>Support Vector Regression Machines</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Support Vector Regression Machines\\n\\nHarris D...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5835</th>\n",
              "      <td>1992</td>\n",
              "      <td>Unsupervised Discrimination of Clustered Data ...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Unsupervised Discrimination of Clustered Data\\...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c45fff69-f3e5-487d-8b6f-93d2cf7a4abe')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c45fff69-f3e5-487d-8b6f-93d2cf7a4abe button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c45fff69-f3e5-487d-8b6f-93d2cf7a4abe');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a4273ca8-839d-41b8-be4d-d517004da976\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a4273ca8-839d-41b8-be4d-d517004da976')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a4273ca8-839d-41b8-be4d-d517004da976 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "papers",
              "summary": "{\n  \"name\": \"papers\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9,\n        \"min\": 1987,\n        \"max\": 2016,\n        \"num_unique_values\": 29,\n        \"samples\": [\n          2004,\n          2009,\n          2006\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"FaceSync: A Linear Operator for Measuring Synchronization of Video Facial Images and Audio Tracks\",\n          \"Unmixing Hyperspectral Data\",\n          \"Temporal Difference Learning of Position Evaluation in the Game of Go\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 47,\n        \"samples\": [\n          \"In this paper we present a new algorithm for computing a low rank approximation of the product $A^TB$ by taking only a single pass of the two matrices $A$ and $B$. The straightforward way to do this is to (a) first sketch $A$ and $B$ individually, and then (b) find the top components using PCA on the sketch. Our algorithm in contrast retains additional summary information about $A,B$ (e.g. row and column norms etc.) and uses this additional information to obtain an improved approximation from the sketches. Our main analytical result establishes a comparable spectral norm guarantee to existing two-pass methods; in addition we also provide results from an Apache Spark implementation that shows better computational and statistical performance on real-world and synthetic evaluation datasets.\",\n          \"Latent SVMs (LSVMs) are a class of powerful tools that have been successfully applied to many applications in computer vision. However, a limitation of LSVMs is that they rely on linear models. For many computer vision tasks, linear models are suboptimal and nonlinear models learned with kernels typically perform much better. Therefore it is desirable to develop the kernel version of LSVM. In this paper, we propose kernel latent SVM (KLSVM) -- a new learning framework that combines latent SVMs and kernel methods. We develop an iterative training algorithm to learn the model parameters. We demonstrate the effectiveness of KLSVM using three different applications in visual recognition. Our KLSVM formulation is very general and can be applied to solve a wide range of applications in computer vision and machine learning.\",\n          \"Neural codes are inevitably shaped by various kinds of biological constraints, \\\\emph{e.g.} noise and metabolic cost. Here we formulate a coding framework which explicitly deals with noise and the metabolic costs associated with the neural representation of information, and analytically derive the optimal neural code for monotonic response functions and arbitrary stimulus distributions. For a single neuron, the theory predicts a family of optimal response functions depending on the metabolic budget and noise characteristics. Interestingly, the well-known histogram equalization solution can be viewed as a special case when metabolic resources are unlimited. For a pair of neurons, our theory suggests that under more severe metabolic constraints, ON-OFF coding is an increasingly more efficient coding scheme compared to ON-ON or OFF-OFF. The advantage could be as large as one-fold, substantially larger than the previous estimation. Some of these predictions could be generalized to the case of large neural populations. In particular, these analytical results may provide a theoretical basis for the predominant segregation into ON- and OFF-cells in early visual processing areas. Overall, we provide a unified framework for optimal neural codes with monotonic tuning curves in the brain, and makes predictions that can be directly tested with physiology experiments.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"paper_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"FaceSync: A linear operator for measuring\\nsynchronization of video facial images and\\naudio tracks\\n\\nMalcolm Slaney!\\nInterval Research\\nmalcolm@ieee.org\\n\\nMichele Covell2\\nInterval Research\\ncovell@ieee.org\\n\\nAbstract\\nFaceSync is an optimal linear algorithm that finds the degree of synchronization between the audio and image recordings of a human\\nspeaker. Using canonical correlation, it finds the best direction to combine all the audio and image data, projecting them onto a single axis.\\nFaceSync uses Pearson's correlation to measure the degree of synchronization between the audio and image data. We derive the optimal linear\\ntransform to combine the audio and visual information and describe an\\nimplementation that avoids the numerical problems caused by computing the correlation matrices.\\n\\n1 Motivation\\nIn many applications, we want to know about the synchronization between an audio signal\\nand the corresponding image data. In a teleconferencing system, we might want to know\\nwhich of the several people imaged by a camera is heard by the microphones; then, we can\\ndirect the camera to the speaker. In post-production for a film, clean audio dialog is often\\ndubbed over the video; we want to adjust the audio signal so that the lip-sync is perfect.\\nWhen analyzing a film, we want to know when the person talking is in the shot, instead of\\noff camera. When evaluating the quality of dubbed films, we can measure of how well the\\ntranslated words and audio fit the actor's face.\\nThis paper describes an algorithm, FaceSync, that measures the degree of synchronization\\nbetween the video image of a face and the associated audio signal. We can do this task by\\nsynthesizing the talking face, using techniques such as Video Rewrite [1], and then comparing the synthesized video with the test video. That process, however, is expensive. Our\\nsolution finds a linear operator that, when applied to the audio and video signals, generates\\nan audio-video-synchronization-error signal. The linear operator gathers information\\nfrom throughout the image and thus allows us to do the computation inexpensively.\\nHershey and Movellan [2] describe an approach based on measuring the mutual information between the audio signal and individual pixels in the video. The correlation between\\nthe audio signal, x, and one pixel in the image y, is given by Pearson's correlation, r. The\\nmutual information between these two variables is given by f(x,y) = -1/2 log(l-?). They\\ncreate movies that show the regions of the video that have high correlation with the audio;\\n1. Currently at IBM Almaden Research, 650 Harry Road, San Jose, CA 95120.\\n2. Currently at Yes Video. com, 2192 Fortune Drive, San Jose, CA 95131.\\n\\n\\fStandard Deviation of Testing Data\\n\\nFaceSync\\n<l1li\\n\\n1 6T~S\\n\\n~\\n\\ntt -\\n\\n~.\\n\\n16T8\\n\\n50\\n10\\n40\\n20\\n30\\n30\\n20\\n40\\n\\n10\\n\\n50\\n20\\n\\n40\\n\\n60\\n\\n80\\n\\nFigure 1: Connections between linear models Figure 2: Standard deviation of the\\nrelating audio, video and fiduciary points\\naligned facial images used to create the\\ncanonical model.\\nfrom the correlation data, they estimate the centroid of the activity pattern and find the\\ntalking face. They make no claim of their algorithms ability to measure synchronization.\\nFaceSync is an optimal linear detector, equivalent to a Wiener filter [3], which combines\\nthe information from all the pixels to measure audio-video synchronization. We developed\\nour approach based on two surprisingly simple algorithms in computer-vision and audiovisual speech synthesis: EigenPoints [4] and ATR's multilinear facial synthesizer [5]. The\\nrelationship of these two algorithms to each other and to our problem is shown in Figure 1.\\nEigenPoints [4] is an algorithm that finds a linear mapping between the brightness of a\\nvideo signal and the location of fiduciary points on the face. At first, the validity of this\\nmapping is not obvious; we might not expect the brightness of pixels on a face to covary\\nlinearly with x and y coordinates. It turns out, however, that the brightness of the image\\npixels, i(x,y), and the location of fiduciary points such as the comer of the mouth, Pi =(Xi'\\ny), describe a function in a high-dimensional space. In the absence of occlusion, the combined brightness-fiduciary function is smoothly varying. Thus the derivatives are defined\\nand a Taylor-series approximation is valid. The real surprise is that EigenPoints can find a\\nlinear approximation that describes the brightness-fiduciary space, and this linear approximation is valid over a useful range of brightness and control-point changes.\\nSimilarly, Yehia, Rubin, and Vatikiotis-Bateson at ATR [5] have shown that it is possible to\\nconnect a specific model of speech, the line-spectral pairs or LSP, with the position of\\nfiduciary points on the face. Their multilinear approximation yielded an average correlation of 0.91 between the true facial locations and those estimated from the audio data.\\nWe derive a linear approximation to connect brightness to audio without the intermediate\\nfiduciary points. Neither linear mapping is exact, so we had to determine whether the\\ndirect path between brightness and audio could be well approximated by a linear transform. We describe FaceSync in the next section.\\nFisher and his colleagues [6] describe a more general approach that finds a non-linear\\nmapping onto subspaces which maximize the mutual information. They report results\\nusing a single-layer perceptron for the non-linear mapping.\\n\\n2 FaceSync Algorithm\\nFaceSync uses a face-recognition algorithm and canonical correlation to measure audiovisual synchrony. There are two steps: training or building the canonical correlation\\nmodel, and evaluating the fit of the model to the data. In both steps we use face-recognition software to find faces and align them with a sample face image. In the training stage,\\ncanonical correlation finds a linear mapping that maximizes the cross-correlation between\\n\\n\\ftwo signals: the aligned face image and the audio signal. Finally, given new audio and\\nvideo data, we use the linear mapping to rotate a new aligned face and the audio signal\\ninto a common space where we can evaluate their correlation as a function of time.\\nIn both training and testing, we use a neural-network face-detection algorithm [7] to find\\nportions of the image that contain a face. This approach uses a pyramid of images to\\nsearch efficiently for pixels that look like faces. The software also allows the face to be\\ntracked through a sequence of image and thus reduce the computational overhead, but we\\ndid not use this capability in our experiments. The output of Rowley's face-detection algorithm is a rectangle that encloses the position of a face. We use this information to align\\nthe image data prior to correlational analysis.\\nWe investigated a number of ways to describe the audio signal. We looked at mel-frequency cepstral coefficients (MFCC) [8], linear-predictive coding (LPC) [8], line spectral\\nfrequencies (LSF) [9], spectrograms, and raw signal energy. For most calculations, we\\nused MFCC analysis, because it is a favorite front-end for speech-recognition systems\\nand, as do several of the other possibilities, it throws away the pitch information. This is\\nuseful because the pitch information affects the spectrogram in a non-linear manner and\\ndoes not show up in the image data. For each form of audio analysis, we used a window\\nsize that was twice the frame interval (2/29.97 seconds,)\\nCanonical correlation analysis (CCA) uses jointly varying data from an input subspace Xi\\nand an output subspace Yi to find canonic correlation matrices, A x and A y . These matrices whiten the input and output data, as well as making the cross correlation diagonal and\\n\\\"maximally compact.\\\" Specifically, the whitened data matrices are\\n\\n11 = A: (x and have the following properties:\\n\\nx) and\\n\\ncp\\n\\n= A~ (y -\\n\\ny),\\n\\n(1)\\n\\nE{l1l1 T } = I, E{cpcpT} = I, E{<P11T} = LK = diag{cr 1,\\n\\ncr 2 ,\\n\\n... , cr L },\\n\\n(2)\\n\\nwhere 1 ::\\\": cr 1 ::\\\": cr 2 ::\\\": ... > 0 and cr M + 1 = ... = cr L = O. In addition, for i starting from\\n1 and then repeating up to L, cr i is the largest possible correlation between 11i and\\n<Pi (where 11i and <Pi are the ilh elements of 11 and <P respectively), given the norm and\\northogonality constraints on 11 and <p, expressed in equation 2. We refer to this property\\nas maximal compaction, since the correlation is (recursively) maximally compacted into\\nthe leading elements of 11 and <p.\\nWe find the matrices\\n\\nAx\\n\\nand\\n\\nx,\\n\\n= R-xxl 12 ( X -\\n\\nAy\\n\\nby whitening the input and output data:\\nx-) andy'\\n\\n= R -yy1I2 ( y -\\n\\n(3)\\n\\ny-)\\n\\nand then finding the left (U) and right (V) singular vectors of the cross-correlation matrix\\nbetween the whitened data\\n- 112\\n-112\\nT\\nK=Ry'x?=R yy RyxRxx\\n=UKLKVK .\\n\\n(4)\\n\\nThe SVD gives the same type of maximal compaction that we need for the cross correlation matrices, A x and A y ' Since the SVD is unique up to sign changes (and a couple of\\nother degeneracies assocIated with repeated singular values), A x and A x must be:\\n- 1/ 2\\n- 1/ 2\\nAx = Rxx V K and Ay = Ryy UK'\\n\\n(5)\\n\\nWe can verify this by calculating E { <P11 T} using the definitions of <P and 11 .\\nT\\n_\\n- 112\\nT\\n_\\nT - 112\\n_\\nA}'(y-y) = (R yy UK) (y-y) = UKR yy (y-y),\\nT\\n_\\n- 112\\nT\\n_\\nT - 112\\n_\\n= Ax(x-x) = (Rxx V K ) (x-x) = VKRxx (x-x),\\n\\n<P =\\n\\n(6)\\n\\n11\\n\\n(7)\\n\\nthen note\\nT\\n\\nE{<P11 } =\\n\\nT - 112\\nT\\n- 112\\nUKR yy E{yx }Rxx V K\\n\\nT\\n\\n- 112\\n\\n- 112\\n\\n= UKR yy RyxRxx\\n\\nVK\\n\\n(8)\\n\\n\\fand then by using equation 4 (twice)\\nT\\n\\nE{<Pl1 }\\n\\nT\\nT\\nT\\nT\\nT\\n= UKKV\\nK = UK(UKLKVK)VK = (UKUK)LK(VKVK) = L K .\\n\\n(9)\\n\\nThis derivation of canonical correlation uses correlation matrices. This introduces a wellknown problem due to doubling the dynamic range of the analysis data. Instead, we formulate the estimation equations in terms of the components of the SVDs of the training\\ndata matrices. Specifically, we take the SVDs of the zero-mean input and output matrices:\\n[x 1 -x ... x N -x] = IN-IUxLxV~'[YI-Y'''YN-Y] = IN-IUyLyV~.(10)\\nFrom these two decompositions, we can write the two correlation matrices as\\n2\\n\\nT\\n\\n- 112\\n\\nRxx = UxLxUx\\nRyy = U yL 2y U yT\\n\\n-1\\n\\nT\\n\\n(11)\\n\\nRxx = UxLx Ux '\\nR-yy1I2 = U yL-y 1 U yT\\n\\n(12)\\n\\nand then write the cross-correlation matrix as\\nT T\\n\\nRyx = UyLyV y VxLxUx'\\nUsing these expressions for the correlation matrices, the K matrix becomes\\nK = (UyL\\n\\n-1\\n\\nT\\n\\nT\\n\\nT\\n\\n-1\\n\\nT\\n\\nT\\n\\n(13)\\nT\\n\\nyU y )(UyL1VY VxLxUx)(UxLx U x ) = UyV y VxU x '\\n\\n(14)\\n\\nNow let's look at the quantity U y K U x in terms of its SVD\\nT\\n\\nT\\n\\nT\\n\\nT\\n\\nT\\n\\nUyKU x= Vy Vx = (UyUK)LK(VKU) = UUKULKVUKU'\\n\\n(15)\\n\\nand, due to the uniqueness of the SVD, note\\nT\\nT\\n(16)\\nUyU k = U UKU and UxV K = V UKU ?\\nNow we can rewrite the equation for A x to remove the need for the squaring operation\\nAx\\n\\n- 1/ 2\\n-1\\nT\\n-1\\n= Rxx\\nV K = UxLx (U x V K) = UxLx V UKU\\n\\n(17)\\n\\nand similarly for Ay\\nAy\\n\\n- 112\\n-1\\nT\\n-1\\n= Ryy\\nU K = UyL y (UyU K ) = UyL y UUKU'\\n\\n(18)\\n\\nUsing these identities, we compute A x and Ay using the following steps:\\n1)\\n\\nFind the SVDs of the data matrices using the expressions in equation 10.\\n\\n2) Form a rotated version of the cross-correlation matrix K and computes its SVD using\\nequation 14.\\n3)\\n\\nCompute the A x and Ay matrixes using equations 17 and 18.\\n\\nGiven the linear mapping between audio data and the video images, as described by the\\nA x and A matrices, we measure the correlation between these two sets of data. For each\\ncandidate face in the image, we rotate the audio data by the first column of Ax ' rotate the\\nface image by the first column of A , and then compute Pearson's correlation of the\\nrotated audio and video data. We use the absolute value of this correlation coefficient as a\\nmeasure of audio-video synchronization.\\n\\n3 Results\\nWe evaluated the performance of the FaceSync algorithm using a number of tests. In the\\nsimplest tests we measured FaceSync's sensitivity to small temporal shifts between the\\naudio and the video signals, evaluated our performance as a function of testing-window\\nsize and looked at different input representations. We also measured the effect of coarticulation.\\nTo train the FaceSync system, we used 19 seconds of video. We used Rowley's face-detection software to find a rectangle bounding the face but we noticed a large amount (several\\n\\n\\f-5r--------,,---------,,-----,\\n\\nAN Sync with MFCC Analysis (testing data)\\n\\n0.5 ,-------'-----,r-----.----''--\\\"'--r--=---'--,\\n\\n0.4\\n0.3\\n\\n-10\\n0.2\\n0 .1\\n-15 4~-----:'-------:8'-------:-'.10\\nRotated Audio Data\\n\\nFigure 3: Optimum projections of the\\naudio and video signals that maximize\\ntheir cross-correlation_\\n\\n~0~0---~5~0-~~0-~-5~0~-~100\\nFrame Offset\\n\\nFigure 4: Correlation of audio and video\\ndata as the audio data is shifted in time\\npast the video. (29.97 frames/sec.)\\n\\npixels) of jitter in the estimated positions. Figure 2 shows the standard deviation of our\\naligned facial data. The standard deviation is high along the edges of the face, where small\\namounts of motion have a dramatic effect on the brightness, and around the mouth, where\\nthe image brightness changes with the spoken sounds.\\nFigure 3 shows the results of the canonical-correlation analysis for the 7 (distinct) seconds\\nof audio and video that we used for testing. Canonical correlation has rotated the two multidimensional signals (audio and image) into the directions that are maximally correlated\\nwith each other. Note that the transformed audio and image signals are correlated.\\nWe can evaluate the quality of these results by looking at the correlation of the two sets of\\ndata as the audio and image data are shifted relative to each other (such shifts are the kinds\\nof errors that you would expect to see with bad lip sync.) An example of such a test is\\nshown in Figure 4. Note that, after only a few frames of shift (about lOOms), the correlation between the audio and image data declined to close to zero.\\nWe used the approach described by Hershey and Movellan to analyze which parts of the\\nfacial image correlate best with the audio data. In their work, they computed correlations\\nover 16 frame intervals. Since we used aligned data, we could measure accurately the correlations over our entire 9 second test sequence. Our results are shown in Figure 5: Each\\npixel shows the correlation that we found using our data. This approach looks at each pixel\\nindividually and produces a maximum correlation near 0.45. Canonical correlation, which\\naccumulates all the pixel information from all over the image, also produces a maximum\\ncorrelation near 0.45, but by accumulating information from all over the image it allows us\\nto measure sychronization without integrating over the full 9 seconds.\\nFigure 6 shows FaceSync's ability to measure audio-visual synchronization as we varied\\nthe testing-window size. For short windows (less than 1.3 seconds), we had insufficient\\ndata to measure the correlation accurately. For long windows (greater than 2.6 seconds),\\nwe had sufficient data to average and minimize the effect of errors, but as a result did not\\nhave high time resolution. As shown in Figure 5, there is a peak in the correlation near 0\\nframe offset; there are often, however, large noise peaks at other shifts. Between 1.3 and\\n2.6 seconds of video produces reliable results.\\nDifferent audio-analysis techniques provide different information to the FaceSync algorithm. Figure 7 shows the audio-video synchronization correlation, similar to Figure 3, for\\nseveral different kinds of analysis. LPC and LSF produced identical narrow peaks; MFCC\\nproduced a slightly lower peak. Hershey used the power from the spectrogram in his algorithm to detect the visual motion. However, our result for spectrogram data is in the noise,\\nindicating that a linear model can not use spectrogram data for fine-grain temporal measurements.\\n\\n\\fCorrelation between audio energy and video pi xels (r)\\n\\n20 Frame Wi ndows\\n\\n40 Frame Windows\\n\\n80 Frame Wi ndows\\n\\n160 Frame Windows\\n\\n0.45\\n0.4\\n\\n10\\n\\n0.35\\n0.3\\n\\n20\\n\\n0.25\\n30\\n\\n0.2\\n\\n0.15\\n40\\n\\n0.1\\n0.05\\n\\n50\\n20\\n\\n40\\n\\n60\\n\\n80\\n\\nFigure 5: Correlation of each separate pixel\\nand audio energy over the entire 9 second\\ntest sequence [2].\\n\\n~5:~ ~5: [3\\n\\n50E::] 50c::]\\n100 200 300 400\\nFrame Number\\n\\n1 00 200 300\\nFrame Number\\n\\nFigure 6: Perfonnance of the FaceSync\\nalgorithm as a function of test window\\nlength. We would like to see a large peak\\n(dark line) for all frames at zero shift.\\n\\nWe also looked at FaceSync's perfonnance when we enhanced the video model with temporal context. Normally, we use one image frame and 67 ms of audio data as our input and\\noutput data. For this experiment, we stacked 13 images to fonn the input to the canonicalcorrelation algorithm Our perfonnance did not vary as we added more visual context,\\nprobably indicating that a single image frame contained all of the infonnation that the linear model was able to capture.\\nAs the preceding experiment shows, we did not improve the performance by adding more\\nimage context. We can, however, use the FaceSync framework with extended visual context to learn something about co-articulation. Coarticulation is a well-known effect in\\nspeech; the audio and physical state of the articulators not only depends on the current\\nphoneme, but also on the past history of the phonemic sequence and on the future sounds.\\nWe let canonical correlation choose the most valuable data, across the range of shifted\\nvideo images. Summing the squared weighting terms gives us an estimate of how much\\nweight canonical correlation assigned to each shifted frame of data. Figure 8 shows that\\none video frame (30ms) before the current audio frame, and four video frames (120ms)\\nafter the current audio are affected by coarticulation. Interestingly, the zero-shift frame is\\nnot the one that shows the maximum importance. Instead, the frames just before and after\\nare more heavily weighted.\\n\\n4 Conclusions\\nWe have described an algorithm, FaceSync, that builds an optimal linear model connecting the audio and video recordings of a person's speech. The model allows us to measure\\nthe degree of synchronization between the audio and video, so that we can, for example,\\ndetermine who is speaking or to what degree the audio and video are sychronized.\\nWhile the goal of Hershey's process is not a temporal synchronization measurement, it is\\nstill interesting to compare the two approaches. Hershey's process does not take into\\naccount the mutual information between adjacent pixels; rather, it compares mutual information for individual pixels, then combines the results by calculating the centroid. In contrast, FaceSync asks what combination of audio and image data produces the best possible\\ncorrelation, thus deriving a single optimal answer. Although the two algorithms both use\\nPearson's correlation to measure sychronization, FaceSync combines the pixels of the face\\nand the audio information in an optimal detector.\\nThe performance of the FaceSync algorithm is dependent on both training and testing data\\nsizes. We did not test the quality of our models as we varied the training data. We do the\\ntraining calculation only once using all the data we have. Most interesting applications of\\n\\n\\fo.s\\n\\n4\\n\\ni----,-----r;:=======i\\\"]\\nMFCC\\nLPC\\nLSF\\nSpectrogram\\nPower\\n\\n0.4\\n\\n0.3\\n\\n3 .S\\n>-\\n\\n~\\n\\nQl\\n\\n3\\n\\ns6mmed weight for each frame position\\n\\nVideo in\\npast\\n\\nVideo in\\nfuture\\n\\n<:\\n\\nW\\n\\no>2.S\\n<:\\n\\nE\\n0>\\n\\n0.2\\n\\n.ijj\\n\\n:;=\\n0 .1\\n\\n2\\nI.S\\n\\no '--_....l!:.\\\"--'.!!....lI.:\\\"\\\"-\\\"\\\"\\\"\\n-100\\n-so\\no\\nFrame Offset\\n\\nso\\n\\n100\\n\\n1\\n-40\\n\\n-20\\n\\n0\\n\\n20\\n\\n40\\n\\nDelta Frame\\n\\nFigure 7: Performance of the FaceSync algo- Figure 8: Contributions of different frames\\nrithm for different kinds of input representa- to the optimum correlation with the audio\\ntions.\\nframe\\nFaceSync depend on the testing data, and we would like to know how much data is necessary to make a decision.\\nIn our FaceSync application, we have more dimensions (pixels in the image) than examples (video frames) . Thus, our covariance matrices are singular, making their inversionwhich we do as part of canonical correlation - problematic. We address the need for a\\npseudo-inverse, while avoiding the increased dynamic range of the covariance matrices,\\nby using an SVD on the (un squared) data matrices themselves (in place of an eigendecomposition of the covariance matrices).\\nWe demonstrated high linear correlations between the audio and video signals, after we\\nfirst found the optimal projection direction by using canonical correlation. We evaluated\\nthe FaceSync algorithm by measuring the correlation between the audio and video signals\\nas we shift the audio data relative to the image data. MFCC, LPC, and LSF all produce\\nsharp correlations as we shift the audio and images, whereas speech power and spectrograms produce no correlation peak at all.\\n\\nReferences\\n[1] C. Bregler, M. Covell, M. Slaney. \\\"Video Rewrite: Driving visual speech with audio.\\\" Proc. SIGGRAPH 97, Los Angeles, CA, pp. 353- 360, August 1997.\\n[2] J. Hershey, J. R. Movellan. \\\"Audio-Vision: Locating sounds via audio-visual synchrony.\\\"\\nAdvances in Neural Information Processing Systems 12, edited by S. A. Solla, T. K. Leen, K-R.\\nMi.iller. MIT Press, Cambridge, MA (in press).\\n[3] L. L. Scharf, John K. Thomas. \\\"Wiener filters in canonical coordinates for transform coding, filtering and quantizing.\\\" IEEE Transactions on Signal Processing, 46(3), pp. 647- 654, March 1998.\\n[4] M. Covell , C. Bregler. \\\"Eigenpoints.\\\" Proc. Int. Con! Image Processing, Lausanne, Switzerland,\\nVol. 3,pp.471-474, 1996.\\n[5] H. C. Yehia, P. E. Rubin, E. Vatikiotis-Bateson. \\\"Quantitative association of vocal-tract and facial\\nbehavior,\\\" Speech Communication, 26, pp. 23-44, 1998.\\n[6] J. W. Fisher III, T. Darrell, W. T. Freeman, P. Viola. \\\"Learning Joint Statistical Models for AudioVisual Fusion and Segregation,\\\" This volume, 2001.\\n[7] H. A. Rowley, S. Baluja, and T. Kanade. \\\"Neural network- based face detection.\\\" IEEE Transa ctions on Pattern Analysis and Machine Intelligence, 20(1), pp. 23- 38, January 1998.\\n[8] L. Rabiner, B. Juang. Fundamentals of Speech Recognition. Prentice Hall, Englewood Cliffs,\\nNew Jersey, 1993.\\n[9] N. Sugamura, F. Itakura, \\\"Speech analysis and synthesis methods developed at ECL in NTTFrom LPC to LSP.\\\" Speech Communications, 4(2), June 1986.\\n\\n\\f\",\n          \"U nmixing Hyperspectral Data\\n\\nLucas Parra, Clay Spence, Paul Sajda\\nSarnoff Corporation, CN-5300, Princeton, NJ 08543, USA\\n{lparra, cspence,psajda} @sarnoff.com\\n\\nAndreas Ziehe, Klaus-Robert Miiller\\nGMD FIRST.lDA, Kekulestr. 7, 12489 Berlin, Germany\\n{ziehe,klaus}@first.gmd.de\\n\\nAbstract\\nIn hyperspectral imagery one pixel typically consists of a mixture\\nof the reflectance spectra of several materials, where the mixture\\ncoefficients correspond to the abundances of the constituting materials. We assume linear combinations of reflectance spectra with\\nsome additive normal sensor noise and derive a probabilistic MAP\\nframework for analyzing hyperspectral data. As the material reflectance characteristics are not know a priori, we face the problem\\nof unsupervised linear unmixing. The incorporation of different\\nprior information (e.g. positivity and normalization of the abundances) naturally leads to a family of interesting algorithms, for\\nexample in the noise-free case yielding an algorithm that can be\\nunderstood as constrained independent component analysis (ICA).\\nSimulations underline the usefulness of our theory.\\n\\n1\\n\\nIntroduction\\n\\nCurrent hyperspectral remote sensing technology can form images of ground surface\\nreflectance at a few hundred wavelengths simultaneously, with wavelengths ranging\\nfrom 0.4 to 2.5 J.Lm and spatial resolutions of 10-30 m. The applications of this\\ntechnology include environmental monitoring and mineral exploration and mining.\\nThe benefit of hyperspectral imagery is that many different objects and terrain\\ntypes can be characterized by their spectral signature.\\nThe first step in most hyperspectral image analysis systems is to perform a spectral\\nunmixing to determine the original spectral signals of some set of prime materials.\\nThe basic difficulty is that for a given image pixel the spectral reflectance patterns\\nof the surface materials is in general not known a priori. However there are general physical and statistical priors which can be exploited to potentially improve\\nspectral unmixing. In this paper we address the problem of unmixing hyperspectral\\nimagery through incorporation of physical and statistical priors within an unsupervised Bayesian framework.\\nWe begin by first presenting the linear superposition model for the reflectances\\nmeasured. We then discuss the advantages of unsupervised over supervised systems.\\n\\n\\fUnmixing Hyperspectral Data\\n\\n943\\n\\nWe derive a general maximum a posteriori (MAP) framework to find the material\\nspectra and infer the abundances. Interestingly, depending on how the priors are\\nincorporated, the zero noise case yields (i) a simplex approach or (ii) a constrained\\nleA algorithm. Assuming non-zero noise our MAP estimate utilizes a constrained\\nleast squares algorithm. The two latter approaches are new algorithms whereas the\\nsimplex algorithm has been previously suggested for the analysis of hyperspectral\\ndata.\\nLinear Modeling To a first approximation the intensities X (Xi>.) measured in\\neach spectral band A = 1, ... , L for a given pixel i = 1, ... , N are linear combinations of the reflectance characteristics S (8 m >.) of the materials m = 1, ... , M\\npresent in that area. Possible errors of this approximation and sensor noise are\\ntaken into account by adding a noise term N (ni>'). In matrix form this can be\\nsummarized as\\n\\nX = AS + N, subject to: AIM = lL,\\n\\nA\\n\\n~\\n\\n0,\\n\\n(1)\\n\\nwhere matrix A (aim) represents the abundance of material m in the area corresponding to pixel i, with positivity and normalization constraints. Note that\\nground inclination or a changing viewing angle may cause an overall scale factor for\\nall bands that varies with the pixels. This can be incorporated in the model by simply replacing the constraint AIM = lL with AIM ~ lL which does does not affect\\nthe discussion in the remainder of the paper. This is clearly a simplified model of\\nthe physical phenomena. For example, with spatially fine grained mixtures, called\\nintimate mixtures, multiple reflectance may causes departures from this first order model. Additionally there are a number of inherent spatial variations in real\\ndata, such as inhomogeneous vapor and dust particles in the atmosphere, that will\\ncause a departure from the linear model in equation (1). Nevertheless, in practical\\napplications a linear model has produced reasonable results for areal mixtures.\\nSupervised vs. Unsupervised techniques Supervised spectral un mixing relies on the prior knowledge about the reflectance patterns S of candidate surface\\nmaterials, sometimes called endmembers, or expert knowledge and a series of semiautomatic steps to find the constituting materials in a particular scene. Once the\\nuser identifies a pixel i containing a single material, i.e. aim = 1 for a given m and\\ni, the corresponding spectral characteristics of that material can be taken directly\\nfrom the observations, i.e., 8 m >. = Xi>. [4]. Given knowledge about the endmembers\\none can simply find the abundances by solving a constrained least squares problem.\\nThe problem with such supervised techniques is that finding the correct S may require substantial user interaction and the result may be error prone, as a pixel that\\nactually contains a mixture can be misinterpreted as a pure endmember. Another\\napproach obtains endmembers directly from a database. This is also problematic\\nbecause the actual surface material on the ground may not match the database entries, due to atmospheric absorption or other noise sources. Finding close matches\\nis an ambiguous process as some endmembers have very similar reflectance characteristics and may match several entries in the database.\\n\\nUnsupervised unmixing, in contrast, tries to identify the endmembers and mixtures\\ndirectly from the observed data X without any user interaction. There are a variety\\nof such approaches. In one approach a simplex is fit to the data distribution [7, 6, 2].\\nThe resulting vertex points of the simplex represent the desired endmembers, but\\nthis technique is very sensitive to noise as a few boundary points can potentially\\nchange the location of the simplex vertex points considerably. Another approach by\\nSzu [9] tries to find abundances that have the highest entropy subject to constraints\\nthat the amount of materials is as evenly distributed as possible - an assumption\\n\\n\\fL. Parra, C. D. Spence, P Sajda, A. Ziehe and K.-R. Muller\\n\\n944\\n\\nwhich is clearly not valid in many actual surface material distributions. A relatively\\nnew approach considers modeling the statistical information across wavelength as\\nstatistically independent AR processes [1]. This leads directly to the contextual\\nlinear leA algorithm [5]. However, the approach in [1] does not take into account\\nconstraints on the abundances, noise, or prior information. Most importantly, the\\nmethod [1] can only integrate information from a small number of pixels at a time\\n(same as the number of endmembers). Typically however we will have only a few\\nendmembers but many thousand pixels.\\n\\n2\\n2.1\\n\\nThe Maximum A Posterior Framework\\nA probabilistic model of unsupervised spectral unmixing\\n\\nOur model has observations or data X and hidden variables A, S, and N that\\nare explained by the noisy linear model (1). We estimate the values of the hidden\\nvariables by using MAP\\n(A SIX)\\n\\np\\n\\n,\\n\\n= p(XIA, S)p(A, S) = Pn(XIA, S)Pa(A)ps(S)\\np(X)\\n\\np(X)\\n\\n(2)\\n\\nwith Pa(A), Ps(S), Pn(N) as the a priori assumptions of the distributions. With\\nMAP we estimate the most probable values for given priors after observing the data,\\nA MAP , SMAP = argmaxp(A, SIX)\\n\\n(3)\\n\\nA,S\\n\\nNote that for maximization the constant factor p(X) can be ignored. Our first assumption, which is indicated in equation (2) is that the abundances are independent\\nof the reflectance spectra as their origins are completely unrelated: (AO) A and S\\nare independent.\\nThe MAP algorithm is entirely defined by the choices of priors that are guided by\\nthe problem of hyperspectral unmixing: (AI) A represent probabilities for each\\npixel i. (A2) S are independent for different material m. (A3) N are normal i.i.d.\\nfor all i, A. In summary, our MAP framework includes the assumptions AO-A3.\\n2.2\\n\\nIncluding Priors\\n\\nPriors on the abundances\\nbe represented as,\\n\\nPositivity and normalization of the abundances can\\n\\n(4)\\n\\neo\\n\\nwhere 60 represent the Kronecker delta function and\\nthe step function. With\\nthis choice a point not satisfying the constraint will have zero a posteriori probability. This prior introduces no particular bias of the solutions other then abundance\\nconstraints. It does however assume the abundances of different pixels to be independent.\\nPrior on spectra Usually we find systematic trends in the spectra that cause\\nsignificant correlation. However such an overall trend can be subtracted and/or\\nfiltered from the data leaving only independent signals that encode the variation\\nfrom that overall trend. For example one can capture the conditional dependency\\nstructure with a linear auto-regressive (AR) model and analyze the resulting \\\"innovations\\\" or prediction errors [3]. In our model we assume that the spectra represent\\nindependent instances of an AR process having a white innovation process em.>. distributed according to Pe(e). With a Toeplitz matrix T of the AR coefficients we\\n\\n\\f945\\n\\nUnmixing Hyperspectral Data\\n\\ncan write, em = Sm T. The AR coefficients can be found in a preprocessing step on\\nthe observations X. If S now represents the innovation process itself, our prior can\\nbe represented as,\\nM\\n\\nPe (S)\\n\\n<X\\n\\nPe(ST) =\\n\\nL\\n\\nL\\n\\nII II Pe( L\\n\\nsm>.d>.>.,) ,\\n\\n(5)\\n\\nm=1 >.=1\\n>.'=1\\nAdditionally Pe (e) is parameterized by a mean and scale parameter and potentially\\nparameters determining the higher moments of the distributions. For brevity we\\nignore the details of the parameterization in this paper.\\n\\nPrior on the noise As outlined in the introduction there are a number of problems that can cause the linear model X = AS to be inaccurate (e.g. multiple\\nreflections, inhomogeneous atmospheric absorption, and detector noise.) As it is\\nhard to treat all these phenomena explicitly, we suggest to pool them into one noise\\nvariable that we assume for simplicity to be normal distributed with a wavelength\\ndependent noise variance a>.,\\nL\\n\\np(XIA, S)\\n\\n= Pn(N) = N(X -\\n\\nAS,~)\\n\\n= II N(x>.\\n\\n- As>., a>.l) ,\\n\\n(6)\\n\\n>.=1\\nwhere N (', .) represents a zero mean Gaussian distribution, and 1 the identity matrix\\nindicating the independent noise at each pixel.\\n2.3\\n\\nMAP Solution for Zero Noise Case\\n\\nLet us consider the noise-free case. Although this simplification may be inaccurate it\\nwill allow us to greatly reduce the number of free hidden variables - from N M + M L\\nto M2 . In the noise-free case the variables A, S are then deterministically dependent\\non each other through a N L-dimensional 8-distribution, Pn(XIAS) = 8(X - AS).\\nWe can remove one of these variables from our discussion by integrating (2). It is\\ninstructive to first consider removing A\\np(SIX)\\n\\n<X\\n\\nI\\n\\ndA 8(X - AS)Pa(A)ps(S) = IS- 1IPa(XS- 1 )Ps(S).\\n\\n(7)\\n\\nWe omit tedious details and assume L = M and invertible S so that we can perform\\nthe variable substitution that introduces the Jacobian determinant IS-II . Let us\\nconsider the influence of the different terms. The Jacobian determinant measures\\nthe volume spanned by the endmembers S. Maximizing its inverse will therefore try\\nto shrink the simplex spanned by S. The term Pa(XS- 1 ) should guarantee that all\\ndata points map into the inside of the simplex, since the term should contribute zero\\nor low probability for points that violate the constraint. Note that these two terms,\\nin principle, define the same objective as the simplex envelope fitting algorithms\\npreviously mentioned [2].\\nIn the present work we are more interested in the algorithm that results from\\nremoving S and finding the MAP estimate of A. We obtain (d. Eq.(7))\\n\\np(AIX) oc\\n\\nI\\n\\ndS 8(X - AS)Pa(A)ps(S) = IA -llps(A- 1 X)Pa(A).\\n\\n(8)\\n\\nFor now we assumed N = M. 1 If Ps (S) factors over m , i.e. endmembers are independent, maximizing the first two terms represents the leA algorithm. However,\\nlIn practice more frequently we have N > M. In that case the observations X can be\\nmapped into a M dimensional subspace using the singular value decomposition (SVD) ,\\nX = UDV T , The discussion applies then to the reduced observations X = u1x with\\nU M being the first M columns of U .\\n\\n\\fL. Parra. C. D. Spence. P Sajda. A. Ziehe and K.-R. Muller\\n\\n946\\n\\nthe prior on A will restrict the solutions to satisfy the abundance constraints and\\nbias the result depending on the detailed choice of Pa(A), so we are led to constrained ICA.\\nIn summary, depending on which variable we integrate out we obtain two methods\\nfor solving the spectral unmixing problem: the known technique of simplex fitting\\nand a new constrained ICA algorithm.\\n2.4\\n\\nMAP Solution for the Noisy Case\\n\\nCombining the choices for the priors made in section 2.2 (Eqs.(4), (5) and (6)) with\\n(2) and (3) we obtain\\nAMAP, SMAP\\n\\n=\\n\\n\\\"''i~ax\\n\\nft {g\\n\\nN(x\\\", -\\n\\na,s\\\" a,)\\n\\nll. P,(t.\\n\\n'm,d\\\",) } ,\\n\\n(9)\\n\\nsubject to AIM = lL, A 2: O. The logarithm of the cost function in (9) is denoted\\nby L = L(A, S). Its gradient with respect to the hidden variables is\\n= _AT nm diag(O')-l - fs(sm)\\n88L\\nSm\\n\\n(10)\\n\\nwhere N = X - AS, nm are the M column vectors of N, fs(s) = - olnc;(s). In (10)\\nfs is applied to each element of Sm.\\nThe optimization with respect to A for given S can be implemented as a standard\\nweighted least squares (L8) problem with a linear constraint and positivity bounds.\\nSince the constraints apply for every pixel independently one can solve N separate\\nconstrained LS problems of M unknowns each. We alternate between gradient steps\\nfor S and explicit solutions for A until convergence. Any additional parameters of\\nPe(e) such as scale and mean may be obtained in a maximum likelihood (ML) sense\\nby maximizing L. Note that the nonlinear optimization is not subject to constraints;\\nthe constraints apply only in the quadratic optimization.\\n\\n3\\n\\nExperiments\\n\\n3.1\\n\\nZero Noise Case: Artificial Mixtures\\n\\nIn our first experiment we use mineral data from the United States Geological Survey (USGS)2 to build artificial mixtures for evaluating our unsupervised unmixing\\nframework. Three target endmembers where chosen (Almandine WS479 , Montmorillonite+Illi CM42 and Dickite NMNH106242). A spectral scene of 100 samples\\nwas constructed by creating a random mixture of the three minerals. Of the 100\\nsamples, there were no pure samples (Le. no mineral had more than a 80% abundance in any sample). Figure 1A is the spectra of the endmembers recovered by the\\nconstrained ICA technique of section 2.3, where the constraints were implemented\\nwith penalty terms added to the conventional maximum likelihood ICA algorithm.\\nThese are nearly identical to the spectra of the true endmembers, shown in figure 1B, which were used for mixing. Interesting to note is the scatter-plot of the\\n100 samples across two bands. The open circles are the absorption values at these\\ntwo bands for endmembers found by the MAP technique. Given that each mixed\\nsample consists of no more than 80% of any endmember, the endmember points\\non the scatter-plot are quite distant from the cluster. A simplex fitting technique\\nwould have significant difficulty recovering the endmembers from this clustering.\\n2see http://speclab.cr .usgs.gov /spectral.lib.456.descript/ decript04.html\\n\\n\\f947\\n\\nUnmixing Hyperspectral Data\\nfound endmembers\\n\\nobserved X and found S\\n\\ntarget endmembers\\n\\no\\n\\ng 0.8\\n\\n~\\n\\n~0.6\\n.,\\n\\n~\\n\\n~ 0.4\\n\\no\\nO~------'\\n\\n50\\n\\n100 150\\nwavelength\\n\\nO~------'\\n\\n200\\n\\nA\\n\\n50\\n\\n100 150\\nwavelength\\n\\nB\\n\\n200\\n\\n0.2'---~------'\\n\\n0.4\\n\\n0.6\\n\\n0.8\\n\\nwavelength=30\\n\\nC\\n\\nFigure 1: Results for noise-free artificial mixture. A recovered endmembers using\\nMAP technique. B \\\"true\\\" target endmembers. C scatter plot of samples across 2\\nbands showing the absorption of the three endmembers computed by MAP (open\\ncircles).\\n\\n3.2\\n\\nNoisy Case: Real Mixtures\\n\\nTo validate the noise model MAP framework of section 2.4 we conducted an experiment using ground truthed USGS data representing real mixtures. We selected\\nlOxl0 blocks of pixels from three different regions 3 in the AVIRIS data of the\\nCuprite, Nevada mining district. We separate these 300 mixed spectra assuming\\ntwo endmembers and an AR detrending with 5 AR coefficients and the MAP techniques of section 2.4. Overall brightness was accounted for as explain in the linear\\nmodeling of section 1. The endmembers are shown in figure 2A and B in comparison\\nto laboratory spectra from the USGS spectral library for these minerals [8J . Figure\\n2C shows the corresponding abundances, which match the ground truth; region\\n(III) mainly consists of Muscovite while regions (1)+(I1) contain (areal) mixtures of\\nKaolinite and Muscovite.\\n\\n4\\n\\nDiscussion\\n\\nHyperspectral unmixing is a challenging practical problem for unsupervised learning. Our probabilistic approach leads to several interesting algorithms: (1) simplex\\nfitting, (2) constrained ICA and (3) constrained least squares that can efficiently use\\nmulti-channel information. An important element of our approach is the explicit\\nuse of prior information. Our simulation examples show that we can recover the\\nendmembers, even in the presence of noise and model uncertainty. The approach\\ndescribed in this paper does not yet exploit local correlations between neighboring\\npixels that are well known to exist. Future work will therefore exploit not only\\nspectral but also spatial prior information for detecting objects and materials.\\nAcknowledgments\\n\\nWe would like to thank Gregg Swayze at the USGS for assistance in obtaining the\\ndata.\\n3The regions were from the image plate2.cuprite95.alpha.2um.image.wlocals.gif in\\nftp:/ /speclab.cr.usgs.gov /pub/cuprite/gregg.thesis.images/, at the coordinates (265,710)\\nand (275,697), which contained Kaolinite and Muscovite 2, and (143,661), which only\\ncontained Muscovite 2.\\n\\n\\fL. Parra, C. D, Spence, P Sajda, A. Ziehe and K-R. Muller\\n\\n948\\n\\nMuscovite\\n\\nKaolinite\\n0.8\\n0.7\\n\\n0 .65\\n\\n0.6\\n\\n0.6\\n0.55\\n\\n0.4\\n\\n0.5\\n\\n'c .?.? \\\", \\\"'0 ..\\n' .,\\n\\n0.45\\n\\n0.3\\n\\n0.4,--~--:-:-:-\\\"-~----:--:--~\\n160\\n\\n190\\n200\\nwaveleng1h\\n\\nA\\n\\n210\\n\\n220\\n\\n180\\n\\n190\\n200\\nwavelength\\n\\nB\\n\\n210\\n\\n220\\n\\nC\\n\\nFigure 2: A Spectra of computed endmember (solid line) vs Muscovite sample\\nspectra from the USGS data base library. Note we show only part of the spectrum\\nsince the discriminating features are located only between band 172 and 220. B\\nComputed endmember (solid line) vs Kaolinite sample spectra from the USGS data\\nbase library. C Abundances for Kaolinite and Muscovite for three regions (lighter\\npixels represent higher abundance). Region 1 and region 2 have similar abundances\\nfor Kaolinite and Muscovite, while region 3 contains more Muscovite.\\n\\nReferences\\n[1] J. Bayliss, J. A. Gualtieri, and R. Cromp. Analyzing hyperspectral data with\\nindependent component analysis. In J. M. Selander, editor, Proc. SPIE Applied\\nImage and Pattern Recognition Workshop, volume 9, P.O. Box 10, Bellingham\\nWA 98227-0010, 1997. SPIE.\\n[2] J.W. Boardman and F.A. Kruse. Automated spectral analysis: a geologic example using AVIRIS data, north Grapevine Mountains, Nevada. In Tenth Thematic\\nConference on Geologic Remote Sensing, pages 407-418, Ann arbor, MI, 1994.\\nEnvironmental Research Institute of Michigan.\\n[3] S. Haykin. Adaptive Filter Theory. Prentice Hall, 1991.\\n[4] F. Maselli, , M. Pieri, and C. Conese. Automatic identification of end-members\\nfor the spectral decomposition of remotely sensed scenes. Remote Sensing for\\nGeography, Geology, Land Planning, and Cultural Heritage (SPIE) , 2960:104109,1996.\\n[5] B. Pearlmutter and L. Parra. Maximum likelihood blind source separation: A\\ncontext-sensitive generalization ofICA. In M. Mozer, M. Jordan, and T. Petsche,\\neditors, Advances in Neural Information Processing Systems 9, pages 613-619,\\nCambridge MA, 1997. MIT Press.\\n[6] J.J. Settle. Linear mixing and the estimation of ground cover proportions. International Journal of Remote Sensing, 14:1159-1177,1993.\\n[7] M.O. Smith, J .B. Adams, and A.R. Gillespie. Reference endmembers for spectral\\nmixture analysis. In Fifth Australian remote sensing conference, volume 1, pages\\n331-340, 1990.\\n[8] U.S. Geological Survey. USGS digital spectral library. Open File Report 93-592,\\n1993.\\n[9] H. Szu and C. Hsu. Landsat spectral demixing a la superresolution of blind\\nmatrix inversion by constraint MaxEnt neural nets. In Wavelet Applications\\nIV, volume 3078, pages 147-160. SPIE, 1997.\\n\\n\\f\",\n          \"Temporal Difference Learning of\\nPosition Evaluation in the Game of Go\\nNicol N. Schraudolph\\nschraudo~salk.edu\\n\\nPeter Dayan\\ndayan~salk.edu\\n\\nTerrence J. Sejnowski\\nterry~salk.edu\\n\\nComputational Neurobiology Laboratory\\nThe Salk Institute for Biological Studies\\nSan Diego, CA 92186-5800\\n\\nAbstract\\nThe game of Go has a high branching factor that defeats the tree\\nsearch approach used in computer chess, and long-range spatiotemporal interactions that make position evaluation extremely\\ndifficult. Development of conventional Go programs is hampered\\nby their knowledge-intensive nature. We demonstrate a viable\\nalternative by training networks to evaluate Go positions via temporal difference (TD) learning.\\nOur approach is based on network architectures that reflect the\\nspatial organization of both input and reinforcement signals on\\nthe Go board, and training protocols that provide exposure to\\ncompetent (though unlabelled) play. These techniques yield far\\nbetter performance than undifferentiated networks trained by selfplay alone. A network with less than 500 weights learned within\\n3,000 games of 9x9 Go a position evaluation function that enables\\na primitive one-ply search to defeat a commercial Go program at\\na low playing level.\\n\\n1\\n\\nINTRODUCTION\\n\\nGo was developed three to four millenia ago in China; it is the oldest and one of the\\nmost popular board games in the world. Like chess, it is a deterministic, perfect\\ninformation, zero-sum game of strategy between two players. They alternate in\\n\\n817\\n\\n\\f818\\n\\nSchraudolph, Dayan, and Sejnowski\\n\\nplacing black and white stones on the intersections of a 19x19 grid (smaller for\\nbeginners) with the objective of surrounding more board area (territory) with their\\nstones than the opponent. Adjacent stones of the same color form groups; an empty\\nintersection adjacent to a group is called a liberty of that group. A group is captured\\nand removed from the board when its last liberty is occupied by the opponent. To\\nprevent loops, it is illegal to make a move which recreates a prior board position. A\\nplayer may pass at any time; the game ends when both players pass in succession.\\nUnlike most other games of strategy, Go has remained an elusive skill for com puters\\nto acquire - indeed it has been recognized as a grand challenge\\\" of Artificial\\nIntelligence (Rivest, 1993). The game tree search approach used extensively in\\ncomputer chess is infeasible: the game tree of Go has an average branching factor\\nof around 200, but even beginners may routinely look ahead up to 60 plies in\\nsome situations. Humans appear to rely mostly on static evaluation of board\\npositions, aided by highly selective yet deep local lookahead. Conventional Go\\nprograms are carefully (and protractedly) tuned expert systems (Fotland, 1993).\\nThey are fundamentally limited by their need for human assistance in compiling\\nand integrating domain knowledge, and still play barely above the level of a\\nhuman beginner - a machine learning approach may thus offer considerable\\nadvantages. (Brugmann, 1993) has shown that a knowledge-free optimization\\napproach to Go can work in principle: he obtained respectable (though inefficient)\\nplay by selecting moves through simulated annealing (Kirkpatrick et al., 1983) over\\npossible continuations of the game.\\n/I\\n\\nThe pattern recognition component inherent in Go is amenable to connectionist\\nmethods. Supervised backpropagation networks have been applied to the game\\n(Stoutamire, 1991; Enderton, 1991) but face a bottleneck in the scarcity of handlabelled training data. We propose an alternative approach based on the TD(A)\\npredictive learning algorithm (Sutton, 1984; Sutton, 1988; Barto et al., 1983), which\\nhas been successfully applied to the game of backgammon by (Tesauro, 1992).\\nHis TD-Gammon program uses a backpropagation network to map preselected\\nfeatures of the board position to an output reflecting the probability that the player\\nto move would win. It was trained by TD(O) while playing only itself, yet learned\\nan evaluation function that - coupled with a full two-ply lookahead to pick the\\nestimated best move - made it competitive with the best human players in the\\nworld (Robertie, 1992; Tesauro, 1994).\\nIn an early experiment we investigated a straightforward adaptation of Tesauro's\\napproach to the Go domain. We trained a fully connected 82-40-1 backpropagation\\nnetwork by randomized! self-play on a 9x9 Go board (a standard didactic size for\\nhumans). The output learned to predict the margin of victory or defeat for black.\\nThis undifferentiated network did learn to squeak past Wally, a weak public domain\\nprogram (Newman, 1988), but it took 659,000 games of training to do so. We have\\nfound that the efficiency of learning can be vastly improved through appropriately\\nstructured network architectures and training strategies, and these are the focus of\\nthe next two sections.\\n1 Unlike backgammon, Go is a deterministic game, so we had to generate moves stochastically to ensure sufficient exploration of the state space. This was done by Gibbs sampling (Geman and Geman, 1984) over values obtained from single-ply search, annealing the\\ntemperature parameter from random towards best-predicted play.\\n\\n\\fTemporal Difference Learning of Position Evaluation in the Game of Go\\n\\nE evaluation\\n:;;;:::-:- :--s;;\\n\\n~_ _ _~~_ _ _ _-'~\\n---\\n\\n',----:.>\\n\\nt\\nt\\n\\nprocessed\\nfeatures\\n\\n/\\n\\nC\\n____c_o_n_s_t_r_a_in_t--::-s_a_t_is_f_a_c_t_io_n____)\\n\\n..\\n\\nraw feature maps\\n\\n1 r\\n[::::J [::::) - - r\\n\\n~\\n\\nJ\\n\\nconnectivity map\\n\\n~~-----------------~\\n--~\\n\\n~/\\\"-...\\n~~~~:try[::J\\n\\nGo board\\n\\n[::J - - -\\n\\nii?Q1 ~\\n\\nFigure 1: A modular network architecture that takes advantage of board symmetries, translation invariance and localized reinforcement to evaluate Go positions.\\nAlso shown is the planned connectivity prediction mechanism (see Discussion).\\n\\n2 NETWORK ARCHITECTURE\\nOne of the particular advantages of Go for predictive learning is that there is much\\nricher information available at the end of the game than just who won. Unlike\\nchess, checkers or backgammon, in which pieces are taken away from the board\\nuntil there are few or none left, Go stones generally remain where they are placed.\\nThis makes the final state of the board richly informative with respect to the course\\nof play; indeed the game is scored by summing contributions from each point on\\nthe board. We make this spatial credit assignment accessible to the network by\\nhaving it predict the fate of every point on the board rather than just the overall\\nscore, and evaluate whole positions accordingly. This bears some similarity with\\nthe Successor Representation (Dayan, 1993) which also integrates over vector rather\\nthan scalar destinies. 2\\nGiven the knowledge-based approach of existing Go programs, there is an embarrassment of input features that one might adopt for Go: Wally already uses about\\n30 of them, stronger programs disproportionately more. In order to demonstrate\\nreinforcement learning as a viable alternative to the conventional approach, however, we require our networks to learn whatever set of features they might need.\\nThe complexity of this task can be significantly reduced by exploiting a number\\n2Sharing information within the network across multiple outputs restricts us to A = 0\\nfor efficient implementation of TD( A). Note that although (Tesauro, 1992) did not have this\\nconstraint, he nevertheless found A = 0 to be optimal.\\n\\n819\\n\\n\\f820\\n\\nSchraudolph, Dayan, and Sejnowski\\n\\nof constraints that hold a priori in this domain. Specifically, patterns of Go stones\\nretain their properties under color reversal, reflection and rotation of the board,\\nand - modulo the considerable influence of the board edges - translation. Each\\nof these invariances is reflected in our network architecture:\\nColor reversal invariance implies that changing the color of every stone in a Go\\nposition, and the player whose tum it is to move, yields an equivalent position from\\nthe other player's perspective. We build this constraint directly into our networks\\nby using antisymmetric input values (+1 for black, -1 for white) and squashing\\nfunctions throughout, and negating the bias input when it is white's tum to move.\\nGo positions are also invariant with respect to the eightfold (reflection x rotation)\\nsymmetry of the square. We provided mechanisms for constraining the network\\nto obey this invariance by appropriate weight sharing and summing of derivatives\\n(Le Cun et al., 1989). Although this is clearly beneficial during the evaluation of\\nthe network against its opponents, it appears to impede the course of learning. 3\\nTo account for translation invariance we use convolution with a weight kernel\\nrather than multiplication by a weight matrix as the basic mapping operation in\\nour network, whose layers are thus feature maps produced by scanning a fixed\\nreceptive field across the input. One particular advantage of this technique is the\\neasy transfer of learned weight kernels to different Go board sizes.\\nIt must be noted, however, that Go is not translation-invariant: the edge of the board\\nnot only affects local play but modulates other aspects of the game, and indeed\\nforms the basis of opening strategy. We currently account for this by allowing each\\nnode in our network to have its own bias weight, giving it one degree of freedom\\nfrom its neighbors. This enables the network to encode absolute position at a\\nmodest increse in the number of adjustable parameters. Furthermore, we provide\\nadditional redundancy around the board edges by selective use of convolution\\nkernels twice as wide as the input.\\n\\nFigure 1 illustrates the modular architecture suggested by these deliberations. In\\nthe experiments described below we implement all the features shown except for\\nthe connectivity map and lateral constraint satisfaction, which are the subject of\\nfuture work.\\n\\n3 TRAINING STRATEGIES\\nTern poral difference learning teaches the network to predict the consequences of\\nfollowing particular strategies on the basis of the play they produce. The question\\narises as to which strategies should be used to generate the large number of Go\\ngames needed for training. We have identified three criteria by which we compare\\nalternative training strategies:\\n? the computational efficiency of move generation,\\n? the quality of generated play, and\\n? reasonable coverage of plausible Go positions.\\n3We are investigating possible causes and cures for this phenomenon.\\n\\n\\fTemporal Difference Learning of Position Evaluation in the Game of Go\\n\\nTesauro trained TD-Gammon by self-play - ie. the network's own position evaluation was used in training to pick both players' moves. This technique does not\\nrequire any external source of expertise beyond the rules of the game: the network\\nis its own teacher. Since Go is a deterministic game, we cannot always pick the\\nestimated best move when training by self-play without running the risk of trapping the network in some suboptimal fixed state. Theoretically, this should not\\nhappen - the network playing white would be able to predict the idiosyncrasies\\nof the network playing black, take advantage of them thus changing the outcome,\\nand forcing black's predictions to change commensurately- but in practice it is a\\nconcern. We therefore pick moves stochastically by Gibbs sampling (Geman and\\nGeman, 1984), in which the probability of a given move is exponentially related to\\nthe predicted value of the position it leads to through a \\\"temperature\\\" parameter\\nthat controls the degree of randomness.\\nWe found self-play alone to be rather cumbersome for two reasons: firstly, the\\nsingle-ply search used to evaluate all legal moves is com putationally intensive and although we are investigating faster ways to accomplish it, we expect move\\nevaluation to remain a computational burden. Secondly, learning from self-play is\\nsluggish as the network must bootstrap itself out of ignorance without the benefit of\\nexposure to skilled opponents. However, there is nothing to keep us from training\\nthe network on moves that are not based on its own predictions - for instance, it\\ncan learn by playing against a conventional Go program, or even by just observing\\ngames between human players.\\nWe use three computer opponents to train our networks: a random move generator,\\nthe public-domain program Wally (Newman, 1988), and the commercial program\\nThe Many Faces of Go (Fotland, 1993). The random move generator naturally doesn't\\nplay Go very we1l 4 , but it does have the advantages of high speed and ergodicity\\n- a few thousand games of random Go proved an effective way to prime our\\nnetworks at the start of training. The two conventional Go programs, by contrast,\\nare rather slow and deterministic, and thus not suitable generators of training data\\nwhen playing among themselves. However, they do make good opponents for the\\nnetwork, which can provide the required variety of play through its Gibbs sam pIer.\\nWhen training on games played between such dissimilar players, we must match\\ntheir strength so as to prevent trivial predictions of the outcome. Against Many Faces\\nwe use standard Go handicaps for this purpose; Wally we modified to intersperse its\\nplay with random moves. The proportion of random moves is reduced adaptively\\nas the network improves, providing us with an on-line performance measure.\\nSince, in all cases, the strategies of both players are intimately intertwined in the\\npredictions, one would never expect them to be correct overall when the network\\nis playing a real opponent. This is a particular problem when the strategy for\\nchoosing moves during learning is different from the policy adopted for 'optimal'\\nnetwork play. (Samuel, 1959) found it inadvisable to let his checker program\\nlearn from games which it won against an opponent, since its predictions might\\notherwise reflect poor as well as good play. This is a particularly pernicious form\\nof over-fitting - the network can learn to predict one strategy in exquisite detail,\\nwithout being able to play well in general.\\n4In order to ensure a minimum of stability in the endgame, it does refuse to fill in its own\\neyes - a particular, locally recognizable type of suicidal move.\\n\\n821\\n\\n\\f822\\n\\nSchraudolph, Dayan, and Sejnowski\\n\\nhl-+reinf\\n\\nhO-+reinf\\n;0\\n\\narchi tecture\\n\\n~...---.-........,~~ I Ir rein;\\n\\nq-q--<t-.......\\n.\\n\\nff\\n\\n~~\\\"-'-'I\\n\\nIt:><t\\n\\n4H-H............-+-+-*......+-++-I\\n\\nboard-+hO\\n\\nr value I\\n\\n~~~ I\\n\\n.......~........~ I lr hO\\n\\\"-\\n\\n11\\n\\nHr hl~\\n\\n11'\\\" board Iinurn I\\n\\nboard -+ reinf\\n\\nturn -+ reinf\\n\\nFigure 2: A small network that learned to play 9x9 Go. Boxes in the architecture\\npanel represent 9x9 layers of units, except for turn which is a single bias unit.\\nArrows indicate convolutions with the corresponding weight kernels. Black disks\\nrepresent excitatory, white ones inhibitory weights; within each matrix, disk area\\nis proportional to weight magnitude.\\n\\n4 RESULTS\\nIn exploring this domain, we trained many networks by a variety of methods.\\nA small sample network that learned to beat Many Faces (at low playing level)\\nin 9x9 Go within 3,000 games of training is shown in Figure 2. This network\\nwas grown during training by adding hidden layers one at a time; although it\\nwas trained without the (reflection x rotation) symmetry constraint, many of the\\nweight kernels learned approximately symmetric features. The direct projection\\nfrom board to reinforcement layer has an interesting structure: the negative central\\nweight within a positive surround stems from the fact that a placed stone occupies\\n(thus loses) a point of territory even while securing nearby areas. Note that the wide\\n17x17 projections from the hidden layers have considerable fringes - ostensibly a\\ntrick the network uses to incorporate edge effects, which are also prominent in the\\nbias projections from the turn unit.\\nWe compared training this architecture by self-play versus play against Wally. The\\ninitial rate of learning is similar, but soon the latter starts to outperform the former\\n(measured against both Wally and Many Faces), demonstrating the advantage of\\nhaving a skilled opponent. After about 2000 games, however, it starts to overfit to\\nWally and consequently worsens against Many Faces. Switching training partner\\nto Many Faces at this point produced (after a further 1,000 games) a network that\\ncould reliably beat this opponent. Although less capable, the self-play network did\\nmanage to edge past Wally after 3,000 games; this compares very favorably with\\n\\n\\fTemporal Difference Learning of Position Evaluation in the Game of Go\\n\\nthe undifferentiated network described in the Introduction. Furthermore, we have\\nverified that weights learned from 9x9 Go offer a suitable basis for further training\\non the full-size (19x19) board.\\n\\n5 DISCUSSION\\nIn general our networks appear more competent in the opening than further into\\nthe game. This suggests that although reinforcement information is indeed propagating all the way back from the final position, it is hard for the network to capture\\nthe multiplicity of mid-game situations and the complex combinatorics characteristic of the endgame. These strengths and weaknesses partially complement those\\nof symbolic systems, suggesting that hybrid approaches might be rewarding. We\\nplan to further improve network performance in a number of ways:\\nIt is possible to augment the input representation of the network in such a way\\nthat its task becomes fully translation-invariant. We intend to do this by adding an\\nextra input layer whose nodes are active when the corresponding points on the Go\\nboard are empty, and inactive when they are occupied (regardless of color). Such\\nan explicit representation of liberties makes the three possible states of a point on\\nthe board (black stone, white stone, or empty) linearly separable to the network,\\nand eliminates the need for special treatment of the board edges.\\n\\nThe use of limited receptive field sizes raises the problem of how to account for\\nlong-ranging spatial interactions on the board. In Go, the distance at which groups\\nof stones interact is a function of their arrangement in context; an important subproblem of position evaluation is therefore to compute the connectivity of groups\\nof stones. We intend to model connectivity explicitly by training the network to\\npredict the correlation pattern of local reinforcement from a given position. This\\ninformation can then be used to control the lateral propagation of local features in\\nthe hidden layer through a constraint satisfaction mechanism.\\nFinally, we can train networks on recorded games between human players, which\\nthe Internet Go Server provides in steady quantities and machine-readable format.\\nWe are only beginning to explore this promising supply of instantaneous (since\\nprerecorded), high-quality Go play for training. The main obstacle encountered so\\nfar has been the human practice of abandoning the game once both players agree on\\nthe outcome - typically well before a position that could be scored mechanically is\\nreached. We address this issue by eliminating early resignations from our training\\nset, and using Wally to bring the remaining games to completion.\\nWe have shown that with sufficient attention to network architecture and training\\nprocedures, a connectionist system trained by temporal difference learning alone\\ncan achieve significant levels of performance in this knowledge-intensive domain.\\nAcknowledgements\\nWe are grateful to Patrice Simard and Gerry Tesauro for helpful discussions, to Tim\\nCasey for the plethora of game records from the Internet Go Server, and to Geoff\\nHinton for tniterations. Support was provided by the McDonnell-Pew Center for\\nCognitive Neuroscience, SERC, NSERC and the Howard Hughes Medical Institute.\\n\\n823\\n\\n\\f824\\n\\nSchraudolph, Dayan, and Sejnowski\\n\\nReferences\\n\\nBarto, A., Sutton, R, and Anderson, C. (1983). Neuronlike adaptive elements that\\ncan solve difficult learning control problems. IEEE Transactions on Systems,\\n\\nMan, and Cybernetics, 13.\\nBrugmann, B. (1993). Monte Carlo Go. Manuscript available by Internet anonymous file transfer from bsdserver.ucsf.edu, file Go/comp/mcgo.tex.Z.\\nDayan, P. (1993). Improving generalization for temporal difference learning: The\\nsuccessor representation. Neural Computation, 5(4):613-624.\\nEnderton, H. D. (1991). The Golem Go program. Technical Report CMU-CS-92101, Carnegie Mellon University. Report available by Internet anonymous file\\ntransfer from bsdserver.ucsf.edu, file Go/comp/golem.sh.Z.\\nFotland, D. (1993). Knowledge representation in the Many Faces of Go. Manuscript\\navailable by Internet anonymous file transfer from bsdserver.ucsf.edu, file\\nGo/comp/mfg.Z.\\nGeman, S. and Geman, D. (1984). Stochastic relaxation, gibbs distributions, and\\nthe bayesian restoration of images. IEEE Transactions on Pattern Analysis and\\n\\nMachine Intelligence, 6.\\nKirkpatrick, S., GelattJr., C. D., and Vecchi, M. P. (1983). Optimization by simulated\\nannealing. Science, 220:671-680.\\nLe Cun, Y., Boser, B., Denker, J., Henderson, D., Howard, R, Hubbard, W., and\\nJackel, L. (1989). Backpropagation applied to handwritten zip code recognition.\\nNeural Computation, 1:541-55l.\\nNewman, W. H. (1988). Wally, a Go playing program. Shareware C program\\navailable by Internet anonymous file transfer from bsdserver.ucsf.edu, file\\nGo/comp/wally.sh.Z.\\nRivest, R (1993). MIT Press, forthcoming. Invited talk: Computational Learning\\nTheory and Natural Learning Systems, Provincetown, MA.\\nRobertie, B. (1992). Carbon versus silicon: Matching wits with TD-Gammon. Inside\\nBackgammon, 2(2):14-22.\\nSamuel, A. L. (1959). Some studies in machine learning using the game of checkers.\\n\\nIBM Journal of Research and Development,3:211-229.\\nStoutamire, D. (1991). Machine learning applied to Go. Master's thesis, Case\\nWestern Reserve University. Reprint available by Internet anonymous file\\ntransfer from bsdserver.ucsf.edu, file Go/comp/report.ps.Z.\\nSutton, R (1984). Temporal Credit Assignment in Reinforcement Learning. PhD thesis,\\nUniversity of Massachusetts, Amherst.\\nSutton, R (1988). Learning to predict by the methods of temporal differences.\\n\\nMachine Learning, 3:9-44.\\nTesauro, G. (1992). Practical issues in temporal difference learning. Machine Learning, 8:257-278.\\nTesauro, G. (1994). TD-Gammon, a self-teaching backgammon program, achieves\\nmaster-level play. Neural Computation, 6(2):215-219.\\n\\n\\f\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Remove the columns\n",
        "papers = papers.drop(columns=['id', 'event_type', 'pdf_name'], axis=1).sample(100)\n",
        "\n",
        "# Print out the first rows of papers\n",
        "papers.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQnKMjEvwaIC"
      },
      "source": [
        "##### Remove punctuation/lower casing\n",
        "\n",
        "Next, let’s perform a simple preprocessing on the content of `paper_text` column to make them more amenable for analysis, and reliable results. To do that, we’ll use a regular expression to remove any punctuation, and then lowercase the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "Qxycyn66waIC",
        "outputId": "3a33591b-cccd-407c-b8c4-032a362babab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6095    contextual semibandits via supervised learning...\n",
              "5650    deep learning without poor local minima\\n\\nken...\n",
              "4825    conditional swap regret and\\nconditional corre...\n",
              "261     support vector regression machines\\n\\nharris d...\n",
              "5835    unsupervised discrimination of clustered data\\...\n",
              "Name: paper_text_processed, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paper_text_processed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6095</th>\n",
              "      <td>contextual semibandits via supervised learning...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5650</th>\n",
              "      <td>deep learning without poor local minima\\n\\nken...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4825</th>\n",
              "      <td>conditional swap regret and\\nconditional corre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>261</th>\n",
              "      <td>support vector regression machines\\n\\nharris d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5835</th>\n",
              "      <td>unsupervised discrimination of clustered data\\...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Load the regular expression library\n",
        "import re\n",
        "\n",
        "# Remove punctuation\n",
        "papers['paper_text_processed'] = \\\n",
        "papers['paper_text'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
        "\n",
        "# Convert the titles to lowercase\n",
        "papers['paper_text_processed'] = \\\n",
        "papers['paper_text_processed'].map(lambda x: x.lower())\n",
        "\n",
        "# Print out the first rows of papers\n",
        "papers['paper_text_processed'].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj5vax4OwaIC"
      },
      "source": [
        "** **\n",
        "#### Step 3: Exploratory Analysis <a class=\"anchor\\\" id=\"eda\"></a>\n",
        "** **\n",
        "\n",
        "To verify whether the preprocessing, we’ll make a simple word cloud using the `wordcloud` package to get a visual representation of most common words. It is key to understanding the data and ensuring we are on the right track, and if any more preprocessing is necessary before training the model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "my note : word cloud to visualize common words in the data"
      ],
      "metadata": {
        "id": "3dWKO9cVLHBk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "qYYM9i-owaIC",
        "outputId": "7600dd86-8ebc-4feb-9096-3d73a47a2114"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=400x200>"
            ],
            "image/png": "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\n",
            "image/jpeg": "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\n"
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Import the wordcloud library\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Join the different processed titles together.\n",
        "long_string = ','.join(list(papers['paper_text_processed'].values))\n",
        "\n",
        "# Create a WordCloud object\n",
        "wordcloud = WordCloud(background_color=\"white\", max_words=1000, contour_width=3, contour_color='steelblue')\n",
        "\n",
        "# Generate a word cloud\n",
        "wordcloud.generate(long_string)\n",
        "\n",
        "# Visualize the word cloud\n",
        "wordcloud.to_image()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g45GP-QdwaIC"
      },
      "source": [
        "** **\n",
        "#### Step 4: Prepare text for LDA analysis  TOKENIZE<a class=\"anchor\\\" id=\"data_preparation\"></a>\n",
        "** **\n",
        "\n",
        "Next, let’s work to transform the textual data in a format that will serve as an input for training LDA model. We start by tokenizing the text and removing stopwords. Next, we convert the tokenized object into a corpus and dictionary."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "my note :\n",
        "\n",
        "Splits the text into individual words (tokenization).\n",
        "\n",
        "Removes \"stopwords\" (common words like \"the\", \"is\", \"and\").\n",
        "\n",
        "Creates a dictionary of words and their frequencies."
      ],
      "metadata": {
        "id": "JOatMqbrLOkk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uD48yKEwaIC",
        "outputId": "dbb656f1-c1cd-4e84-c1a4-99ac4dfa8bab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['contextual', 'semibandits', 'via', 'supervised', 'learning', 'oracles', 'akshay', 'krishnamurthy', 'akshay', 'csumassedu', 'alekh', 'agarwal', 'alekha', 'microsoftcom', 'college', 'information', 'computer', 'sciences', 'university', 'massachusetts', 'amherst', 'miroslav', 'dudk', 'mdudik', 'microsoftcom', 'microsoft', 'research', 'new', 'york', 'ny']\n"
          ]
        }
      ],
      "source": [
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
        "\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        # deacc=True removes punctuations\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
        "\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc))\n",
        "             if word not in stop_words] for doc in texts]\n",
        "\n",
        "\n",
        "data = papers.paper_text_processed.values.tolist()\n",
        "data_words = list(sent_to_words(data))\n",
        "\n",
        "# remove stop words\n",
        "data_words = remove_stopwords(data_words)\n",
        "\n",
        "print(data_words[:1][0][:30])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYFyK-EdwaIC",
        "outputId": "5277f289-5103-41a9-da8f-eee238b48297"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 3), (1, 1), (2, 1), (3, 1), (4, 2), (5, 5), (6, 1), (7, 8), (8, 7), (9, 1), (10, 1), (11, 27), (12, 26), (13, 2), (14, 1), (15, 1), (16, 2), (17, 1), (18, 2), (19, 1), (20, 1), (21, 2), (22, 3), (23, 1), (24, 12), (25, 1), (26, 1), (27, 2), (28, 2), (29, 2)]\n"
          ]
        }
      ],
      "source": [
        "import gensim.corpora as corpora\n",
        "\n",
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(data_words)\n",
        "\n",
        "# Create Corpus\n",
        "texts = data_words\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "# View\n",
        "print(corpus[:1][0][:30])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bnt9spsCwaID"
      },
      "source": [
        "** **\n",
        "#### Step 5: LDA model tranining <a class=\"anchor\\\" id=\"train_model\"></a>\n",
        "** **\n",
        "\n",
        "To keep things simple, we'll keep all the parameters to default except for inputting the number of topics. For this tutorial, we will build a model with 10 topics where each topic is a combination of keywords, and each keyword contributes a certain weightage to the topic."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "my note\n",
        "\n",
        "building LDA model with topics.\n",
        "Each topic is a mix of keywords, and each document gets a mix of topics\n",
        "\n",
        "what is LDA? method to find topics by assuming that each document is a mix of topics and each topic is a mix of words"
      ],
      "metadata": {
        "id": "JnrASMMBLYAK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noJnB9B1waID",
        "outputId": "74e1fb92-7b47-4e5c-f9a9-42f55e1802ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0,\n",
            "  '0.006*\"model\" + 0.006*\"data\" + 0.006*\"learning\" + 0.005*\"function\" + '\n",
            "  '0.004*\"time\" + 0.004*\"set\" + 0.004*\"one\" + 0.004*\"algorithm\" + '\n",
            "  '0.004*\"problem\" + 0.003*\"matrix\"'),\n",
            " (1,\n",
            "  '0.006*\"data\" + 0.006*\"learning\" + 0.005*\"algorithm\" + 0.005*\"set\" + '\n",
            "  '0.004*\"time\" + 0.004*\"model\" + 0.004*\"function\" + 0.004*\"error\" + '\n",
            "  '0.004*\"network\" + 0.004*\"using\"'),\n",
            " (2,\n",
            "  '0.006*\"model\" + 0.005*\"learning\" + 0.004*\"function\" + 0.004*\"algorithm\" + '\n",
            "  '0.004*\"matrix\" + 0.004*\"image\" + 0.004*\"one\" + 0.003*\"using\" + '\n",
            "  '0.003*\"network\" + 0.003*\"also\"'),\n",
            " (3,\n",
            "  '0.007*\"algorithm\" + 0.006*\"data\" + 0.005*\"set\" + 0.004*\"learning\" + '\n",
            "  '0.004*\"model\" + 0.004*\"using\" + 0.004*\"function\" + 0.004*\"matrix\" + '\n",
            "  '0.003*\"one\" + 0.003*\"number\"'),\n",
            " (4,\n",
            "  '0.008*\"time\" + 0.006*\"learning\" + 0.006*\"model\" + 0.005*\"algorithm\" + '\n",
            "  '0.005*\"set\" + 0.004*\"data\" + 0.004*\"network\" + 0.004*\"using\" + '\n",
            "  '0.004*\"function\" + 0.004*\"one\"'),\n",
            " (5,\n",
            "  '0.005*\"algorithm\" + 0.005*\"model\" + 0.004*\"learning\" + 0.004*\"matrix\" + '\n",
            "  '0.004*\"function\" + 0.004*\"time\" + 0.003*\"set\" + 0.003*\"two\" + '\n",
            "  '0.003*\"information\" + 0.003*\"number\"'),\n",
            " (6,\n",
            "  '0.008*\"model\" + 0.005*\"learning\" + 0.004*\"data\" + 0.004*\"set\" + '\n",
            "  '0.004*\"algorithm\" + 0.003*\"figure\" + 0.003*\"time\" + 0.003*\"using\" + '\n",
            "  '0.003*\"input\" + 0.003*\"network\"'),\n",
            " (7,\n",
            "  '0.007*\"model\" + 0.006*\"set\" + 0.005*\"algorithm\" + 0.004*\"using\" + '\n",
            "  '0.004*\"learning\" + 0.004*\"data\" + 0.004*\"one\" + 0.004*\"figure\" + '\n",
            "  '0.004*\"also\" + 0.003*\"function\"'),\n",
            " (8,\n",
            "  '0.007*\"model\" + 0.006*\"algorithm\" + 0.006*\"learning\" + 0.005*\"set\" + '\n",
            "  '0.005*\"data\" + 0.004*\"number\" + 0.004*\"figure\" + 0.004*\"function\" + '\n",
            "  '0.004*\"network\" + 0.003*\"time\"'),\n",
            " (9,\n",
            "  '0.005*\"learning\" + 0.005*\"data\" + 0.005*\"model\" + 0.005*\"network\" + '\n",
            "  '0.004*\"algorithm\" + 0.004*\"set\" + 0.004*\"function\" + 0.004*\"using\" + '\n",
            "  '0.004*\"figure\" + 0.004*\"one\"')]\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# number of topics\n",
        "num_topics = 10\n",
        "\n",
        "# Build LDA model\n",
        "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                       id2word=id2word,\n",
        "                                       num_topics=num_topics)\n",
        "\n",
        "# Print the Keyword in the 10 topics\n",
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bk1-T73PwaID"
      },
      "source": [
        "** **\n",
        "#### Step 6: Analyzing our LDA model <a class=\"anchor\\\" id=\"results\"></a>\n",
        "** **\n",
        "\n",
        "Now that we have a trained model let’s visualize the topics for interpretability. To do so, we’ll use a popular visualization package, pyLDAvis which is designed to help interactively with:\n",
        "\n",
        "1. Better understanding and interpreting individual topics, and\n",
        "2. Better understanding the relationships between the topics.\n",
        "\n",
        "For (1), you can manually select each topic to view its top most frequent and/or “relevant” terms, using different values of the λ parameter. This can help when you’re trying to assign a human interpretable name or “meaning” to each topic.\n",
        "\n",
        "For (2), exploring the Intertopic Distance Plot can help you learn about how topics relate to each other, including potential higher-level structure between groups of topics."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyLDAvis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wp_NTwx0M-5",
        "outputId": "81d3ac14-eacd-4bc6-e172-6b0f4c407ea3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyLDAvis\n",
            "  Downloading pyLDAvis-3.4.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: numpy>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.13.1)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.2.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (3.1.4)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.10.1)\n",
            "Collecting funcy (from pyLDAvis)\n",
            "  Downloading funcy-2.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.5.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (4.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (75.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2024.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.5.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->pyLDAvis) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->pyLDAvis) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim->pyLDAvis) (1.16.0)\n",
            "Downloading pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
            "Installing collected packages: funcy, pyLDAvis\n",
            "Successfully installed funcy-2.0 pyLDAvis-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('./results', exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GB0jnqlU5CB0",
        "outputId": "96caf2d0-f26d-407c-9325-08e2993ce2a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        },
        "id": "P_vMqoLzwaID",
        "outputId": "b238da8a-5ca1-4481-f528-205d1393e411"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
              "topic                                                \n",
              "4     -0.004444 -0.001028       1        1  15.491984\n",
              "0     -0.001494  0.006452       2        1  15.420320\n",
              "2     -0.006569  0.001241       3        1  14.715879\n",
              "9     -0.001017 -0.003257       4        1  13.217646\n",
              "1     -0.001531 -0.008253       5        1  11.720551\n",
              "7      0.000187  0.004325       6        1  10.547646\n",
              "8      0.001650 -0.003113       7        1   7.214651\n",
              "3      0.009912 -0.000414       8        1   5.438570\n",
              "6     -0.002815  0.002133       9        1   3.472803\n",
              "5      0.006122  0.001913      10        1   2.759949, topic_info=          Term         Freq        Total Category  logprob  loglift\n",
              "548      model  1286.000000  1286.000000  Default  30.0000  30.0000\n",
              "36   algorithm  1041.000000  1041.000000  Default  29.0000  29.0000\n",
              "523     matrix   688.000000   688.000000  Default  28.0000  28.0000\n",
              "793        set   988.000000   988.000000  Default  27.0000  27.0000\n",
              "476   learning  1205.000000  1205.000000  Default  26.0000  26.0000\n",
              "..         ...          ...          ...      ...      ...      ...\n",
              "939      using    15.074971   774.277850  Topic10  -6.0111  -0.3489\n",
              "329     figure    14.341841   702.856180  Topic10  -6.0610  -0.3020\n",
              "204       data    16.722680  1046.716975  Topic10  -5.9074  -0.5467\n",
              "571     neural    13.167883   579.994935  Topic10  -6.1464  -0.1953\n",
              "334      first    12.183443   498.013918  Topic10  -6.2241  -0.1206\n",
              "\n",
              "[882 rows x 6 columns], token_table=       Topic      Freq        Term\n",
              "term                              \n",
              "11627      1  0.150012  abundances\n",
              "11627      2  0.075006  abundances\n",
              "11627      3  0.300024  abundances\n",
              "11627      4  0.075006  abundances\n",
              "11627      5  0.150012  abundances\n",
              "...      ...       ...         ...\n",
              "12915      6  0.081749         zik\n",
              "12915      7  0.163497         zik\n",
              "12915      8  0.081749         zik\n",
              "12915      9  0.040874         zik\n",
              "12915     10  0.040874         zik\n",
              "\n",
              "[3877 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[5, 1, 3, 10, 2, 8, 9, 4, 7, 6])"
            ],
            "text/html": [
              "\n",
              "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
              "\n",
              "\n",
              "<div id=\"ldavis_el19311351084006676003312548861\" style=\"background-color:white;\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "\n",
              "var ldavis_el19311351084006676003312548861_data = {\"mdsDat\": {\"x\": [-0.004443722685983072, -0.0014944676175048146, -0.006569031546327729, -0.0010171754436562302, -0.001530996390417643, 0.0001868774933656669, 0.001649556496290796, 0.009912026330878987, -0.0028153104322406193, 0.0061222437955946556], \"y\": [-0.001027758040712094, 0.006451952494491522, 0.0012407883826659517, -0.0032567423576316513, -0.008252809732466641, 0.004324973237668883, -0.0031126903988046226, -0.00041411360869440637, 0.0021329978350173927, 0.0019134021884656585], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [15.491984050334304, 15.420319724821299, 14.715879447403179, 13.217646439438974, 11.720551006485044, 10.547645932500108, 7.214651407763707, 5.4385702119288215, 3.4728027399000103, 2.759949039424564]}, \"tinfo\": {\"Term\": [\"model\", \"algorithm\", \"matrix\", \"set\", \"learning\", \"data\", \"function\", \"number\", \"using\", \"input\", \"time\", \"figure\", \"information\", \"error\", \"two\", \"distribution\", \"used\", \"network\", \"models\", \"one\", \"problem\", \"image\", \"given\", \"also\", \"networks\", \"xi\", \"regret\", \"since\", \"training\", \"algorithms\", \"news\", \"trending\", \"voting\", \"series\", \"ergodic\", \"twitter\", \"kasiviswanathan\", \"melville\", \"stationarity\", \"chan\", \"t_p\", \"fpr\", \"differencing\", \"shifts\", \"manners\", \"trend\", \"devavrat\", \"aggressive\", \"claudio\", \"posts\", \"ethernet\", \"mandelbrot\", \"declared\", \"austria\", \"wafer\", \"xiaoyue\", \"wien\", \"schulmans\", \"leisch\", \"tpr\", \"sources\", \"shortcut\", \"time\", \"trends\", \"advance\", \"shortcuts\", \"chain\", \"autoregressive\", \"membrane\", \"majority\", \"weighted\", \"neighbor\", \"ut\", \"max\", \"functions\", \"stationary\", \"contour\", \"nearest\", \"harmonium\", \"steps\", \"policy\", \"learning\", \"network\", \"classification\", \"log\", \"non\", \"show\", \"algorithm\", \"set\", \"model\", \"process\", \"using\", \"figure\", \"state\", \"results\", \"neural\", \"probability\", \"data\", \"one\", \"problem\", \"two\", \"function\", \"models\", \"let\", \"value\", \"first\", \"distribution\", \"kernel\", \"linear\", \"input\", \"number\", \"training\", \"case\", \"information\", \"also\", \"given\", \"performance\", \"bn\", \"prox\", \"argyriou\", \"richtarik\", \"xr\", \"kwktr\", \"tensors\", \"subgradient\", \"cyclic\", \"admm\", \"psca\", \"bcd\", \"kxri\", \"waxi\", \"xri\", \"matricization\", \"scholes\", \"kb\", \"xrj\", \"rir\", \"ekf\", \"ktr\", \"nonlocal\", \"tensor\", \"tomioka\", \"metabolic\", \"micchelli\", \"yeir\", \"categorical\", \"multipliers\", \"hippocampus\", \"sh\", \"regularizer\", \"valuation\", \"ep\", \"regularization\", \"responses\", \"functions\", \"morton\", \"replay\", \"eg\", \"field\", \"data\", \"hamming\", \"problem\", \"first\", \"order\", \"function\", \"vision\", \"model\", \"mean\", \"convex\", \"optimization\", \"xi\", \"learning\", \"one\", \"time\", \"following\", \"information\", \"approximation\", \"vector\", \"input\", \"matrix\", \"well\", \"case\", \"set\", \"ie\", \"number\", \"see\", \"neural\", \"given\", \"algorithm\", \"two\", \"method\", \"results\", \"models\", \"figure\", \"network\", \"training\", \"distribution\", \"used\", \"also\", \"image\", \"signatures\", \"casual\", \"muscovite\", \"verification\", \"wilkinson\", \"centralized\", \"acceptance\", \"forgeries\", \"unmixing\", \"brightness\", \"handwriting\", \"forgery\", \"abundances\", \"signature\", \"radar\", \"dwell\", \"spence\", \"tolat\", \"endmembers\", \"broom\", \"usgs\", \"positivity\", \"cuprite\", \"endmember\", \"oonference\", \"fiduciary\", \"fukishima\", \"ziehe\", \"initials\", \"uxlx\", \"audio\", \"wavelet\", \"hyperspectral\", \"wavelets\", \"hpnn\", \"facesync\", \"image\", \"detection\", \"correlation\", \"spectral\", \"scale\", \"output\", \"coding\", \"true\", \"matrix\", \"linear\", \"transparent\", \"false\", \"search\", \"information\", \"also\", \"noise\", \"function\", \"model\", \"images\", \"used\", \"matrices\", \"training\", \"learning\", \"given\", \"synaptic\", \"latent\", \"one\", \"random\", \"using\", \"results\", \"two\", \"algorithms\", \"algorithm\", \"number\", \"network\", \"neural\", \"problem\", \"state\", \"figure\", \"optimal\", \"performance\", \"time\", \"data\", \"first\", \"based\", \"set\", \"error\", \"networks\", \"wjc\", \"netinf\", \"leskovec\", \"transmission\", \"opposition\", \"glas\", \"bji\", \"refinement\", \"pooling\", \"infection\", \"xdx\", \"hli\", \"diffusions\", \"breakeven\", \"break\", \"eigenmaps\", \"strategic\", \"sll\", \"infected\", \"berger\", \"lstm\", \"multiagent\", \"aji\", \"mle\", \"infections\", \"ic\", \"weibull\", \"nadaraya\", \"caltech\", \"rumor\", \"contagion\", \"cascades\", \"pk\", \"mse\", \"connie\", \"social\", \"dictionary\", \"edge\", \"optimizers\", \"domain\", \"players\", \"city\", \"sparse\", \"network\", \"player\", \"different\", \"performance\", \"figure\", \"hidden\", \"weights\", \"objective\", \"matrices\", \"recognition\", \"kernels\", \"using\", \"data\", \"structure\", \"number\", \"two\", \"networks\", \"well\", \"learning\", \"matrix\", \"function\", \"local\", \"xi\", \"one\", \"given\", \"set\", \"algorithm\", \"linear\", \"methods\", \"model\", \"results\", \"error\", \"used\", \"training\", \"method\", \"distribution\", \"also\", \"information\", \"time\", \"problem\", \"neural\", \"image\", \"descended\", \"correctness\", \"bichromaticity\", \"karger\", \"cover\", \"subtree\", \"kernelsum\", \"growth\", \"monochromatic\", \"redefined\", \"tca\", \"findallnn\", \"lwe\", \"query\", \"descend\", \"bichromatic\", \"sentiment\", \"dhh\", \"descends\", \"synergy\", \"callahan\", \"lippman\", \"beygelzimer\", \"fmm\", \"krauthgamer\", \"marginal\", \"multipole\", \"ruhl\", \"semg\", \"maxim\", \"etal\", \"supplemental\", \"summations\", \"nets\", \"tolerance\", \"runtime\", \"source\", \"net\", \"tree\", \"domain\", \"error\", \"target\", \"latent\", \"reference\", \"differences\", \"subroutine\", \"cluster\", \"data\", \"neural\", \"based\", \"networks\", \"linear\", \"single\", \"performance\", \"algorithm\", \"learning\", \"training\", \"models\", \"set\", \"using\", \"network\", \"point\", \"methods\", \"time\", \"function\", \"also\", \"one\", \"matrix\", \"approach\", \"problem\", \"distribution\", \"two\", \"model\", \"figure\", \"first\", \"results\", \"image\", \"given\", \"number\", \"verb\", \"selector\", \"transparent\", \"pds\", \"destination\", \"musical\", \"hit\", \"animate\", \"chromatic\", \"sumida\", \"vovk\", \"optscale\", \"ffull\", \"blind\", \"sequencing\", \"mixability\", \"yf\", \"ambiguities\", \"copied\", \"modulated\", \"man\", \"pdp\", \"specular\", \"stores\", \"performmusic\", \"dog\", \"factorial\", \"isoluminant\", \"tsybakov\", \"pursuit\", \"stochastically\", \"blur\", \"basic\", \"deblurring\", \"filter\", \"units\", \"morton\", \"pf\", \"harmonium\", \"estimation\", \"background\", \"pattern\", \"ni\", \"set\", \"model\", \"object\", \"using\", \"also\", \"tr\", \"et\", \"may\", \"stochastic\", \"figure\", \"approximation\", \"target\", \"ie\", \"one\", \"loss\", \"algorithm\", \"methods\", \"image\", \"two\", \"performance\", \"method\", \"kernel\", \"approach\", \"data\", \"models\", \"shown\", \"matrix\", \"used\", \"learning\", \"distribution\", \"function\", \"case\", \"network\", \"given\", \"time\", \"results\", \"problem\", \"training\", \"first\", \"neural\", \"kerr\", \"faults\", \"string\", \"lens\", \"skinner\", \"behrman\", \"lof\", \"elizabeth\", \"bigram\", \"beams\", \"fault\", \"synergy\", \"irradiance\", \"wichita\", \"optically\", \"xtk\", \"logn\", \"lincoln\", \"interpretanon\", \"itlt\", \"retraining\", \"wave\", \"perfonn\", \"neuralworks\", \"llm\", \"modication\", \"bernasconi\", \"skrzypekt\", \"spheres\", \"constance\", \"optical\", \"dishes\", \"regret\", \"lagrangian\", \"link\", \"exchangeable\", \"cluster\", \"nodes\", \"tau\", \"layers\", \"infinite\", \"factorization\", \"brief\", \"judge\", \"zik\", \"gain\", \"swap\", \"number\", \"layer\", \"binary\", \"algorithm\", \"model\", \"equilibrium\", \"related\", \"set\", \"general\", \"output\", \"three\", \"documents\", \"net\", \"learning\", \"figure\", \"error\", \"used\", \"features\", \"analysis\", \"data\", \"feature\", \"network\", \"problem\", \"given\", \"matrix\", \"one\", \"training\", \"distribution\", \"two\", \"function\", \"using\", \"time\", \"based\", \"input\", \"state\", \"different\", \"case\", \"results\", \"performance\", \"also\", \"image\", \"first\", \"neural\", \"vcee\", \"rankings\", \"semibandit\", \"reyzin\", \"qt\", \"klt\", \"amo\", \"regret\", \"mslr\", \"spiral\", \"kale\", \"semibandits\", \"rtj\", \"eels\", \"xta\", \"articles\", \"reg\", \"mstd\", \"dudk\", \"swap\", \"reedy\", \"linucb\", \"schapire\", \"rounds\", \"ucb\", \"kux\", \"xtk\", \"bandits\", \"vocabularies\", \"certify\", \"pmin\", \"competitor\", \"round\", \"losses\", \"equilibrium\", \"tree\", \"algorithm\", \"composite\", \"player\", \"bound\", \"runtime\", \"query\", \"min\", \"contextual\", \"data\", \"sequence\", \"oracle\", \"mixability\", \"set\", \"using\", \"number\", \"case\", \"matrix\", \"xi\", \"margin\", \"actions\", \"let\", \"function\", \"error\", \"distribution\", \"prior\", \"log\", \"action\", \"models\", \"one\", \"training\", \"learning\", \"also\", \"input\", \"probability\", \"two\", \"first\", \"neural\", \"model\", \"problem\", \"networks\", \"network\", \"given\", \"time\", \"information\", \"figure\", \"results\", \"sumida\", \"exposure\", \"monthly\", \"neutral\", \"determiner\", \"subjective\", \"dyer\", \"inferencing\", \"ocain\", \"price\", \"chomsky\", \"theories\", \"exemplar\", \"man\", \"cybenko\", \"caste\", \"ambiguities\", \"inferences\", \"compatible\", \"options\", \"oii\", \"bsb\", \"selector\", \"animate\", \"discusses\", \"earning\", \"nlp\", \"oval\", \"pds\", \"phrase\", \"mpot\", \"predicate\", \"verb\", \"pf\", \"noun\", \"stores\", \"contracts\", \"pos\", \"snr\", \"glas\", \"dog\", \"hit\", \"forecast\", \"binary\", \"boltzmann\", \"model\", \"input\", \"filters\", \"language\", \"role\", \"simple\", \"units\", \"pattern\", \"models\", \"end\", \"distribution\", \"sparse\", \"memory\", \"used\", \"however\", \"learning\", \"figure\", \"information\", \"representation\", \"state\", \"image\", \"space\", \"set\", \"problem\", \"based\", \"data\", \"output\", \"using\", \"log\", \"algorithm\", \"networks\", \"error\", \"network\", \"two\", \"time\", \"training\", \"one\", \"function\", \"matrix\", \"also\", \"number\", \"results\", \"performance\", \"transducer\", \"swap\", \"lmin\", \"xri\", \"competitor\", \"qtk\", \"bri\", \"rmf\", \"regk\", \"garzon\", \"orbits\", \"fjt\", \"logn\", \"projens\", \"oblivious\", \"pti\", \"peterfreund\", \"factoring\", \"rojas\", \"botelho\", \"crp\", \"nash\", \"bought\", \"nys\", \"lkj\", \"xr\", \"subroutines\", \"angers\", \"hxi\", \"cyclic\", \"kh\", \"ys\", \"proj\", \"bigram\", \"coherence\", \"apg\", \"dfc\", \"randomized\", \"equilibrium\", \"lim\", \"bcd\", \"player\", \"macbeth\", \"kb\", \"roj\", \"tree\", \"since\", \"matrix\", \"players\", \"critical\", \"divide\", \"regret\", \"binary\", \"feature\", \"algorithms\", \"optimization\", \"conditional\", \"algorithm\", \"step\", \"deep\", \"show\", \"xi\", \"information\", \"work\", \"function\", \"given\", \"define\", \"input\", \"number\", \"convex\", \"class\", \"two\", \"learning\", \"model\", \"rank\", \"time\", \"networks\", \"image\", \"new\", \"set\", \"one\", \"used\", \"also\", \"results\", \"network\", \"problem\", \"using\", \"figure\", \"data\", \"neural\", \"first\"], \"Freq\": [1286.0, 1041.0, 688.0, 988.0, 1205.0, 1046.0, 891.0, 594.0, 774.0, 448.0, 937.0, 702.0, 537.0, 512.0, 674.0, 518.0, 520.0, 796.0, 512.0, 799.0, 651.0, 547.0, 544.0, 597.0, 490.0, 416.0, 140.0, 293.0, 591.0, 331.0, 8.908386806111809, 6.572073922731338, 16.60452854499008, 101.57524244234389, 2.7731690714479993, 12.207206158634014, 1.1008581342754646, 1.0784232721610947, 6.072339039328686, 1.8634814498128829, 1.4629709640174446, 2.561652319416152, 2.1598609322394435, 3.898430637450852, 1.0553876421831958, 10.438079420765318, 1.0292771582081328, 1.0181785872644156, 0.6686826989172244, 1.0086322301955202, 1.0152437533692233, 1.3574686426992078, 1.3236945383178653, 1.0043878002165467, 3.8651688936220174, 0.9902790482375625, 1.6962056501258518, 0.9794090930746887, 1.3136928333657854, 2.3095166776414437, 22.348582606584717, 8.330602834191396, 267.7467333472413, 8.522251801171219, 3.513528311721884, 3.0133566660639315, 17.268232367680454, 4.672643347760291, 27.481308149497693, 16.792639210271997, 41.54974036688502, 23.754618508833264, 14.70104178231168, 66.71789131608999, 88.08659340766029, 23.0190234682263, 14.447871373146615, 31.20225131494576, 10.970408974650011, 27.191394456318072, 29.897879801857176, 215.0570529405382, 149.9107663369254, 51.18981420860525, 69.01917782562107, 66.84492458499884, 59.9106891080416, 178.48881540180773, 168.0175962995805, 204.51327554704758, 55.8312060742339, 131.08086101445812, 118.59753973556352, 71.66005690935435, 96.63720386309855, 100.25363351286329, 69.28546064641621, 154.58261050268663, 125.66449066041976, 106.78609794637164, 107.9549711905211, 130.5493321495153, 87.79280787610936, 67.27777883529987, 66.96981196735835, 82.61111874769026, 83.34808209236098, 64.94442860146714, 81.79418697168694, 74.21230364527499, 84.73243610158163, 83.0488635296964, 74.44424424425262, 78.83259027465445, 77.1172151818865, 73.98923592551256, 72.17445083001567, 8.854021771593002, 3.230488849706928, 1.4295495361522708, 2.388490157675881, 32.03802650722677, 2.399438452128888, 6.3645550096861285, 3.393792783783496, 8.35092321693586, 1.917397002138523, 5.739823738602849, 7.916993863672676, 1.837411753808697, 1.8395010895827986, 6.61586829144536, 2.552835322204035, 3.4662579232983948, 11.460734832278106, 1.1899460345496555, 0.8968795366794607, 2.7977314744406323, 1.8768632631326834, 2.9456734469278762, 30.004360687958442, 1.2057776123222461, 17.188253301904247, 0.9013110784801923, 3.135290636990454, 1.7561729040144096, 2.892677441892845, 10.71632180814064, 19.35970853426741, 10.762348944011583, 8.393084425387036, 15.62610663017556, 29.02559906356106, 19.156924341406604, 88.26271092593659, 8.615167255928672, 10.011807034939881, 52.50987816174212, 36.59603293366857, 195.11702480681518, 16.47619967024057, 124.53879786637972, 98.16762096779686, 57.424341895933566, 160.97336183152146, 26.797668891477567, 216.1703255450083, 54.20955025152198, 46.011224169675764, 64.87903793894228, 79.29498328909607, 190.9848143863348, 134.85140425401056, 153.49411024952676, 64.6096629962788, 96.89157292475552, 59.50251087968439, 74.00797854911579, 81.69449023993893, 113.92330916516414, 58.82153966446889, 81.14342058590748, 145.10887567676392, 57.24838519940425, 94.47903421857524, 61.57230002076122, 91.27737432762366, 85.2752630132967, 125.63308132193107, 95.77321792113158, 65.5067421542105, 80.56367947016376, 77.10592038730526, 89.70954776253122, 95.05567139259341, 81.88685462842467, 75.81247267093127, 75.42793589035517, 79.27018375469093, 75.20438008746763, 15.007663928763076, 1.869630746935072, 3.5856016222473124, 7.936797163568855, 1.68867134923275, 1.3080880066384901, 4.227149729891593, 12.012594040842563, 4.430401205700825, 3.9329661576316, 1.946438495283462, 1.966144898935809, 4.451748885179908, 10.059806233223199, 1.27259760879195, 1.2731375037255794, 3.399989179543646, 0.9392740395925329, 7.812566456613689, 0.9203939996504639, 2.4499897680393823, 1.5060531404506554, 1.1955765745086662, 1.7934743079851834, 1.4761119941092948, 2.4025768295771996, 0.8893408558342263, 1.4447772126575515, 1.1728124760189962, 1.4185450705520155, 18.021182334327293, 7.396383091019552, 4.218011835173132, 7.931333799017324, 12.279447668726982, 7.098060006168572, 120.70531852894682, 30.288251784434554, 25.88195522942493, 32.65070413786613, 62.05462185835814, 70.23739639427791, 30.565101699424172, 39.936589845818915, 127.9921585363846, 99.22967894052121, 19.580921858766978, 11.957463205295577, 42.97991579579659, 98.69408026317315, 107.43174559482942, 65.78841402330497, 146.78372012455355, 197.5806236464778, 56.12745248833325, 92.54751362435258, 40.12709474340722, 100.67949652448358, 179.1294186954251, 91.03113174271364, 30.758589737202538, 61.94762580010178, 117.55297722300256, 59.121628197636326, 113.66339757573112, 87.45615791525572, 100.727387089838, 59.34022003766667, 134.38804164145404, 90.18904260837235, 110.84031249529384, 87.8771108696758, 94.13543742978176, 64.18830572205151, 92.89781787930036, 65.58857607675353, 71.7878505847287, 101.72277489344705, 105.2227636845729, 72.72885095208957, 69.4014605830158, 92.61927694562334, 71.31524330778542, 70.17060306472494, 1.7233008586063663, 9.039408125982805, 2.2795474323280014, 10.286106084411164, 1.5820802360738202, 19.302439100035958, 2.566958170394028, 4.660271007873163, 12.454412173412182, 8.74155997404547, 2.1195433005359843, 3.541016512809761, 1.9508606095383332, 1.6240032484577538, 3.9733799752948746, 2.3724007301242533, 4.747493313600141, 1.6456425394094096, 11.236653031369068, 1.2928812862406085, 9.420927874362244, 1.2399401465175328, 3.2356777999936086, 2.8792108020440015, 0.7808776859211729, 6.639266220643685, 1.5918407223270892, 1.5576013419786223, 5.175897424518773, 1.0311975476547974, 1.8471931841156248, 5.651576754568971, 12.310441519038513, 5.65502632113915, 6.188742712306724, 10.630549313580838, 8.441358111137403, 19.8647548794954, 7.954670510877037, 36.78946589976732, 17.83645430759543, 8.240213537287845, 38.98262395376668, 143.06097336915562, 14.087826730043561, 79.95545539022672, 85.11824583320545, 117.02882717585346, 37.62629672116945, 47.801877818387574, 31.973632682417534, 36.91475943758881, 38.694696359285736, 23.524821724633913, 117.609585215053, 150.09647431745407, 46.449788294107655, 92.06690804666528, 101.48594163798116, 77.92185596864682, 51.19696789386391, 161.21206551932738, 101.89320735009981, 121.516217427213, 49.9208245483352, 65.72341372043925, 108.7705690925511, 80.23270299974942, 125.85756445568929, 129.0375599769633, 77.07205830836442, 59.63430001067155, 148.42355324503535, 79.26352950759669, 74.8670374139299, 75.42673033994053, 81.2305735943421, 58.2263865650144, 71.7543240836095, 78.06434719653271, 73.07191537076267, 98.7046472694596, 78.07384432601403, 72.22571770541, 68.97069143266691, 1.6339896568949983, 1.2896727402150732, 2.8162858318778103, 1.2565454403724663, 11.631744942559543, 2.1149495361340214, 3.197662966372261, 1.179303923279131, 2.8512830633931223, 0.8601817754449198, 1.5151195008672294, 2.4938568635070593, 1.778774546315568, 33.85922161561579, 1.1068794611430917, 1.9088848139343544, 2.6298706884004397, 2.1021205241622334, 1.8835433539747974, 2.162343876898628, 0.8154030778507864, 0.803572686538068, 3.279278709872619, 0.8060019255858984, 0.7905124270461418, 12.231711598177181, 0.7991699579716199, 1.030427263851445, 3.90036546562451, 0.5306332051676886, 2.1655005860621936, 1.9314708751350558, 3.1512852026271623, 10.868990673570641, 4.87150054228799, 9.560570542986286, 37.949179645450414, 18.659143123356106, 34.91409033623933, 37.2721325099402, 97.74508967725791, 34.05991026050463, 65.54492175747828, 11.586100444369263, 13.946323983338155, 5.188855746252219, 22.241274758448498, 154.80731275517374, 88.8640539382565, 70.49036716209325, 74.28008116436752, 76.78770571096344, 44.23159370358786, 70.4380143956606, 130.80628731172538, 145.78377533973614, 82.50141124171074, 70.93537783769216, 118.14748567568084, 96.41593745593629, 97.02936473137858, 43.75166294231849, 52.64217203905534, 104.33875947724255, 98.89058158381783, 73.286813590867, 88.57957350500237, 76.13717546446749, 54.92111045957321, 72.70853745860092, 62.904712765811034, 73.11440820881921, 99.15682671550036, 69.82992576469309, 59.540247941934034, 60.9696791207478, 58.144545078497956, 56.961828591670816, 56.569389579088956, 5.181934427599667, 3.561724752091968, 22.877183681900565, 2.9664701199427563, 1.7737164914498138, 1.7435628386244417, 8.23475605380348, 0.8365496039356398, 2.229728887980152, 2.572188939207753, 0.8483924084185102, 1.1258621705415703, 1.6203919606300878, 14.315629733464101, 1.3677623084144286, 17.74218647632185, 2.169054276097802, 1.1548617499232239, 0.7974771470498103, 0.8450515975028294, 8.653476265876874, 3.7418552848543167, 1.8956753272795392, 1.9212443211777315, 0.570934658987615, 5.785658648972079, 4.267592595694364, 0.9839555967501408, 1.7394590293822132, 5.495453894161585, 5.424368852400203, 10.907129135675122, 10.503986264604688, 13.463728250940013, 15.673368668636016, 47.32212611983355, 6.389986839436685, 8.29606815061666, 7.891994360028135, 50.01763128633587, 17.886680780654665, 32.69316296691181, 7.9564289018083185, 148.5076249146981, 174.70543203347185, 31.781392620626097, 105.52492164673247, 83.03066120924964, 12.304385296591528, 47.52726494019361, 52.002277827263896, 23.330771994101646, 88.77367266354645, 42.54627780113411, 25.2352650649282, 40.96521667714588, 89.16501727541954, 34.02240659927423, 107.74003130209685, 47.63695065211223, 64.36813937987826, 74.67661544576593, 57.23697028886024, 46.66466848516042, 44.48753432696317, 50.807007261888735, 98.34549522004917, 58.45006307243042, 40.29235270064652, 71.19531297448125, 58.39558410271917, 103.0445854283002, 57.61635863829672, 79.87788509227627, 52.515921050330874, 70.57256954067675, 57.03150862205453, 75.92995459557015, 54.15800015254832, 57.46102572214635, 55.31552504364986, 50.33709605227417, 49.76624334464201, 3.693898748091021, 0.8732603343762975, 1.2560266830874673, 0.8189168447498237, 0.810982854686884, 0.9297729879391181, 2.4841231947215827, 0.9174462815410327, 1.6320958821576275, 0.4486032900980792, 3.1454249069856615, 1.199474729474359, 0.5847008847824191, 1.3126752493242, 1.0409513096656677, 0.9305857153183587, 1.17345961672149, 1.4447889183750489, 0.2834159677891349, 0.4296808129833499, 0.7101461189611439, 0.8622694203124738, 0.27908813747451655, 0.41859954389319487, 0.5669661522678687, 0.3887408043470733, 0.41875174386473357, 0.27907070977364484, 0.5651746080111475, 0.27823903415587264, 7.037837098377492, 1.642468639180466, 21.37113903193227, 1.951793603738471, 3.1917802520156147, 1.2639779254046257, 15.222492707065447, 15.295100485687657, 3.442564399701303, 19.107216592190852, 6.681801567799283, 8.308507845739017, 7.77131966907464, 1.7723353044632222, 3.509254245751006, 13.394913111376669, 5.653521366311678, 58.72120316742199, 27.81548454945746, 18.487675628006116, 90.39976476002013, 105.92109130661214, 4.719519646175818, 14.453642430397979, 79.44820314655128, 25.022506704172535, 31.694120669650186, 16.419886639234388, 16.97115593034534, 8.947124305761072, 89.54448337989946, 57.397296759083694, 43.453728745655695, 43.67783634670644, 21.93002504707519, 26.42872987498509, 72.77999518543191, 27.3642769292412, 56.340701807079206, 48.36335302794396, 41.26913252170474, 48.908515839812566, 54.48438826033714, 43.322107262948926, 39.00445776671496, 46.874128150923546, 56.36084963286009, 49.21290408644647, 55.612965933021165, 34.125154408939224, 33.06042996081806, 29.93875398064977, 32.54180404004855, 33.15992645956099, 35.448428273610986, 32.8594831894036, 34.55159364031933, 32.06738880442185, 30.846674970984516, 31.20474895774253, 4.666824126779247, 2.5627156259311836, 4.068961952708576, 0.5973585442758795, 2.0366053559543764, 1.7497747859073425, 0.8790981506759847, 23.91431767550086, 1.261607007741179, 0.9842338681350963, 1.1074127049177942, 1.7983856415625905, 0.9893971315774959, 1.3307563837095422, 0.3970907559363414, 1.5816534263847146, 2.622156814606987, 1.1506638532961198, 0.5147720308930698, 6.248578596536656, 1.1517114067961052, 0.7611013576484124, 1.1454619711824807, 3.0489077681008676, 1.3774851522500111, 0.2500492834880366, 0.8042121386189458, 1.8786264233495444, 4.38331476245879, 0.24115003832778825, 3.4163275696808317, 1.541816768103219, 3.073453019640297, 3.213494549329798, 4.420249024804545, 17.259462193160893, 82.86329639737798, 3.944392931931747, 7.004702554119318, 26.39380706711504, 4.15589064191754, 10.848949688866188, 19.026672339257455, 5.335628270893545, 67.72103037877307, 8.993468898820337, 6.300162560854433, 7.79868268461047, 59.81134528915345, 48.969976145671886, 39.270667267995904, 31.533500070827394, 43.17527662027649, 29.158602032114718, 11.620399792360935, 9.83039581318498, 24.320031680945288, 47.93613844591009, 31.768152054824142, 31.647131366936247, 15.732096919785526, 22.630913778323446, 12.829142335870163, 30.140146876881197, 41.131755700714855, 32.97470232746193, 53.55949662576994, 32.857465211309695, 26.79603093218884, 23.11951085856784, 35.06406625489903, 28.641276892651465, 30.966937712602768, 49.208050555108876, 32.6177402504444, 26.525741144617804, 33.71444279870899, 27.061044598100164, 32.80560686045106, 26.524372738732794, 28.58617104658944, 24.90299798105741, 0.9456115552614608, 1.1287515355692461, 0.5302296916382887, 0.8275848983077695, 0.20398087174975968, 2.031711573977505, 0.6735310817400988, 0.2656437404375353, 0.2666564041647748, 3.946070023715797, 0.20374874139936308, 3.6111064348053934, 0.3305254401531303, 2.9992612771444085, 0.2074183208107079, 0.4676314890889709, 0.3804210228962497, 0.45566965392631914, 0.4529835178561222, 1.2071345737948722, 0.193819299507095, 0.25704875247810743, 0.9442176471326421, 0.25505042150750307, 0.19673089422103757, 0.1333201087673572, 0.2495679113787326, 0.18820310188935613, 0.8149848313595015, 1.9639322141251498, 1.455238847857552, 1.5157021887305386, 1.2297232271741159, 2.9134740520692413, 1.4066724267230097, 0.6312146703551386, 0.9217547709066329, 1.8491183921084904, 2.5762719225981368, 4.207229786119992, 1.780907876732648, 2.169897988980229, 2.1480233489659453, 9.557621431162131, 1.7237861319199912, 63.037518386408266, 23.674686061578747, 4.145088440647632, 4.937417394849482, 4.236225867744988, 11.12915257375073, 12.836833825129792, 9.532049821143374, 22.464827404622685, 6.33188538401356, 22.25980591553934, 9.364313319507675, 8.588070328602578, 21.101443600512052, 14.34167357461138, 40.04157223597239, 25.69353976485478, 20.353376493573798, 10.047645616879302, 15.066067101696433, 20.160748501888484, 15.340568025349715, 31.118138804278857, 22.570441478030197, 17.13802331101381, 31.321587459632234, 13.390231679654148, 24.273424046284667, 13.249264880168283, 29.23979698580734, 17.390896419648232, 17.73613792748311, 23.608726135246503, 20.97565833988715, 25.444140222579335, 18.976701974198548, 22.390088075350967, 23.42567026911371, 19.781555044865357, 17.867138942573387, 17.552375817567786, 16.089071129725145, 15.060577497601097, 0.5514150029736705, 3.64242752578656, 0.6795288071959215, 1.74750133592313, 0.9375306240661567, 0.45578430973248363, 0.8025268399075459, 1.4431706290577246, 0.364040167436208, 0.26027754388619934, 0.19098457470930297, 0.1762937401146335, 0.5159265837326749, 0.37519993485451125, 0.7324897699004281, 0.5699381917525412, 0.18494950532691962, 0.11802636082359141, 0.18054852700708018, 0.36154364874575634, 0.34456501730956945, 0.17215564684607243, 0.16907881452257462, 0.6126798714254208, 0.16794097338683642, 6.293727421909998, 0.4059751487405931, 0.11853837199618461, 0.3064596495895148, 1.7130657992218783, 0.8629895291840581, 1.421187483279543, 0.7255039472714445, 0.6539912488049233, 0.7674122608611686, 0.9340290994377853, 5.184471123152189, 3.586296460873974, 2.1559524656343267, 1.2473873646497835, 1.4985821131193415, 3.5266839168634996, 1.309680282450612, 2.1343267388237823, 1.8062435101591323, 7.6685164822605065, 13.029567609573423, 26.463891707020817, 4.036881326903858, 2.697828257044713, 2.379803589211921, 6.435551697815591, 7.285111347998587, 12.395513162146841, 13.028498037339773, 12.828722848718419, 6.325608406320269, 32.51240430226257, 8.926883965210532, 4.986983988653158, 10.684430259641264, 14.26523138151557, 17.400411053890046, 11.574410290220467, 24.82838260217314, 17.04827981082947, 6.328389723674843, 14.289132812969834, 17.251502830693973, 8.219469319420867, 8.80178482084614, 18.011428891320364, 26.821005696230127, 27.98988348789595, 7.97055895602564, 21.649321831746853, 13.818257574619913, 14.342766061601827, 10.714850783973457, 19.730726881765126, 17.156063196415207, 13.385971414274179, 14.21522923754829, 13.57103747899152, 16.405452580169182, 14.474243264218961, 15.074970585393254, 14.341841103552378, 16.722680259187307, 13.167883201732616, 12.183442904221122], \"Total\": [1286.0, 1041.0, 688.0, 988.0, 1205.0, 1046.0, 891.0, 594.0, 774.0, 448.0, 937.0, 702.0, 537.0, 512.0, 674.0, 518.0, 520.0, 796.0, 512.0, 799.0, 651.0, 547.0, 544.0, 597.0, 490.0, 416.0, 140.0, 293.0, 591.0, 331.0, 20.873496811691304, 16.354218342758344, 41.381758670225224, 258.8096307550283, 7.214340642605594, 32.35098174621117, 3.0019780762835606, 2.9441466292515215, 16.61190959628304, 5.112340284057441, 4.05138921395435, 7.133784029888135, 6.070753056921025, 11.0120172235196, 2.9881836792225256, 29.679381398386834, 2.97471551725725, 2.964752608635596, 1.948245679254532, 2.9590892915633438, 2.9997681516712693, 4.026527539023008, 3.927376506399656, 2.981535515311151, 11.533357719809391, 2.9601026580000385, 5.0721522590764065, 2.9508755005913265, 3.9738235911394786, 6.991857765408478, 68.36426636006581, 25.89805950135635, 937.4490146802859, 26.873811324843306, 10.806278800448014, 9.265777549301863, 59.465700730449036, 14.879003387994047, 100.64093563246452, 60.6989725601021, 163.86797075542174, 89.97022813001851, 53.15036354457244, 280.40362705177057, 383.5079859353936, 87.61856030749402, 52.29415060868284, 124.8965660285748, 38.94050265005578, 109.9310172761371, 122.46711801363213, 1205.1782702475336, 796.5389811872275, 232.66664269588813, 331.7985291309385, 321.5164410414259, 282.8684738277409, 1041.1090794014465, 988.3668380897847, 1286.7065804685665, 267.5281711306679, 774.2778495356994, 702.8561796555684, 373.06563545329203, 549.059784892796, 579.994934919972, 361.11617019401916, 1046.716974569776, 799.7463272432241, 651.7295187699319, 674.657823131087, 891.1421391589545, 512.2745430439918, 356.0072210589595, 354.66029626606894, 498.0139175711803, 518.8523239025814, 345.8664315447301, 522.8355537698322, 448.1085196065393, 594.8489023134764, 591.8658346035966, 456.98415700558553, 537.9255898397998, 597.6923935598069, 544.8332121057057, 477.3055700648004, 23.947093453965106, 8.769847157091046, 3.9133656083250736, 6.542875595041853, 89.87126456547641, 6.847069079718707, 18.517673081208205, 9.943400534775213, 24.822864176226147, 5.722214662229305, 17.2289057483504, 23.808613186755966, 5.590352343716853, 5.639550812598285, 20.343228450545315, 7.868894080051951, 10.71472134500812, 35.598223408461735, 3.7054603786386537, 2.7954434183049632, 8.744386697763513, 5.87279961510509, 9.248170703909658, 94.6380631546264, 3.874415571358885, 55.32052923562, 2.9154158445280123, 10.159249826087386, 5.7656198492503865, 9.63766234693514, 37.00814877202266, 69.64037324729928, 37.927482508509016, 29.25338036023668, 56.972266254536805, 110.57206690100705, 71.74105901484143, 383.5079859353936, 30.63287879866233, 36.29147392916702, 229.75777811540524, 154.66057662121494, 1046.716974569776, 64.90277102611688, 651.7295187699319, 498.0139175711803, 270.5674432943957, 891.1421391589545, 113.1904024486552, 1286.7065804685665, 261.40922478481576, 215.97201247126048, 326.1176046477651, 416.4245722936561, 1205.1782702475336, 799.7463272432241, 937.4490146802859, 327.4400537143497, 537.9255898397998, 297.4728912703419, 394.85265195929617, 448.1085196065393, 688.0525830923598, 297.14924112591524, 456.98415700558553, 988.3668380897847, 293.1209101776674, 594.8489023134764, 327.4026698785376, 579.994934919972, 544.8332121057057, 1041.1090794014465, 674.657823131087, 365.92917409337605, 549.059784892796, 512.2745430439918, 702.8561796555684, 796.5389811872275, 591.8658346035966, 518.8523239025814, 520.3265433696762, 597.6923935598069, 547.4266231698, 37.42583612276102, 4.842818441707165, 9.705409321459538, 22.66198376656954, 4.87012138433884, 3.7971001904796178, 12.38988689178041, 35.250130516704296, 13.034073057318626, 11.680707483364257, 5.7961333777172275, 5.862431879737668, 13.332246764185873, 30.148543383492363, 3.8389210103816946, 3.86107667101129, 10.33315788077992, 2.870067810150376, 23.905127388702624, 2.8652194923053464, 7.650334551936177, 4.745970294864998, 3.771751625321963, 5.662292339188883, 4.7137451460551, 7.722554228552813, 2.883155593306745, 4.7212511868927605, 3.8460761206454204, 4.654000405477931, 61.75293768842579, 24.964103035577555, 14.17083846146619, 27.18811450919691, 43.406943663223196, 24.89331254448817, 547.4266231698, 122.43372331707802, 106.2295954737405, 139.01581641954618, 286.7899542180503, 333.79135284197173, 132.674294517511, 179.93912481557257, 688.0525830923598, 522.8355537698322, 81.19003903099977, 46.427392291827566, 203.10864223961394, 537.9255898397998, 597.6923935598069, 341.813963488139, 891.1421391589545, 1286.7065804685665, 284.8130087577546, 520.3265433696762, 191.58658596304278, 591.8658346035966, 1205.1782702475336, 544.8332121057057, 141.79546274264882, 343.26012896821345, 799.7463272432241, 327.98280900378916, 774.2778495356994, 549.059784892796, 674.657823131087, 331.24362533120325, 1041.1090794014465, 594.8489023134764, 796.5389811872275, 579.994934919972, 651.7295187699319, 373.06563545329203, 702.8561796555684, 410.5778655144649, 477.3055700648004, 937.4490146802859, 1046.716974569776, 498.0139175711803, 455.9895789873327, 988.3668380897847, 512.5204943275562, 490.9328048326748, 4.9091783285586965, 25.9223205480392, 6.975273310399184, 33.908819813939225, 5.299695349453345, 65.41552328501218, 8.836103632788046, 16.274246552289455, 43.79978126550145, 30.86211944305335, 7.517729767801026, 12.609751658447113, 7.000365624283876, 5.889907970207454, 14.458573447612675, 8.68450437299815, 17.404912586484006, 6.038642253099199, 41.237672792409654, 4.7502871380595915, 34.62879429117723, 4.5620892282852505, 11.95071598923493, 10.705324371086169, 2.905791084331183, 24.71688205238746, 5.943636144773679, 5.816387762123273, 19.409015813041197, 3.8747807692523444, 6.958968208949713, 21.798335410345818, 49.57483292429649, 22.062592056778346, 24.46501960454143, 43.24042371652291, 34.03472228704479, 83.94486823721407, 32.41825049731489, 172.28482710926195, 78.69040106792858, 33.73680943762945, 187.6661706840152, 796.5389811872275, 61.62040808983655, 437.4617965481396, 477.3055700648004, 702.8561796555684, 190.95390827015632, 252.81987522720047, 161.78653516391313, 191.58658596304278, 204.32808948344456, 115.00623858275229, 774.2778495356994, 1046.716974569776, 260.7096648727202, 594.8489023134764, 674.657823131087, 490.9328048326748, 297.14924112591524, 1205.1782702475336, 688.0525830923598, 891.1421391589545, 289.275749854278, 416.4245722936561, 799.7463272432241, 544.8332121057057, 988.3668380897847, 1041.1090794014465, 522.8355537698322, 368.90935877678925, 1286.7065804685665, 549.059784892796, 512.5204943275562, 520.3265433696762, 591.8658346035966, 365.92917409337605, 518.8523239025814, 597.6923935598069, 537.9255898397998, 937.4490146802859, 651.7295187699319, 579.994934919972, 547.4266231698, 4.436632965641832, 3.5508276782268338, 7.928036430145059, 3.5472122919679303, 34.22825048529213, 6.224793277816902, 9.569314040218956, 3.537455266222232, 8.649516167969992, 2.6523992202226125, 4.710069422924088, 7.816843200099557, 5.597344102470699, 106.61064370029904, 3.4978635091237806, 6.054380371092246, 8.417560022506313, 6.750234395997171, 6.112853068728048, 7.0257761104900185, 2.6631489291784463, 2.6448148541659995, 10.794027084336749, 2.6644488803961677, 2.6256497932623977, 40.85581656926896, 2.686903735085692, 3.468424995011751, 13.155805377537474, 1.7909439674500953, 7.318312348357205, 6.544356771687217, 10.843103944609325, 38.36068478059495, 16.972209050432415, 34.24352119266877, 151.31993760645784, 74.83593538595855, 157.6509817651282, 172.28482710926195, 512.5204943275562, 160.11035914158617, 343.26012896821345, 48.28919654046524, 61.12982201593234, 19.397436484464002, 105.89284202654027, 1046.716974569776, 579.994934919972, 455.9895789873327, 490.9328048326748, 522.8355537698322, 266.35925191214443, 477.3055700648004, 1041.1090794014465, 1205.1782702475336, 591.8658346035966, 512.2745430439918, 988.3668380897847, 774.2778495356994, 796.5389811872275, 286.07734433013286, 368.90935877678925, 937.4490146802859, 891.1421391589545, 597.6923935598069, 799.7463272432241, 688.0525830923598, 414.9557686616432, 651.7295187699319, 518.8523239025814, 674.657823131087, 1286.7065804685665, 702.8561796555684, 498.0139175711803, 549.059784892796, 547.4266231698, 544.8332121057057, 594.8489023134764, 17.011727467181757, 12.552469465728693, 81.19003903099977, 10.970609109691912, 6.802264744293446, 6.808072417696628, 32.53220954844107, 3.3991235930694406, 9.093859292180566, 10.673409121134597, 3.5546250376080963, 4.778838335278514, 6.925093723235647, 61.38259441942079, 5.878861972940224, 76.31381512149258, 9.331779259431919, 4.974364664830614, 3.4397679919038717, 3.749680613604766, 38.638389600602615, 16.754594581474684, 8.489512204281674, 8.622597013908425, 2.564095949139733, 26.179429919903107, 19.399527840340607, 4.555334073859155, 8.068593804244694, 25.527344362367792, 25.50122446609013, 51.77054825562817, 50.314877010672674, 65.43190063042555, 76.82531211963067, 256.7074377066641, 30.63287879866233, 41.07425596694795, 38.94050265005578, 287.0865822972259, 94.84098902032096, 186.23202024281193, 39.57217083854799, 988.3668380897847, 1286.7065804685665, 194.1251658141284, 774.2778495356994, 597.6923935598069, 65.59053068323459, 318.3130546855174, 359.9585570082605, 142.1739408650082, 702.8561796555684, 297.4728912703419, 160.11035914158617, 293.1209101776674, 799.7463272432241, 239.76136563898902, 1041.1090794014465, 368.90935877678925, 547.4266231698, 674.657823131087, 477.3055700648004, 365.92917409337605, 345.8664315447301, 414.9557686616432, 1046.716974569776, 512.2745430439918, 303.55768545246656, 688.0525830923598, 520.3265433696762, 1205.1782702475336, 518.8523239025814, 891.1421391589545, 456.98415700558553, 796.5389811872275, 544.8332121057057, 937.4490146802859, 549.059784892796, 651.7295187699319, 591.8658346035966, 498.0139175711803, 579.994934919972, 17.22172610309975, 4.298230882574358, 6.1899993668578706, 4.304437764887478, 4.291551066584811, 5.188303975648507, 13.873364534312511, 5.249486859102871, 9.530420287776387, 2.622280029014964, 18.422036143830915, 7.0257761104900185, 3.4515121929551738, 7.833783405262243, 6.215510737835141, 5.559682480466634, 7.019177888453748, 8.826832327629223, 1.73362109843743, 2.6496096615844, 4.380157253572239, 5.320770281729246, 1.7229153550100842, 2.586475799855916, 3.511709307901392, 2.4137636609334856, 2.605348176402548, 1.7504175280653507, 3.5534083458285872, 1.7547660300176273, 45.194344357431675, 10.485326561974382, 140.8296285785366, 12.51792463568302, 20.784617069975923, 8.057591879072545, 105.89284202654027, 107.60274148597955, 23.001244239065148, 139.09117243908042, 47.33698142173738, 60.40739006253658, 56.3799440259474, 11.613266681012888, 24.46520764651013, 107.16670460581682, 41.66004351577828, 594.8489023134764, 259.8270855165634, 163.80476033895317, 1041.1090794014465, 1286.7065804685665, 34.878113469849026, 128.4765099649989, 988.3668380897847, 250.83473051008198, 333.79135284197173, 155.188408963082, 161.59254359424614, 74.83593538595855, 1205.1782702475336, 702.8561796555684, 512.5204943275562, 520.3265433696762, 223.35670108190592, 287.37699014967444, 1046.716974569776, 310.8876233977584, 796.5389811872275, 651.7295187699319, 544.8332121057057, 688.0525830923598, 799.7463272432241, 591.8658346035966, 518.8523239025814, 674.657823131087, 891.1421391589545, 774.2778495356994, 937.4490146802859, 455.9895789873327, 448.1085196065393, 373.06563545329203, 437.4617965481396, 456.98415700558553, 549.059784892796, 477.3055700648004, 597.6923935598069, 547.4266231698, 498.0139175711803, 579.994934919972, 22.868008892648565, 13.612249758055444, 21.792212131172025, 3.3769082938385226, 11.727951122154355, 10.130981252098602, 5.157315214206144, 140.8296285785366, 7.594285415403135, 5.940514467729519, 6.847683625891314, 11.321317041771389, 6.31046936260425, 8.53620737219728, 2.5624845867407884, 10.217911420728475, 17.05183128036484, 7.569998443204342, 3.4253967761184354, 41.66004351577828, 7.723257105944112, 5.160969850479692, 7.776432699815192, 20.907964878735914, 9.487523647151823, 1.7264558414838247, 5.559682480466634, 13.147231531950005, 30.76725922532682, 1.694467244434409, 24.215120274435577, 10.929407343811349, 22.439743603108642, 23.985236623491208, 34.878113469849026, 157.6509817651282, 1041.1090794014465, 31.78999849581801, 61.62040808983655, 288.64238400255385, 34.24352119266877, 106.61064370029904, 208.30046939990135, 46.01959729624313, 1046.716974569776, 88.84888365196069, 58.25536194782542, 76.31381512149258, 988.3668380897847, 774.2778495356994, 594.8489023134764, 456.98415700558553, 688.0525830923598, 416.4245722936561, 128.1128379953197, 105.3966582397921, 356.0072210589595, 891.1421391589545, 512.5204943275562, 518.8523239025814, 199.9010910474502, 331.7985291309385, 151.2344463871977, 512.2745430439918, 799.7463272432241, 591.8658346035966, 1205.1782702475336, 597.6923935598069, 448.1085196065393, 361.11617019401916, 674.657823131087, 498.0139175711803, 579.994934919972, 1286.7065804685665, 651.7295187699319, 490.9328048326748, 796.5389811872275, 544.8332121057057, 937.4490146802859, 537.9255898397998, 702.8561796555684, 549.059784892796, 10.673409121134597, 13.353936674011232, 6.274320203772805, 9.968876850235787, 2.533038894566718, 25.428665204507492, 8.446196858150966, 3.338390438144465, 3.356905670773828, 49.73262103807641, 2.5940494325133434, 46.34791205804529, 4.252422761466919, 38.638389600602615, 2.698841182301409, 6.099507495725806, 4.974364664830614, 5.959167461688448, 5.9272002034641025, 15.820323690783683, 2.5546693188307796, 3.388546311645654, 12.552469465728693, 3.3991235930694406, 2.6247997764195046, 1.7807884982789914, 3.3358567004165884, 2.5253449523129086, 10.970609109691912, 26.597543132603263, 19.73596619868444, 20.7464796417034, 17.011727467181757, 41.07425596694795, 19.56482588963154, 8.622597013908425, 12.960789205452862, 26.97195556522801, 38.65718833394242, 65.41552328501218, 26.179429919903107, 32.53220954844107, 32.23499583582179, 163.80476033895317, 25.48343471340719, 1286.7065804685665, 448.1085196065393, 67.4164924288502, 82.41587356164344, 70.10500040211592, 212.45318303042464, 256.7074377066641, 186.23202024281193, 512.2745430439918, 115.86542587448113, 518.8523239025814, 187.6661706840152, 170.72104143381537, 520.3265433696762, 327.67014006661555, 1205.1782702475336, 702.8561796555684, 537.9255898397998, 222.44386268479758, 373.06563545329203, 547.4266231698, 386.3964121134187, 988.3668380897847, 651.7295187699319, 455.9895789873327, 1046.716974569776, 333.79135284197173, 774.2778495356994, 331.7985291309385, 1041.1090794014465, 490.9328048326748, 512.5204943275562, 796.5389811872275, 674.657823131087, 937.4490146802859, 591.8658346035966, 799.7463272432241, 891.1421391589545, 688.0525830923598, 597.6923935598069, 594.8489023134764, 549.059784892796, 477.3055700648004, 5.34891702166716, 41.66004351577828, 7.8558866083555445, 20.343228450545315, 10.929407343811349, 5.474794259117331, 9.831764334355071, 17.825782465452754, 4.7284567129173, 3.3922774476563835, 2.540834582259165, 2.39585012913888, 7.019177888453748, 5.135263479514969, 10.140823893196892, 7.892713165092855, 2.564620543872808, 1.6528280011226952, 2.5346017764984587, 5.094097888609249, 4.8718202905318675, 2.4375479011439576, 2.4008151452821376, 8.715852054893732, 2.3968482492935084, 89.87126456547641, 5.830060043141087, 1.7128342526432896, 4.428612858222963, 24.822864176226147, 12.53240934495567, 20.896971561326545, 10.593730089063204, 9.530420287776387, 11.402773029046333, 14.035899661355195, 85.25198624052811, 58.15731588374609, 34.878113469849026, 19.406364094852467, 23.808613186755966, 61.62040808983655, 20.768532198720976, 35.598223408461735, 29.89329666328467, 157.6509817651282, 293.03481141248847, 688.0525830923598, 78.69040106792858, 49.292080213215314, 42.9551514552068, 140.8296285785366, 163.80476033895317, 310.8876233977584, 331.24362533120325, 326.1176046477651, 139.20424809580578, 1041.1090794014465, 218.3767292018041, 106.90134987499172, 282.8684738277409, 416.4245722936561, 537.9255898397998, 323.56395970026773, 891.1421391589545, 544.8332121057057, 148.27678434620805, 448.1085196065393, 594.8489023134764, 215.97201247126048, 238.4540633019336, 674.657823131087, 1205.1782702475336, 1286.7065804685665, 208.0952004594036, 937.4490146802859, 490.9328048326748, 547.4266231698, 338.89506591970763, 988.3668380897847, 799.7463272432241, 520.3265433696762, 597.6923935598069, 549.059784892796, 796.5389811872275, 651.7295187699319, 774.2778495356994, 702.8561796555684, 1046.716974569776, 579.994934919972, 498.0139175711803], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -8.2623, -8.5664, -7.6396, -5.8285, -9.4293, -7.9472, -10.3532, -10.3738, -8.6455, -9.8268, -10.0688, -9.5086, -9.6792, -9.0887, -10.3954, -8.1038, -10.4204, -10.4312, -10.8517, -10.4407, -10.4341, -10.1436, -10.1688, -10.4449, -9.0973, -10.459, -9.9209, -10.4701, -10.1764, -9.6122, -7.3425, -8.3293, -4.8592, -8.3066, -9.1926, -9.3462, -7.6004, -8.9075, -7.1358, -7.6283, -6.7224, -7.2815, -7.7613, -6.2488, -5.9709, -7.3129, -7.7787, -7.0088, -8.0541, -7.1464, -7.0515, -5.0784, -5.4392, -6.5137, -6.2149, -6.2469, -6.3564, -5.2647, -5.3252, -5.1286, -6.4269, -5.5734, -5.6735, -6.1773, -5.8783, -5.8416, -6.211, -5.4085, -5.6156, -5.7784, -5.7675, -5.5775, -5.9743, -6.2404, -6.245, -6.0351, -6.0262, -6.2757, -6.0451, -6.1423, -6.0098, -6.0298, -6.1392, -6.0819, -6.1039, -6.1453, -6.1702, -8.2638, -9.272, -10.0873, -9.574, -6.9777, -9.5694, -8.5939, -9.2227, -8.3222, -9.7937, -8.6972, -8.3756, -9.8363, -9.8351, -8.5552, -9.5074, -9.2015, -8.0057, -10.2707, -10.5535, -9.4158, -9.815, -9.3643, -7.0433, -10.2575, -7.6004, -10.5485, -9.3019, -9.8815, -9.3824, -8.0729, -7.4814, -8.0686, -8.3172, -7.6957, -7.0764, -7.492, -5.9643, -8.2911, -8.1409, -6.4836, -6.8447, -5.171, -7.6427, -5.62, -5.8579, -6.3942, -5.3634, -7.1563, -5.0686, -6.4518, -6.6157, -6.2721, -6.0714, -5.1924, -5.5404, -5.411, -6.2763, -5.871, -6.3586, -6.1404, -6.0416, -5.7091, -6.3701, -6.0484, -5.4671, -6.3972, -5.8962, -6.3244, -5.9307, -5.9987, -5.6113, -5.8826, -6.2625, -6.0556, -6.0994, -5.948, -5.8902, -6.0393, -6.1164, -6.1214, -6.0718, -6.1244, -7.6893, -9.7721, -9.1209, -8.3264, -9.8739, -10.1293, -8.9563, -7.9119, -8.9094, -9.0285, -9.7319, -9.7218, -8.9046, -8.0893, -10.1568, -10.1564, -9.1741, -10.4605, -8.3421, -10.4808, -9.5018, -9.9884, -10.2192, -9.8137, -10.0085, -9.5213, -10.5151, -10.0299, -10.2385, -10.0482, -7.5063, -8.3969, -8.9585, -8.327, -7.8899, -8.438, -5.6045, -6.9871, -7.1443, -6.912, -6.2698, -6.146, -6.978, -6.7106, -5.5459, -5.8004, -7.4233, -7.9165, -6.6371, -5.8058, -5.721, -6.2114, -5.4089, -5.1117, -6.3702, -5.8701, -6.7058, -5.7859, -5.2098, -5.8867, -6.9717, -6.2716, -5.631, -6.3183, -5.6646, -5.9267, -5.7854, -6.3146, -5.4971, -5.896, -5.6898, -5.9219, -5.8531, -6.236, -5.8664, -6.2145, -6.1241, -5.7756, -5.7418, -6.1111, -6.158, -5.8694, -6.1308, -6.1469, -9.7462, -8.0889, -9.4665, -7.9597, -9.8317, -7.3303, -9.3478, -8.7514, -7.7684, -8.1224, -9.5393, -9.0261, -9.6222, -9.8056, -8.9109, -9.4266, -8.7329, -9.7924, -7.8713, -10.0336, -8.0476, -10.0754, -9.1163, -9.233, -10.5378, -8.3975, -9.8256, -9.8473, -8.6465, -10.2598, -9.6768, -8.5586, -7.78, -8.5579, -8.4678, -7.9268, -8.1573, -7.3015, -8.2167, -6.6853, -7.4092, -8.1815, -6.6274, -5.3272, -7.6452, -5.909, -5.8464, -5.5281, -6.6628, -6.4234, -6.8256, -6.6819, -6.6348, -7.1324, -5.5231, -5.2792, -6.4521, -5.768, -5.6706, -5.9348, -6.3548, -5.2078, -5.6666, -5.4904, -6.3801, -6.105, -5.6012, -5.9056, -5.4553, -5.4304, -5.9457, -6.2023, -5.2904, -5.9177, -5.9748, -5.9673, -5.8932, -6.2262, -6.0172, -5.933, -5.999, -5.6984, -5.9328, -6.0107, -6.0568, -9.6793, -9.9159, -9.1349, -9.9419, -7.7165, -9.4212, -9.0079, -10.0054, -9.1225, -10.3209, -9.7548, -9.2564, -9.5944, -6.6481, -10.0687, -9.5238, -9.2033, -9.4273, -9.5371, -9.3991, -10.3744, -10.389, -8.9827, -10.3859, -10.4054, -7.6662, -10.3945, -10.1403, -8.8092, -10.804, -9.3976, -9.512, -9.0225, -7.7844, -8.5869, -7.9126, -6.534, -7.2439, -6.6174, -6.552, -5.5879, -6.6422, -5.9875, -7.7205, -7.5351, -8.5238, -7.0683, -5.1281, -5.6832, -5.9148, -5.8624, -5.8292, -6.3808, -5.9155, -5.2966, -5.1882, -5.7575, -5.9085, -5.3983, -5.6016, -5.5953, -6.3918, -6.2068, -5.5226, -5.5763, -5.8759, -5.6864, -5.8377, -6.1644, -5.8838, -6.0287, -5.8783, -5.5736, -5.9242, -6.0836, -6.0599, -6.1073, -6.1279, -6.1348, -8.4197, -8.7946, -6.9347, -8.9775, -9.4918, -9.5089, -7.9565, -10.2433, -9.263, -9.1201, -10.2293, -9.9463, -9.5822, -7.4035, -9.7517, -7.1889, -9.2905, -9.9209, -10.2911, -10.2332, -7.9069, -8.7453, -9.4253, -9.4119, -10.6253, -8.3095, -8.6138, -10.081, -9.5113, -8.3609, -8.3739, -7.6754, -7.7131, -7.4648, -7.3129, -6.2079, -8.2101, -7.9491, -7.999, -6.1525, -7.1808, -6.5777, -7.9909, -5.0642, -4.9017, -6.606, -5.4059, -5.6456, -7.5549, -6.2035, -6.1136, -6.9151, -5.5787, -6.3142, -6.8366, -6.3521, -5.5743, -6.5378, -5.3851, -6.2012, -5.9002, -5.7517, -6.0176, -6.2219, -6.2696, -6.1368, -5.4764, -5.9967, -6.3687, -5.7994, -5.9976, -5.4297, -6.011, -5.6843, -6.1037, -5.8082, -6.0212, -5.735, -6.0729, -6.0137, -6.0518, -6.1461, -6.1575, -8.3784, -9.8206, -9.4571, -9.8848, -9.8946, -9.7579, -8.7751, -9.7712, -9.1952, -10.4867, -8.5391, -9.5032, -10.2217, -9.413, -9.6449, -9.757, -9.5251, -9.3171, -10.9459, -10.5298, -10.0273, -9.8332, -10.9613, -10.5559, -10.2525, -10.6299, -10.5555, -10.9613, -10.2557, -10.9643, -7.7337, -9.1888, -6.623, -9.0163, -8.5245, -9.4508, -6.9623, -6.9575, -8.4488, -6.735, -7.7857, -7.5678, -7.6346, -9.1128, -8.4296, -7.0902, -7.9528, -5.6122, -6.3595, -6.7679, -5.1808, -5.0224, -8.1333, -7.0141, -5.3099, -6.4653, -6.2289, -6.8866, -6.8535, -7.4937, -5.1903, -5.6351, -5.9134, -5.9082, -6.5972, -6.4106, -5.3976, -6.3758, -5.6536, -5.8063, -5.9649, -5.7951, -5.6871, -5.9164, -6.0214, -5.8376, -5.6533, -5.7889, -5.6666, -6.155, -6.1867, -6.2859, -6.2025, -6.1837, -6.117, -6.1928, -6.1426, -6.2172, -6.256, -6.2445, -7.862, -8.4614, -7.9991, -9.9177, -8.6912, -8.843, -9.5313, -6.228, -9.1701, -9.4183, -9.3004, -8.8156, -9.4131, -9.1167, -10.326, -8.944, -8.4385, -9.2621, -10.0665, -7.5701, -9.2612, -9.6754, -9.2666, -8.2877, -9.0822, -10.7885, -9.6203, -8.7719, -7.9246, -10.8248, -8.1739, -8.9695, -8.2797, -8.2351, -7.9163, -6.5541, -4.9853, -8.0302, -7.4559, -6.1293, -7.9779, -7.0184, -6.4566, -7.728, -5.1871, -7.206, -7.5619, -7.3485, -5.3113, -5.5112, -5.732, -5.9514, -5.6372, -6.0297, -6.9497, -7.117, -6.2112, -5.5326, -5.944, -5.9478, -6.6467, -6.2831, -6.8507, -5.9966, -5.6857, -5.9067, -5.4217, -5.9103, -6.1142, -6.2618, -5.8453, -6.0476, -5.9695, -5.5064, -5.9176, -6.1243, -5.8845, -6.1044, -5.9119, -6.1244, -6.0495, -6.1875, -9.0098, -8.8328, -9.5883, -9.1431, -10.5436, -8.245, -9.3491, -10.2795, -10.2757, -7.5812, -10.5448, -7.6699, -10.061, -7.8555, -10.5269, -9.714, -9.9204, -9.7399, -9.7458, -8.7656, -10.5947, -10.3124, -9.0113, -10.3202, -10.5798, -10.9689, -10.3419, -10.6241, -9.1585, -8.2789, -8.5787, -8.538, -8.7471, -7.8846, -8.6127, -9.414, -9.0354, -8.3392, -8.0076, -7.5171, -8.3768, -8.1792, -8.1893, -6.6966, -8.4094, -4.8102, -5.7895, -7.532, -7.3571, -7.5102, -6.5443, -6.4016, -6.6992, -5.8419, -7.1083, -5.8511, -6.717, -6.8035, -5.9046, -6.2907, -5.264, -5.7077, -5.9407, -6.6466, -6.2415, -5.9502, -6.2234, -5.5161, -5.8373, -6.1126, -5.5096, -6.3594, -5.7645, -6.37, -5.5784, -6.098, -6.0783, -5.7923, -5.9105, -5.7174, -6.0107, -5.8453, -5.8001, -5.9691, -6.0709, -6.0887, -6.1758, -6.2418, -9.3194, -7.4315, -9.1105, -8.166, -8.7887, -9.5099, -8.9441, -8.3573, -9.7346, -10.0702, -10.3797, -10.4598, -9.3859, -9.7044, -9.0355, -9.2864, -10.4118, -10.861, -10.4359, -9.7415, -9.7896, -10.4835, -10.5015, -9.2141, -10.5083, -6.8846, -9.6256, -10.8567, -9.9068, -8.1859, -8.8715, -8.3727, -9.045, -9.1488, -8.9889, -8.7924, -7.0785, -7.447, -7.9559, -8.5031, -8.3196, -7.4638, -8.4544, -7.966, -8.1329, -6.687, -6.1569, -5.4484, -7.3287, -7.7317, -7.8571, -6.8623, -6.7383, -6.2068, -6.157, -6.1725, -6.8795, -5.2425, -6.5351, -7.1173, -6.3554, -6.0663, -5.8677, -6.2754, -5.5122, -5.8881, -6.8791, -6.0646, -5.8762, -6.6176, -6.5492, -5.8331, -5.435, -5.3923, -6.6484, -5.6492, -6.0982, -6.0609, -6.3525, -5.742, -5.8818, -6.1299, -6.0698, -6.1162, -5.9265, -6.0518, -6.0111, -6.061, -5.9074, -6.1464, -6.2241], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.0134, 0.9532, 0.9517, 0.9296, 0.9088, 0.8902, 0.8617, 0.8605, 0.8585, 0.8556, 0.8463, 0.8407, 0.8314, 0.8264, 0.8241, 0.8199, 0.8036, 0.7961, 0.7955, 0.7886, 0.7814, 0.7776, 0.7773, 0.7768, 0.7716, 0.7699, 0.7695, 0.7619, 0.758, 0.7571, 0.7468, 0.7306, 0.6117, 0.7164, 0.7413, 0.7416, 0.6283, 0.7066, 0.5668, 0.5799, 0.4927, 0.5331, 0.5796, 0.4291, 0.3938, 0.5282, 0.5785, 0.4779, 0.598, 0.4679, 0.4548, 0.1414, 0.1946, 0.3508, 0.2947, 0.2942, 0.3127, 0.1013, 0.0929, 0.0256, 0.298, 0.0887, 0.0854, 0.215, 0.1276, 0.1095, 0.2139, -0.0478, 0.0142, 0.056, 0.0324, -0.0559, 0.101, 0.1987, 0.1979, 0.0684, 0.0363, 0.1923, 0.0098, 0.0667, -0.084, -0.099, 0.0502, -0.0555, -0.1829, -0.1317, -0.0242, 0.8745, 0.8708, 0.8624, 0.8618, 0.838, 0.8209, 0.8015, 0.7945, 0.7801, 0.7761, 0.7703, 0.7684, 0.7568, 0.7492, 0.7462, 0.7438, 0.7409, 0.7361, 0.7336, 0.7327, 0.7299, 0.7288, 0.7254, 0.7208, 0.7022, 0.7006, 0.6956, 0.6938, 0.6807, 0.666, 0.6301, 0.5893, 0.6099, 0.6209, 0.5759, 0.532, 0.5491, 0.4004, 0.6009, 0.5817, 0.3935, 0.4282, 0.1897, 0.4985, 0.2145, 0.2455, 0.3194, 0.1582, 0.4287, 0.0857, 0.2963, 0.3232, 0.2548, 0.211, 0.0273, 0.0894, 0.06, 0.2465, 0.1554, 0.2602, 0.1951, 0.1674, 0.0711, 0.2498, 0.1411, -0.0491, 0.2363, 0.0296, 0.1985, 0.0204, 0.0149, -0.2452, -0.0827, 0.1492, -0.0497, -0.0242, -0.1891, -0.2563, -0.1085, -0.0539, -0.0618, -0.1507, -0.1155, 1.0024, 0.9645, 0.9205, 0.8671, 0.8571, 0.8506, 0.8409, 0.8397, 0.8372, 0.8277, 0.8251, 0.8238, 0.8194, 0.8187, 0.8121, 0.8068, 0.8047, 0.7993, 0.7979, 0.7806, 0.7776, 0.7684, 0.7673, 0.7666, 0.7552, 0.7486, 0.7401, 0.7321, 0.7286, 0.7281, 0.6846, 0.6998, 0.7044, 0.6843, 0.6536, 0.6615, 0.4044, 0.5194, 0.5042, 0.4675, 0.3855, 0.3576, 0.4482, 0.4109, 0.2343, 0.2544, 0.494, 0.5597, 0.3632, 0.2205, 0.2, 0.2684, 0.1127, 0.0425, 0.292, 0.1895, 0.353, 0.1449, 0.01, 0.127, 0.388, 0.204, -0.0012, 0.2029, -0.0024, 0.0792, 0.0145, 0.1967, -0.1311, 0.0298, -0.0559, 0.0292, -0.0187, 0.1563, -0.1074, 0.0821, 0.0218, -0.3047, -0.3811, -0.0076, 0.0337, -0.4513, -0.056, -0.0291, 0.9768, 0.9701, 0.9052, 0.8307, 0.8147, 0.8031, 0.7875, 0.7731, 0.7661, 0.7622, 0.7576, 0.7536, 0.7459, 0.7353, 0.7319, 0.726, 0.7245, 0.7236, 0.7234, 0.7223, 0.7219, 0.7209, 0.7171, 0.7104, 0.7096, 0.7091, 0.7062, 0.7061, 0.7019, 0.6998, 0.6973, 0.6737, 0.6306, 0.6623, 0.6491, 0.6206, 0.6294, 0.5824, 0.6187, 0.4797, 0.5393, 0.6141, 0.4521, 0.3066, 0.5479, 0.3241, 0.2995, 0.2309, 0.3993, 0.358, 0.4023, 0.3769, 0.3596, 0.4367, 0.1391, 0.0815, 0.2986, 0.1578, 0.1293, 0.183, 0.2651, 0.012, 0.1137, 0.0312, 0.2667, 0.1774, 0.0286, 0.1081, -0.0373, -0.0643, 0.1091, 0.2013, -0.1362, 0.0882, 0.1, 0.0923, 0.0376, 0.1855, 0.0452, -0.0119, 0.0273, -0.2274, -0.0984, -0.0596, -0.0479, 1.145, 1.131, 1.1088, 1.106, 1.0645, 1.0643, 1.0477, 1.0453, 1.0341, 1.0178, 1.0096, 1.0014, 0.9975, 0.9969, 0.9932, 0.9896, 0.9804, 0.9772, 0.9666, 0.9654, 0.9602, 0.9525, 0.9525, 0.9482, 0.9434, 0.9378, 0.9313, 0.9301, 0.928, 0.9274, 0.9261, 0.9235, 0.9081, 0.8827, 0.8957, 0.868, 0.7607, 0.7549, 0.6363, 0.6129, 0.4868, 0.5961, 0.4881, 0.7164, 0.666, 0.8252, 0.5833, 0.2326, 0.2679, 0.2768, 0.2554, 0.2256, 0.3484, 0.2304, 0.0695, 0.0316, 0.1734, 0.1667, 0.0197, 0.0606, 0.0386, 0.2661, 0.1968, -0.0517, -0.0547, 0.0451, -0.0566, -0.0575, 0.1216, -0.0493, 0.0338, -0.0784, -0.4193, -0.1653, 0.0199, -0.054, -0.0985, -0.1143, -0.209, 1.0605, 0.9896, 0.9826, 0.9414, 0.9051, 0.8871, 0.8754, 0.8473, 0.8435, 0.8263, 0.8166, 0.8036, 0.7968, 0.7935, 0.7911, 0.7904, 0.7901, 0.789, 0.7876, 0.7592, 0.753, 0.7502, 0.75, 0.7479, 0.7472, 0.7397, 0.7351, 0.7168, 0.7149, 0.7134, 0.7014, 0.6919, 0.6827, 0.6683, 0.6597, 0.5583, 0.6819, 0.6497, 0.6531, 0.5019, 0.5811, 0.5094, 0.6451, 0.3538, 0.2525, 0.4396, 0.2563, 0.2754, 0.5758, 0.3475, 0.3146, 0.442, 0.1802, 0.3045, 0.4016, 0.2814, 0.0555, 0.2966, -0.0191, 0.2023, 0.1087, 0.0482, 0.1283, 0.1898, 0.1984, 0.1491, -0.1157, 0.0786, 0.2299, -0.0192, 0.0621, -0.21, 0.0515, -0.1627, 0.0857, -0.1744, -0.0076, -0.2641, -0.067, -0.1793, -0.121, -0.0426, -0.2064, 1.0896, 1.0353, 1.0341, 0.9696, 0.9629, 0.9098, 0.909, 0.8848, 0.8644, 0.8634, 0.8615, 0.8614, 0.8536, 0.8427, 0.8421, 0.8416, 0.8404, 0.8192, 0.818, 0.8099, 0.8097, 0.8093, 0.8088, 0.8079, 0.8055, 0.803, 0.801, 0.7929, 0.7905, 0.7874, 0.7694, 0.7753, 0.7435, 0.7706, 0.7554, 0.7767, 0.6894, 0.6781, 0.7297, 0.644, 0.6712, 0.6452, 0.6474, 0.7492, 0.6872, 0.5495, 0.6318, 0.3135, 0.3946, 0.4475, 0.1853, 0.1319, 0.6289, 0.4443, 0.1081, 0.324, 0.2747, 0.3829, 0.3755, 0.5051, 0.0294, 0.1239, 0.1614, 0.1514, 0.3081, 0.2427, -0.0369, 0.1989, -0.0198, 0.0282, 0.0487, -0.0149, -0.0573, 0.0144, 0.0411, -0.0377, -0.1317, -0.1267, -0.1957, 0.0366, 0.0224, 0.1065, 0.0306, 0.0057, -0.1111, -0.0469, -0.2216, -0.2083, -0.1525, -0.2934, 1.3224, 1.2418, 1.2335, 1.1795, 1.161, 1.1555, 1.1424, 1.1386, 1.1166, 1.114, 1.0898, 1.0719, 1.0588, 1.0531, 1.0471, 1.046, 1.0394, 1.0278, 1.0164, 1.0145, 1.0087, 0.9975, 0.9964, 0.9863, 0.9819, 0.9795, 0.9782, 0.966, 0.963, 0.9619, 0.9532, 0.9532, 0.9236, 0.9016, 0.846, 0.6996, 0.3808, 0.8248, 0.7372, 0.5196, 0.8027, 0.6265, 0.5185, 0.757, 0.1736, 0.6212, 0.6874, 0.6308, 0.1068, 0.1509, 0.1938, 0.2381, 0.1431, 0.2527, 0.5115, 0.5394, 0.228, -0.011, 0.1308, 0.1147, 0.3695, 0.2264, 0.4445, 0.0787, -0.0559, 0.0241, -0.2019, 0.0108, 0.0949, 0.1631, -0.0454, 0.0559, -0.0184, -0.3521, -0.0831, -0.0065, -0.2507, -0.0907, -0.4409, -0.098, -0.2906, -0.1816, 0.9365, 0.8895, 0.8893, 0.8715, 0.8411, 0.8332, 0.8313, 0.8291, 0.8274, 0.8263, 0.8161, 0.808, 0.8056, 0.8043, 0.7944, 0.7919, 0.7894, 0.7893, 0.7888, 0.7872, 0.7815, 0.7813, 0.7729, 0.7704, 0.7693, 0.7681, 0.7675, 0.7636, 0.7604, 0.7543, 0.7529, 0.7437, 0.7331, 0.7142, 0.7277, 0.7457, 0.7168, 0.6801, 0.6518, 0.6163, 0.6724, 0.6527, 0.6517, 0.5189, 0.6667, 0.3441, 0.4196, 0.5712, 0.5453, 0.5539, 0.4111, 0.3646, 0.3879, 0.2333, 0.4534, 0.2114, 0.3624, 0.3706, 0.1551, 0.2314, -0.0443, 0.0513, 0.0857, 0.2629, 0.1509, 0.0587, 0.1338, -0.0981, -0.0028, 0.079, -0.1489, 0.1442, -0.1023, 0.1396, -0.2123, 0.0198, -0.0035, -0.1585, -0.1106, -0.2465, -0.0799, -0.2155, -0.2785, -0.1889, -0.1499, -0.1629, -0.1699, -0.0959, 1.3178, 1.1531, 1.1423, 1.1354, 1.134, 1.1041, 1.0843, 1.0762, 1.0259, 1.0224, 1.0019, 0.9806, 0.9795, 0.9735, 0.9621, 0.9618, 0.9605, 0.9506, 0.9482, 0.9445, 0.941, 0.9396, 0.9368, 0.9349, 0.9317, 0.9311, 0.9255, 0.9193, 0.9192, 0.9165, 0.9143, 0.9018, 0.9088, 0.9108, 0.8914, 0.8801, 0.79, 0.8039, 0.8063, 0.8454, 0.8244, 0.7293, 0.8263, 0.7758, 0.7836, 0.5667, 0.4769, 0.3319, 0.6199, 0.6846, 0.6968, 0.5042, 0.4771, 0.3679, 0.3542, 0.3544, 0.4986, 0.1235, 0.3928, 0.5249, 0.3138, 0.2161, 0.1587, 0.2594, 0.0094, 0.1255, 0.4359, 0.1444, 0.0495, 0.3213, 0.2907, -0.0332, -0.2152, -0.238, 0.3277, -0.1782, 0.0196, -0.052, 0.1359, -0.3239, -0.252, -0.0703, -0.1488, -0.1103, -0.2927, -0.2173, -0.3489, -0.302, -0.5467, -0.1953, -0.1206]}, \"token.table\": {\"Topic\": [1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 7, 1, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 6, 1, 2, 4, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 6, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 2, 3, 4, 5, 7, 1, 2, 4, 5, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 5, 8, 1, 2, 4, 5, 8, 1, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 4, 5, 1, 2, 3, 4, 5, 6, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 7, 8, 3, 4, 6, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 1, 3, 1, 2, 3, 4, 6, 1, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 10, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 6, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 4, 5, 7, 3, 1, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 5, 1, 5, 1, 2, 5, 8, 1, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 8, 1, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 8, 1, 1, 2, 3, 4, 5, 6, 7, 10, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 1, 2, 3, 5, 6, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 4, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 4, 1, 2, 3, 4, 5, 6, 1, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 7, 1, 2, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 8, 1, 5, 1, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 5, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 7, 1, 2, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 6, 8, 5, 4, 7, 1, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 6, 1, 2, 4, 5, 7, 8, 1, 2, 3, 4, 5, 6, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 2, 4, 5, 7, 8, 1, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 3, 4, 5, 6, 8, 1, 3, 4, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 1, 3, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 3, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 10, 1, 2, 3, 4, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 3, 4, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 4, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 3, 4, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 5, 7, 1, 8, 1, 2, 3, 4, 6, 8, 2, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 6, 7, 8, 1, 5, 1, 2, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 9, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 4, 5, 7, 1, 2, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 4, 5, 7, 2, 4, 5, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 6, 7, 8, 1, 2, 5, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 5, 7, 1, 2, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 9, 10, 2, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 2, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 4, 8, 1, 2, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 4, 5, 1, 2, 3, 1, 2, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 2, 1, 4, 6, 7, 8, 1, 2, 3, 4, 5, 6, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"Freq\": [0.15001222489914878, 0.07500611244957439, 0.30002444979829757, 0.07500611244957439, 0.15001222489914878, 0.07500611244957439, 0.07500611244957439, 0.07500611244957439, 0.16142197402357422, 0.08071098701178711, 0.32284394804714844, 0.08071098701178711, 0.08071098701178711, 0.08071098701178711, 0.08071098701178711, 0.08071098701178711, 0.19836750632321262, 0.09918375316160631, 0.12563275400470134, 0.17853075569089136, 0.09918375316160631, 0.05951025189696379, 0.0727347523185113, 0.0859592527400588, 0.03967350126464252, 0.03967350126464252, 0.16129543653388542, 0.1043676354042788, 0.15180746967895098, 0.1802713702437543, 0.09487966854934438, 0.056927801129606624, 0.08539170169440993, 0.09487966854934438, 0.037951867419737745, 0.037951867419737745, 0.3495150248733983, 0.17475751243669915, 0.17475751243669915, 0.17475751243669915, 0.3701551731049328, 0.0925387932762332, 0.0925387932762332, 0.0925387932762332, 0.0925387932762332, 0.0925387932762332, 0.0925387932762332, 0.3372962712258843, 0.2510309844784502, 0.1673539896523001, 0.08367699482615006, 0.2510309844784502, 0.08367699482615006, 0.08367699482615006, 0.17097151827965576, 0.12102478260245296, 0.1287088957835611, 0.12390632504536851, 0.12582735334064554, 0.10373552794495969, 0.0864462732874664, 0.07972267425399679, 0.027854910281516952, 0.03169696687207101, 0.16905985720934802, 0.13585167097179754, 0.17811663527413454, 0.11170026279903353, 0.10566241075584253, 0.09358670666946052, 0.08151100258307852, 0.05735959441031451, 0.027170334194359505, 0.03924603828074151, 0.1288288103206305, 0.13217501318610142, 0.17902185330269435, 0.13050191175336598, 0.12213640458968866, 0.13886741891704327, 0.05855855014574114, 0.05521234728027022, 0.030115825789238302, 0.023423420058296455, 0.2010306978638149, 0.2010306978638149, 0.2010306978638149, 0.19389933685756458, 0.19389933685756458, 0.19389933685756458, 0.19389933685756458, 0.19389933685756458, 0.14962923780920778, 0.13571023894323497, 0.15310898752570098, 0.12527098979375537, 0.11831149036076895, 0.12179124007726215, 0.09047349262882332, 0.04175699659791845, 0.034797497164932044, 0.027837997731945636, 0.29419348035444354, 0.14249175672768352, 0.14249175672768352, 0.14249175672768352, 0.14249175672768352, 0.14249175672768352, 0.14249175672768352, 0.07124587836384176, 0.07124587836384176, 0.07124587836384176, 0.16146299210659268, 0.13736403806083258, 0.13495414265625658, 0.1421838288699846, 0.13254424725168057, 0.12290466563337653, 0.06506717592355228, 0.048197908091520206, 0.03132864025948813, 0.021689058641184093, 0.1613592411564583, 0.20169905144557287, 0.1378276851544748, 0.1378276851544748, 0.08067962057822915, 0.14455098686932724, 0.047063112003967005, 0.047063112003967005, 0.026893206859409716, 0.023531556001983502, 0.25553451941026323, 0.09786735848691729, 0.09786735848691729, 0.09786735848691729, 0.19573471697383457, 0.09786735848691729, 0.09786735848691729, 0.09786735848691729, 0.19573471697383457, 0.17812917752189308, 0.03238712318579875, 0.2914841086721887, 0.16193561592899372, 0.129548492743195, 0.03238712318579875, 0.0647742463715975, 0.04858068477869812, 0.04858068477869812, 0.016193561592899374, 0.3353976482469103, 0.33604401246621995, 0.13441760498648797, 0.06720880249324399, 0.13441760498648797, 0.13441760498648797, 0.06720880249324399, 0.06720880249324399, 0.06720880249324399, 0.1054396427462114, 0.16870342839393823, 0.1581594641193171, 0.09489567847159026, 0.09489567847159026, 0.18979135694318053, 0.08435171419696912, 0.0527198213731057, 0.03163189282386342, 0.02108792854924228, 0.15212328125047925, 0.15212328125047925, 0.07606164062523962, 0.15212328125047925, 0.07606164062523962, 0.07606164062523962, 0.07606164062523962, 0.15212328125047925, 0.14693318243981457, 0.1337749869974431, 0.15131924758727172, 0.1359680195711717, 0.1535122801610003, 0.09210736809660018, 0.07456310750677157, 0.05263278176948582, 0.037281553753385786, 0.017544260589828605, 0.15899869929728852, 0.09937418706080534, 0.09937418706080534, 0.13912386188512746, 0.09937418706080534, 0.21862321153377173, 0.07949934964864426, 0.0596245122364832, 0.03974967482432213, 0.019874837412161065, 0.0840032128000022, 0.3360128512000088, 0.0840032128000022, 0.1260048192000033, 0.0840032128000022, 0.0840032128000022, 0.0420016064000011, 0.0840032128000022, 0.0420016064000011, 0.19274121267634592, 0.19274121267634592, 0.19274121267634592, 0.19274121267634592, 0.19274121267634592, 0.21051359021814467, 0.21051359021814467, 0.21051359021814467, 0.21051359021814467, 0.38382585830842586, 0.18528765810697354, 0.09264382905348677, 0.09264382905348677, 0.09264382905348677, 0.2779314871604603, 0.09264382905348677, 0.09264382905348677, 0.09264382905348677, 0.16516966868726718, 0.16516966868726718, 0.33033933737453436, 0.16516966868726718, 0.12613463734824223, 0.12613463734824223, 0.12613463734824223, 0.37840391204472673, 0.12613463734824223, 0.10492716688293265, 0.10492716688293265, 0.10492716688293265, 0.10492716688293265, 0.10492716688293265, 0.2098543337658653, 0.10492716688293265, 0.10492716688293265, 0.1465158884902855, 0.12209657374190457, 0.1465158884902855, 0.13430623111609502, 0.10378208768061888, 0.0854676016193332, 0.10988691636771411, 0.0427338008096666, 0.06104828687095228, 0.0427338008096666, 0.22634410857050374, 0.11317205428525187, 0.11317205428525187, 0.33951616285575564, 0.11317205428525187, 0.11317205428525187, 0.16291263174167997, 0.11403884221917598, 0.16291263174167997, 0.11403884221917598, 0.08145631587083998, 0.22807768443835197, 0.04887378952250399, 0.032582526348336, 0.032582526348336, 0.1545280138911855, 0.11589601041838914, 0.1738440156275837, 0.13521201215478731, 0.05794800520919457, 0.21247601910038008, 0.03863200347279638, 0.03863200347279638, 0.03863200347279638, 0.01931600173639819, 0.08351744247562723, 0.3758284911403225, 0.12527616371344083, 0.08351744247562723, 0.041758721237813615, 0.12527616371344083, 0.041758721237813615, 0.041758721237813615, 0.041758721237813615, 0.041758721237813615, 0.23544706855561023, 0.11772353427780512, 0.11772353427780512, 0.15696471237040682, 0.07848235618520341, 0.15696471237040682, 0.07848235618520341, 0.039241178092601706, 0.07848235618520341, 0.19630561129107243, 0.19630561129107243, 0.19630561129107243, 0.19630561129107243, 0.19630561129107243, 0.1524377653408333, 0.1524377653408333, 0.13165079733981058, 0.09700585067143937, 0.1351152920066477, 0.10047034533827649, 0.07621888267041665, 0.09007686133776513, 0.0346449466683712, 0.031180452001534083, 0.20748934954543144, 0.13832623303028763, 0.06916311651514381, 0.27665246606057525, 0.13832623303028763, 0.06916311651514381, 0.06916311651514381, 0.06916311651514381, 0.16978193972779138, 0.16978193972779138, 0.33956387945558275, 0.16978193972779138, 0.10171114420488156, 0.20342228840976312, 0.10171114420488156, 0.10171114420488156, 0.10171114420488156, 0.10171114420488156, 0.10171114420488156, 0.10171114420488156, 0.08868401851727416, 0.19510484073800316, 0.1596312333310935, 0.12415762592418382, 0.14189442962763865, 0.07094721481381933, 0.14189442962763865, 0.01773680370345483, 0.03547360740690966, 0.03547360740690966, 0.17122250538748732, 0.08561125269374366, 0.34244501077497463, 0.17122250538748732, 0.08561125269374366, 0.08561125269374366, 0.08561125269374366, 0.34901340113228224, 0.29511179958297457, 0.29511179958297457, 0.37549533525655643, 0.10304489517990764, 0.15456734276986145, 0.15456734276986145, 0.2576122379497691, 0.10304489517990764, 0.10304489517990764, 0.05152244758995382, 0.05152244758995382, 0.05152244758995382, 0.22937531265010835, 0.137625187590065, 0.09175012506004335, 0.27525037518013, 0.09175012506004335, 0.045875062530021674, 0.09175012506004335, 0.045875062530021674, 0.1619312154821497, 0.1772490331628936, 0.12473080111462882, 0.11160124310256263, 0.11597776243991803, 0.11597776243991803, 0.07221256906636406, 0.07002430939768636, 0.026259116024132383, 0.024070856355454685, 0.16394766310242984, 0.16394766310242984, 0.16394766310242984, 0.16394766310242984, 0.16394766310242984, 0.16394766310242984, 0.20649132566850578, 0.41298265133701156, 0.17344188936251398, 0.34688377872502796, 0.17344188936251398, 0.17344188936251398, 0.17344188936251398, 0.2633588659333449, 0.2633588659333449, 0.28587908308789606, 0.11771491656560425, 0.11771491656560425, 0.11771491656560425, 0.11771491656560425, 0.0840820832611459, 0.0840820832611459, 0.033632833304458355, 0.033632833304458355, 0.016816416652229178, 0.3912102655288602, 0.1956051327644301, 0.1956051327644301, 0.10996431414546502, 0.21992862829093004, 0.10996431414546502, 0.10996431414546502, 0.10996431414546502, 0.21992862829093004, 0.10996431414546502, 0.10996431414546502, 0.08892364304769888, 0.20748850044463074, 0.08892364304769888, 0.23712971479386372, 0.08892364304769888, 0.11856485739693186, 0.05928242869846593, 0.05928242869846593, 0.029641214349232965, 0.029641214349232965, 0.14258511484012232, 0.14258511484012232, 0.14258511484012232, 0.12581039544716677, 0.13000407529540564, 0.10064831635773341, 0.08806727681301674, 0.05871151787534449, 0.029355758937672245, 0.03774311863415003, 0.21919773031951398, 0.10315187309153598, 0.15042981492515664, 0.12464184665227265, 0.1332378360765673, 0.11174786251583065, 0.06446992068221, 0.05157593654576799, 0.021489973560736664, 0.021489973560736664, 0.5132822880852663, 0.11332210723924475, 0.0944350893660373, 0.11332210723924475, 0.15109614298565968, 0.20775719660528205, 0.07554807149282984, 0.14165263404905595, 0.04721754468301865, 0.02833052680981119, 0.01888701787320746, 0.11305882616183972, 0.17335686678148757, 0.23365490740113543, 0.14320784647166365, 0.10552157108438374, 0.09798431600692777, 0.045223530464735893, 0.03768627538727991, 0.03768627538727991, 0.015074510154911963, 0.175395931753214, 0.087697965876607, 0.175395931753214, 0.175395931753214, 0.087697965876607, 0.087697965876607, 0.087697965876607, 0.087697965876607, 0.16871372075732458, 0.16871372075732458, 0.16871372075732458, 0.16871372075732458, 0.16871372075732458, 0.16871372075732458, 0.16871372075732458, 0.0914962695178745, 0.0914962695178745, 0.0914962695178745, 0.182992539035749, 0.0914962695178745, 0.0914962695178745, 0.0914962695178745, 0.182992539035749, 0.0914962695178745, 0.12582573731565927, 0.18873860597348888, 0.15728217164457406, 0.12582573731565927, 0.06291286865782963, 0.09436930298674444, 0.06291286865782963, 0.12582573731565927, 0.03145643432891482, 0.03145643432891482, 0.12212270984934266, 0.15804115392267873, 0.08620426577600658, 0.11493902103467545, 0.12212270984934266, 0.1436737762933443, 0.0933879545906738, 0.07902057696133936, 0.035918444073336074, 0.04310213288800329, 0.24524811739313804, 0.20437343116094836, 0.04087468623218968, 0.24524811739313804, 0.12262405869656902, 0.04087468623218968, 0.08174937246437936, 0.04087468623218968, 0.28739892753466845, 0.14369946376733422, 0.28739892753466845, 0.14369946376733422, 0.19556885606938432, 0.15210911027618781, 0.17383898317278607, 0.08691949158639303, 0.06518961868979477, 0.06518961868979477, 0.06518961868979477, 0.10864936448299128, 0.043459745793196516, 0.043459745793196516, 0.26771636668815996, 0.19122597620582857, 0.05736779286174857, 0.13385818334407998, 0.05736779286174857, 0.05736779286174857, 0.09561298810291428, 0.05736779286174857, 0.038245195241165715, 0.019122597620582858, 0.1543115907755494, 0.2314673861633241, 0.1543115907755494, 0.0771557953877747, 0.1543115907755494, 0.1543115907755494, 0.0771557953877747, 0.0771557953877747, 0.13890688731713383, 0.21299056055293852, 0.13890688731713383, 0.14353711689437162, 0.08797436196751808, 0.09723482112199368, 0.05093252534961573, 0.06482321408132911, 0.02315114788618897, 0.03704183661790235, 0.2907172816171569, 0.2907172816171569, 0.28162448043645055, 0.1412035876923577, 0.07530858010259077, 0.24475288533342, 0.15061716020518154, 0.12237644266671, 0.07530858010259077, 0.06589500758976692, 0.047067862564119226, 0.047067862564119226, 0.018827145025647692, 0.17529379722689006, 0.08764689861344503, 0.058431265742296686, 0.058431265742296686, 0.3505875944537801, 0.058431265742296686, 0.058431265742296686, 0.08764689861344503, 0.029215632871148343, 0.029215632871148343, 0.14201064288058357, 0.14201064288058357, 0.20287234697226225, 0.0811489387889049, 0.12172340818335735, 0.10143617348613113, 0.0811489387889049, 0.04057446939445245, 0.04057446939445245, 0.060861704091678674, 0.20526208693359413, 0.20526208693359413, 0.20526208693359413, 0.20526208693359413, 0.26512880468756694, 0.37052939852772676, 0.0805708795649569, 0.3222835182598276, 0.12085631934743535, 0.12085631934743535, 0.0805708795649569, 0.12085631934743535, 0.04028543978247845, 0.0805708795649569, 0.0805708795649569, 0.1480820544290002, 0.18629677815261314, 0.10031364977448401, 0.1433052139635486, 0.1480820544290002, 0.09362607312285173, 0.06974187079559364, 0.06496503033014202, 0.02961641088580004, 0.016241257582535504, 0.18339678175908056, 0.12226452117272038, 0.13754758631931044, 0.13754758631931044, 0.06113226058636019, 0.19867984690567062, 0.04584919543977014, 0.04584919543977014, 0.030566130293180095, 0.015283065146590048, 0.2546229011581907, 0.07483535062330866, 0.17773395773035808, 0.15902512007453093, 0.14967070124661733, 0.14967070124661733, 0.07483535062330866, 0.0561265129674815, 0.0561265129674815, 0.04677209413956792, 0.04677209413956792, 0.16860360244681377, 0.14162702605532357, 0.14162702605532357, 0.11465044966383336, 0.10790630556596081, 0.10790630556596081, 0.08092972917447061, 0.06069729688085296, 0.033720720489362754, 0.040464864587235305, 0.2858887996605966, 0.2858887996605966, 0.22539615238497276, 0.45079230476994553, 0.16358973277400865, 0.16358973277400865, 0.3271794655480173, 0.16358973277400865, 0.14700986180211814, 0.14700986180211814, 0.14700986180211814, 0.14700986180211814, 0.2940197236042363, 0.14700986180211814, 0.12251526453339251, 0.11434758023116634, 0.24503052906678502, 0.13068294883561868, 0.13885063313784485, 0.11434758023116634, 0.049006105813357, 0.04083842151113084, 0.0245030529066785, 0.0245030529066785, 0.3361665995281528, 0.1290292518108005, 0.1290292518108005, 0.1876789117248007, 0.11729931982800045, 0.09383945586240035, 0.1055693878452004, 0.1055693878452004, 0.03518979594840013, 0.023459863965600087, 0.05864965991400022, 0.14814300383302115, 0.14814300383302115, 0.2962860076660423, 0.14814300383302115, 0.14690879384384561, 0.11752703507507649, 0.14690879384384561, 0.23505407015015298, 0.08814527630630736, 0.11752703507507649, 0.058763517537538244, 0.058763517537538244, 0.058763517537538244, 0.029381758768769122, 0.1308690216358711, 0.09815176622690333, 0.11451039393138723, 0.147227649340355, 0.22902078786277447, 0.11451039393138723, 0.06543451081793555, 0.06543451081793555, 0.049075883113451665, 0.01635862770448389, 0.32944841953666343, 0.16472420976833171, 0.16472420976833171, 0.16472420976833171, 0.15315623107817397, 0.12801117821459318, 0.15315623107817397, 0.1828731117351331, 0.11886752262783652, 0.09143655586756655, 0.0754351585907424, 0.043432364037094114, 0.032002794553648295, 0.022859138966891637, 0.14284968152678426, 0.14284968152678426, 0.2856993630535685, 0.14284968152678426, 0.09537137389946017, 0.09537137389946017, 0.19074274779892034, 0.09537137389946017, 0.19074274779892034, 0.09537137389946017, 0.19074274779892034, 0.09537137389946017, 0.09537137389946017, 0.09537137389946017, 0.1599684460805921, 0.14647713135090362, 0.12142183256719642, 0.1387678086482245, 0.12142183256719642, 0.1117851791888475, 0.0751658963511216, 0.061674581621433106, 0.04240127486473526, 0.023127968108037415, 0.16296066392175404, 0.13968056907578918, 0.11640047422982432, 0.13968056907578918, 0.09312037938385946, 0.11640047422982432, 0.06984028453789459, 0.06984028453789459, 0.04656018969192973, 0.04656018969192973, 0.14233330009181788, 0.17327532185090874, 0.10520287398090887, 0.0990144696290907, 0.14852170444363605, 0.14233330009181788, 0.10520287398090887, 0.043318830462727186, 0.030942021759090846, 0.018565213055454506, 0.15279171518394943, 0.11459378638796208, 0.11459378638796208, 0.11459378638796208, 0.07639585759197472, 0.22918757277592416, 0.03819792879598736, 0.07639585759197472, 0.07639585759197472, 0.03819792879598736, 0.09286946661793323, 0.1276955165996582, 0.08126078329069157, 0.2147606415539706, 0.2147606415539706, 0.11608683327241653, 0.05223907497258744, 0.05804341663620827, 0.023217366654483308, 0.017413024990862478, 0.29193698288382586, 0.29193698288382586, 0.25899511592399455, 0.25899511592399455, 0.11839648267669185, 0.11839648267669185, 0.11839648267669185, 0.11839648267669185, 0.11839648267669185, 0.2367929653533837, 0.11839648267669185, 0.11839648267669185, 0.11839648267669185, 0.17868870742178694, 0.13103838544264376, 0.13103838544264376, 0.23825160989571592, 0.10721322445307216, 0.08338806346350057, 0.05956290247392898, 0.03573774148435739, 0.023825160989571593, 0.011912580494785796, 0.23429608874241603, 0.11714804437120802, 0.11714804437120802, 0.11714804437120802, 0.11714804437120802, 0.11714804437120802, 0.11714804437120802, 0.11714804437120802, 0.14362952266810483, 0.230677718224532, 0.12186747377899805, 0.11751506400117669, 0.1044578346677126, 0.11751506400117669, 0.060933736889499025, 0.0522289173338563, 0.030466868444749513, 0.02611445866692815, 0.11514761891412004, 0.11514761891412004, 0.11514761891412004, 0.23029523782824007, 0.11514761891412004, 0.11514761891412004, 0.11514761891412004, 0.11435907795063119, 0.34307723385189354, 0.11435907795063119, 0.11435907795063119, 0.11435907795063119, 0.11435907795063119, 0.19049480965286167, 0.19049480965286167, 0.19049480965286167, 0.19049480965286167, 0.19049480965286167, 0.17261404641680012, 0.15535264177512012, 0.1380912371334401, 0.1467219394542801, 0.08630702320840006, 0.08630702320840006, 0.06904561856672005, 0.060414916245880045, 0.05178421392504003, 0.017261404641680012, 0.1766069182050125, 0.1766069182050125, 0.353213836410025, 0.1766069182050125, 0.1766069182050125, 0.12549608923723102, 0.08366405949148735, 0.3346562379659494, 0.08366405949148735, 0.12549608923723102, 0.041832029745743675, 0.08366405949148735, 0.08366405949148735, 0.041832029745743675, 0.10531439934640495, 0.2808383982570799, 0.12286679923747244, 0.12286679923747244, 0.08776199945533747, 0.08776199945533747, 0.08776199945533747, 0.03510479978213499, 0.03510479978213499, 0.03510479978213499, 0.17202765296313408, 0.057342550987711356, 0.1433563774692784, 0.11468510197542271, 0.08601382648156704, 0.08601382648156704, 0.1433563774692784, 0.11468510197542271, 0.028671275493855678, 0.057342550987711356, 0.4158384180368414, 0.1386128060122805, 0.1386128060122805, 0.1386128060122805, 0.1386128060122805, 0.1386128060122805, 0.10926392333534651, 0.1443844701217079, 0.13853104565731433, 0.14633561160983907, 0.1912118658368564, 0.06633881059646038, 0.08389908398964108, 0.06243652762019801, 0.03512054678636138, 0.021462556369443067, 0.1602304072587251, 0.14281405864364627, 0.1219144403055517, 0.15326386781269355, 0.08708174307539407, 0.17416348615078814, 0.0557323155682522, 0.05224904584523644, 0.03831596695317339, 0.017416348615078812, 0.13508713942782696, 0.14451182357395445, 0.16964431463029434, 0.14765338495599692, 0.08796371869718966, 0.15079494633803942, 0.05968966625880727, 0.047123420730637314, 0.02827405243838239, 0.0251324910563399, 0.1366435255014057, 0.1366435255014057, 0.1366435255014057, 0.1366435255014057, 0.2732870510028114, 0.1366435255014057, 0.33335909624977755, 0.12410655875947682, 0.12410655875947682, 0.12410655875947682, 0.12410655875947682, 0.12410655875947682, 0.12410655875947682, 0.12410655875947682, 0.12410655875947682, 0.2351600619443207, 0.2351600619443207, 0.2351600619443207, 0.2351600619443207, 0.2351600619443207, 0.14976857003465507, 0.14976857003465507, 0.07488428501732754, 0.14976857003465507, 0.14976857003465507, 0.14976857003465507, 0.07488428501732754, 0.07488428501732754, 0.1606857260499737, 0.04017143151249342, 0.28120002058745397, 0.2008571575624671, 0.12051429453748028, 0.04017143151249342, 0.04017143151249342, 0.04017143151249342, 0.04017143151249342, 0.10309529265145688, 0.1546429389771853, 0.10309529265145688, 0.05154764632572844, 0.10309529265145688, 0.20619058530291376, 0.10309529265145688, 0.05154764632572844, 0.05154764632572844, 0.05154764632572844, 0.16554265942705898, 0.18209692536976488, 0.13243412754164718, 0.13243412754164718, 0.08277132971352949, 0.08277132971352949, 0.13243412754164718, 0.049662797828117694, 0.016554265942705898, 0.033108531885411796, 0.19385107704152135, 0.1076950428008452, 0.25846810272202847, 0.1076950428008452, 0.08615603424067615, 0.06461702568050712, 0.06461702568050712, 0.043078017120338076, 0.021539008560169038, 0.021539008560169038, 0.05428281609005937, 0.05428281609005937, 0.10856563218011873, 0.1628484482701781, 0.2714140804502968, 0.05428281609005937, 0.1628484482701781, 0.05428281609005937, 0.05428281609005937, 0.23265385860358104, 0.23265385860358104, 0.23265385860358104, 0.13831364378563438, 0.1608298183553888, 0.15118002925406546, 0.12223066195009549, 0.12223066195009549, 0.09006469827901772, 0.08684810191190995, 0.05468213824083219, 0.03216596367107776, 0.03859915640529331, 0.1343143046735762, 0.11192858722798017, 0.1925171700321259, 0.12983716118445698, 0.11640573071709938, 0.08954286978238414, 0.09849715676062255, 0.05820286535854969, 0.031340004423834444, 0.03581714791295366, 0.1444023777822266, 0.1444023777822266, 0.1444023777822266, 0.2888047555644532, 0.1444023777822266, 0.1444023777822266, 0.1294908355972008, 0.1294908355972008, 0.2589816711944016, 0.1294908355972008, 0.1294908355972008, 0.07758926199654391, 0.23923355782267705, 0.09052080566263455, 0.16811006765917846, 0.11638389299481586, 0.10991812116177054, 0.09698657749567989, 0.06465771833045325, 0.025863087332181303, 0.012931543666090652, 0.16930917511220492, 0.1280489559672138, 0.13231725449945428, 0.16646364275737796, 0.09959363241894408, 0.1266261897898003, 0.08109767211256874, 0.04126021914499112, 0.03699192061275065, 0.019918726483788816, 0.16921506260536487, 0.10413234621868607, 0.1301654327733576, 0.10413234621868607, 0.10413234621868607, 0.20826469243737214, 0.07809925966401456, 0.052066173109343035, 0.052066173109343035, 0.026033086554671517, 0.13349849088482896, 0.13349849088482896, 0.16316482219256873, 0.11866532523095907, 0.0889989939232193, 0.16316482219256873, 0.05933266261547954, 0.07416582826934942, 0.05933266261547954, 0.014833165653869884, 0.12792888054697885, 0.12792888054697885, 0.12792888054697885, 0.12792888054697885, 0.2558577610939577, 0.12792888054697885, 0.1666620089751547, 0.19678164915138746, 0.1465822488576662, 0.09839082457569373, 0.12047856070493111, 0.10039880058744259, 0.0622472563642144, 0.0582313043407167, 0.028111664164483924, 0.02409571214098622, 0.37531213578821354, 0.1404837297031756, 0.19850961805883507, 0.12215976495928313, 0.13742973557919352, 0.12215976495928313, 0.10688979433937273, 0.05191790010769533, 0.06413387660362364, 0.030539941239820782, 0.027485947115838703, 0.21715529406773207, 0.21715529406773207, 0.0930665546004566, 0.0930665546004566, 0.15511092433409435, 0.062044369733637736, 0.062044369733637736, 0.031022184866818868, 0.062044369733637736, 0.031022184866818868, 0.17021213572973087, 0.08510606786486544, 0.34042427145946175, 0.08510606786486544, 0.05673737857657696, 0.05673737857657696, 0.05673737857657696, 0.05673737857657696, 0.02836868928828848, 0.02836868928828848, 0.17057767501850238, 0.17057767501850238, 0.34115535003700476, 0.42053417757406414, 0.14017805919135473, 0.14017805919135473, 0.14017805919135473, 0.14017805919135473, 0.3468421899676532, 0.14700236274724443, 0.18066702597180423, 0.16495684980034298, 0.13690296377987649, 0.11109338864104731, 0.08977243526549278, 0.06284070468584495, 0.05386346115929567, 0.025809575138829175, 0.028053886020466493, 0.22946067155645783, 0.22946067155645783, 0.13819790446013938, 0.0834402442023483, 0.07822522893970153, 0.08083273657102491, 0.067795198414408, 0.04172012210117415, 0.02868258394455723, 0.020860061050587075, 0.15863135908237372, 0.13996884624915326, 0.12130633341593283, 0.13996884624915326, 0.13996884624915326, 0.08398130774949196, 0.12130633341593283, 0.027993769249830654, 0.027993769249830654, 0.027993769249830654, 0.29478720872046243, 0.17541430530981225, 0.15149417276756513, 0.13156072898235918, 0.11561397395419444, 0.08372046389786494, 0.12757404022531801, 0.09966721892602969, 0.05581364259857663, 0.035880198813370685, 0.023920132542247126, 0.13582138231625077, 0.1560110472551529, 0.16702359176728135, 0.1468339268283792, 0.10461917286522018, 0.10461917286522018, 0.07525238749954434, 0.04955645030457798, 0.027531361280321104, 0.031202209451030583, 0.0764344569746121, 0.12229513115937936, 0.1528689139492242, 0.290450936503526, 0.1375820225543018, 0.0764344569746121, 0.03057378278984484, 0.04586067418476726, 0.06114756557968968, 0.01528689139492242, 0.28268908713803576, 0.28268908713803576, 0.12326130107419912, 0.24652260214839825, 0.18489195161129868, 0.10785363843992422, 0.07703831317137445, 0.07703831317137445, 0.07703831317137445, 0.03081532526854978, 0.03081532526854978, 0.03081532526854978, 0.1725288109905166, 0.1725288109905166, 0.3450576219810332, 0.2824822293346602, 0.07704060800036187, 0.07704060800036187, 0.15408121600072375, 0.05136040533357458, 0.20544162133429833, 0.07704060800036187, 0.05136040533357458, 0.05136040533357458, 0.14139537778824168, 0.09950045103617007, 0.1623428411642775, 0.19900090207234014, 0.09950045103617007, 0.12044791441220587, 0.07331612181612532, 0.04189492675207161, 0.04713179259608056, 0.020947463376035806, 0.13510537992054036, 0.29723183582518875, 0.10808430393643229, 0.10808430393643229, 0.08106322795232421, 0.08106322795232421, 0.08106322795232421, 0.02702107598410807, 0.05404215196821614, 0.02702107598410807, 0.12295506685593935, 0.030738766713984837, 0.1536938335699242, 0.12295506685593935, 0.09221630014195452, 0.2459101337118787, 0.030738766713984837, 0.09221630014195452, 0.061477533427969674, 0.030738766713984837, 0.15860740593255263, 0.07930370296627631, 0.07930370296627631, 0.31721481186510525, 0.07930370296627631, 0.07930370296627631, 0.07930370296627631, 0.07930370296627631, 0.07930370296627631, 0.16174803108157815, 0.14648878286633493, 0.137333233937189, 0.1647998807246268, 0.12817768500804308, 0.0823999403623134, 0.07019254179011883, 0.04272589500268102, 0.04272589500268102, 0.024414797144389155, 0.04607557757388278, 0.11518894393470695, 0.2764534654432967, 0.18430231029553112, 0.09215115514776556, 0.09215115514776556, 0.04607557757388278, 0.04607557757388278, 0.02303778878694139, 0.04607557757388278, 0.2258043391946576, 0.2258043391946576, 0.14113490923197422, 0.07056745461598711, 0.28226981846394844, 0.07056745461598711, 0.14113490923197422, 0.07056745461598711, 0.07056745461598711, 0.07056745461598711, 0.20229088723256008, 0.16183270978604805, 0.04045817744651201, 0.2832072421255841, 0.12137453233953605, 0.04045817744651201, 0.08091635489302403, 0.04045817744651201, 0.1432855812795574, 0.1944590031651136, 0.11940465106629783, 0.11940465106629783, 0.11940465106629783, 0.1398740198205203, 0.0716427906397787, 0.04435029896748205, 0.023880930213259564, 0.023880930213259564, 0.12787101875804732, 0.1370046629550507, 0.2210341895674818, 0.12604428991864666, 0.10595027268523921, 0.11691064572164327, 0.05845532286082163, 0.04201476330621555, 0.03653457678801352, 0.025574203751609466, 0.11586549414976999, 0.13342087205125028, 0.19662023249657937, 0.1404430232118424, 0.0877768895074015, 0.12639872089065818, 0.08075473834680938, 0.04915505812414484, 0.038621831383256665, 0.031599680222664545, 0.24249671047975904, 0.1454980262878554, 0.0727490131439277, 0.26674638152773494, 0.0969986841919036, 0.0242496710479759, 0.0727490131439277, 0.0242496710479759, 0.0242496710479759, 0.2268152714824518, 0.12960872656140104, 0.06480436328070052, 0.29161963476315234, 0.09720654492105078, 0.03240218164035026, 0.09720654492105078, 0.03240218164035026, 0.03240218164035026, 0.344140363494221, 0.344140363494221, 0.16780867569656513, 0.16780867569656513, 0.16780867569656513, 0.16780867569656513, 0.16780867569656513, 0.16780867569656513, 0.29954555002734107, 0.29954555002734107, 0.1267507944062688, 0.105625662005224, 0.1690010592083584, 0.105625662005224, 0.1267507944062688, 0.0845005296041792, 0.1478759268073136, 0.0633753972031344, 0.0422502648020896, 0.0422502648020896, 0.14686046080002826, 0.1803223379443385, 0.1840403242937063, 0.13570650175192484, 0.0966676450835629, 0.08737267921014338, 0.050192815716465354, 0.050192815716465354, 0.037179863493678036, 0.031602883969626336, 0.26000525435055283, 0.26000525435055283, 0.26000525435055283, 0.16513856970399834, 0.18299138805037654, 0.11827492154475558, 0.1071169100782692, 0.11827492154475558, 0.089264091731891, 0.07364287567881007, 0.06025326191902643, 0.0535584550391346, 0.03124243210616185, 0.28972807977937437, 0.21952286787011152, 0.21952286787011152, 0.21952286787011152, 0.08610841613023063, 0.08610841613023063, 0.08610841613023063, 0.17221683226046125, 0.2583252483906919, 0.08610841613023063, 0.17221683226046125, 0.14603478411575085, 0.14603478411575085, 0.14603478411575085, 0.14603478411575085, 0.14603478411575085, 0.14603478411575085, 0.14603478411575085, 0.2819115174652312, 0.2819115174652312, 0.3331136919021064, 0.11236515806148899, 0.3090041846690947, 0.11236515806148899, 0.11236515806148899, 0.08427386854611674, 0.11236515806148899, 0.028091289515372247, 0.08427386854611674, 0.056182579030744494, 0.18793382089638758, 0.16191221492611851, 0.12143416119458889, 0.1272167402990931, 0.12143416119458889, 0.1272167402990931, 0.04047805373152963, 0.06071708059729444, 0.02313031641801693, 0.02313031641801693, 0.1043421656762163, 0.19129397040639656, 0.14781806804130646, 0.2086843313524326, 0.06956144378414421, 0.11303734614923434, 0.05217108283810815, 0.05217108283810815, 0.026085541419054076, 0.026085541419054076, 0.20900139671393186, 0.10450069835696593, 0.10450069835696593, 0.10450069835696593, 0.31350209507089777, 0.10450069835696593, 0.10450069835696593, 0.058066188836902316, 0.11613237767380463, 0.11613237767380463, 0.11613237767380463, 0.11613237767380463, 0.058066188836902316, 0.23226475534760926, 0.11613237767380463, 0.058066188836902316, 0.07979311658875098, 0.15958623317750195, 0.15958623317750195, 0.15958623317750195, 0.07979311658875098, 0.07979311658875098, 0.07979311658875098, 0.07979311658875098, 0.07979311658875098, 0.07979311658875098, 0.09870712176008153, 0.09870712176008153, 0.09870712176008153, 0.09870712176008153, 0.09870712176008153, 0.09870712176008153, 0.09870712176008153, 0.19741424352016307, 0.3808581032268928, 0.170276540242912, 0.340553080485824, 0.170276540242912, 0.170276540242912, 0.170276540242912, 0.1460478912009286, 0.2920957824018572, 0.1460478912009286, 0.1460478912009286, 0.1460478912009286, 0.1460478912009286, 0.35775920318293597, 0.17887960159146798, 0.17887960159146798, 0.07988544659786863, 0.15977089319573726, 0.07988544659786863, 0.07988544659786863, 0.15977089319573726, 0.07988544659786863, 0.15977089319573726, 0.07988544659786863, 0.10920226421271137, 0.1577366038628053, 0.1698701887753288, 0.13346943403775835, 0.12133584912523485, 0.10920226421271137, 0.06066792456261742, 0.06066792456261742, 0.06066792456261742, 0.02426716982504697, 0.1747945506527387, 0.13692239801131198, 0.18062103567449664, 0.08739727532636934, 0.19227400571801256, 0.09613700285900628, 0.055351607706700584, 0.03204566761966876, 0.023305940087031823, 0.020392697576152845, 0.1462511112898489, 0.10391526328489264, 0.12700754401486877, 0.17319210547482106, 0.13470497092486083, 0.09236912291990457, 0.10776397673988866, 0.06157941527993638, 0.03078970763996819, 0.026940994184972165, 0.12222199081286565, 0.11503246194152061, 0.13660104855555574, 0.15816963516959084, 0.10784293307017558, 0.07189528871345038, 0.13660104855555574, 0.07189528871345038, 0.04313717322807023, 0.028758115485380153, 0.1783968441082503, 0.1584827777891898, 0.14852574462965953, 0.13359019489036417, 0.12114390344095136, 0.0854645346193013, 0.07467774869647686, 0.04480664921788612, 0.03319011053176749, 0.02240332460894306, 0.25164680239699666, 0.25164680239699666, 0.2323183780602624, 0.2323183780602624, 0.2323183780602624, 0.2323183780602624, 0.1433635580284913, 0.1433635580284913, 0.2867271160569826, 0.1433635580284913, 0.18819842979787177, 0.17977163443378796, 0.12640193046125714, 0.11235727152111746, 0.1292108622492851, 0.0842679536408381, 0.058987567548586674, 0.06741436291267049, 0.028089317880279366, 0.02528038609225143, 0.10305897540747982, 0.20611795081495965, 0.10305897540747982, 0.10305897540747982, 0.10305897540747982, 0.10305897540747982, 0.05152948770373991, 0.10305897540747982, 0.05152948770373991, 0.1132909250887048, 0.1132909250887048, 0.1132909250887048, 0.2265818501774096, 0.1132909250887048, 0.1132909250887048, 0.1568370769905576, 0.12623472050459514, 0.18935208075689272, 0.14727384058869433, 0.14727384058869433, 0.08606912761676942, 0.05164147657006165, 0.045903534728943686, 0.028689709205589806, 0.022951767364471843, 0.14433751605333162, 0.09622501070222107, 0.09622501070222107, 0.19245002140444215, 0.19245002140444215, 0.048112505351110536, 0.14433751605333162, 0.048112505351110536, 0.048112505351110536, 0.19376203096924777, 0.19376203096924777, 0.19376203096924777, 0.19376203096924777, 0.19376203096924777, 0.19376203096924777, 0.37809829993386596, 0.2847616110336886, 0.2847616110336886, 0.12729307968070683, 0.12729307968070683, 0.12729307968070683, 0.12729307968070683, 0.12729307968070683, 0.12729307968070683, 0.12729307968070683, 0.12729307968070683, 0.10716418509196218, 0.14173327705711128, 0.17284545982574545, 0.17284545982574545, 0.11062109428847709, 0.12790564027105164, 0.06568127473378328, 0.04148291035817891, 0.034569091965149096, 0.024198364375604364, 0.0720805683096364, 0.0720805683096364, 0.1441611366192728, 0.1441611366192728, 0.1441611366192728, 0.0720805683096364, 0.1441611366192728, 0.0720805683096364, 0.0720805683096364, 0.0720805683096364, 0.20795752223714756, 0.1476799795597135, 0.10247182255163793, 0.11151345395325304, 0.11151345395325304, 0.10548569968550964, 0.0753469283467926, 0.0693191740790492, 0.03918040274033215, 0.027124894204845335, 0.14246682672695302, 0.14246682672695302, 0.14246682672695302, 0.14246682672695302, 0.14246682672695302, 0.14246682672695302, 0.14246682672695302, 0.12512441243420044, 0.19185743239910738, 0.12512441243420044, 0.09592871619955369, 0.09592871619955369, 0.1418076674254272, 0.08341627495613364, 0.06673301996490691, 0.037537323730260135, 0.033366509982453454, 0.20846156652475908, 0.16676925321980726, 0.12507693991485544, 0.08338462660990363, 0.041692313304951815, 0.12507693991485544, 0.08338462660990363, 0.12507693991485544, 0.041692313304951815, 0.041692313304951815, 0.14438850968813277, 0.11551080775050622, 0.11551080775050622, 0.259899317438639, 0.1732662116257593, 0.05775540387525311, 0.05775540387525311, 0.028877701937626555, 0.028877701937626555, 0.028877701937626555, 0.17865615936647425, 0.17865615936647425, 0.3573123187329485, 0.17865615936647425, 0.04814976765963194, 0.14444930297889583, 0.24074883829815974, 0.09629953531926388, 0.14444930297889583, 0.09629953531926388, 0.09629953531926388, 0.09629953531926388, 0.04814976765963194, 0.2800706384802011, 0.1153232040800828, 0.18122217784013012, 0.08237371720005914, 0.1153232040800828, 0.08237371720005914, 0.06589897376004732, 0.049424230320035484, 0.01647474344001183, 0.01647474344001183, 0.1294049791330335, 0.0776429874798201, 0.1294049791330335, 0.0776429874798201, 0.1035239833064268, 0.2329289624394603, 0.0776429874798201, 0.0776429874798201, 0.0776429874798201, 0.0258809958266067, 0.24835295184461567, 0.24835295184461567, 0.33465144962580845, 0.10927866573771051, 0.22636295045668606, 0.10147304675644547, 0.14050114166277067, 0.10147304675644547, 0.10147304675644547, 0.07805618981265036, 0.09366742777518045, 0.02341685694379511, 0.02341685694379511, 0.09790527606315745, 0.14685791409473617, 0.048952638031578725, 0.14685791409473617, 0.29371582818947234, 0.09790527606315745, 0.024476319015789363, 0.07342895704736808, 0.024476319015789363, 0.024476319015789363, 0.0991718700163336, 0.10961101422857923, 0.20878288424491281, 0.19312416792654435, 0.08351315369796512, 0.11483058633470206, 0.07307400948571949, 0.052195721061228204, 0.031317432636736926, 0.031317432636736926, 0.1270826611499386, 0.38124798344981586, 0.1270826611499386, 0.1270826611499386, 0.1270826611499386, 0.1270826611499386, 0.08574926023071149, 0.16568501129323915, 0.18603229338188257, 0.14824448378868765, 0.11045667419549277, 0.10318978773526298, 0.07121548731025192, 0.06249522355797617, 0.02906754584091915, 0.037787809593194895, 0.23894127442092564, 0.135518931761122, 0.1462177947948948, 0.11768749337150068, 0.1069886303377279, 0.10342234265980363, 0.067759465880561, 0.0392291644571669, 0.024964013745469842, 0.024964013745469842, 0.5583647608047598, 0.11112390363061173, 0.16946395303668288, 0.161129660264387, 0.12779248917520347, 0.10556770844908114, 0.14446107471979525, 0.06111814699683645, 0.06111814699683645, 0.03611526867994881, 0.022224780726122346, 0.1415405291471917, 0.20657266416076625, 0.13388968973382998, 0.15301678826723425, 0.10711175178706399, 0.0879846532536597, 0.053555875893531994, 0.06885755472025541, 0.026777937946765997, 0.022952518240085137, 0.339656996042424, 0.26828049471442317, 0.1887899777620015, 0.19872629238105421, 0.07949051695242168, 0.06955420233336897, 0.06955420233336897, 0.05961788771431626, 0.01987262923810542, 0.01987262923810542, 0.01987262923810542, 0.15229522841260082, 0.12300768448710066, 0.15815273719770084, 0.10543515813180057, 0.11715017570200063, 0.10543515813180057, 0.07614761420630041, 0.08200512299140043, 0.052717579065900286, 0.02928754392550016, 0.144611776324962, 0.30730002469054424, 0.21691766448744298, 0.09038236020310124, 0.072305888162481, 0.054229416121860746, 0.0361529440812405, 0.01807647204062025, 0.0361529440812405, 0.01807647204062025, 0.11750907837981638, 0.1803627714666949, 0.10657800132122881, 0.15850061734951976, 0.12570738617375707, 0.12844015543840395, 0.07105200088081921, 0.04918984676364407, 0.0382587697050565, 0.027327692646468924, 0.11113841116946911, 0.13282395481229234, 0.14637741958905687, 0.1626415773211743, 0.14366672663370397, 0.13011326185693944, 0.07318870979452843, 0.04337108728564648, 0.029817622508881956, 0.029817622508881956, 0.3430042413595697, 0.17762802026608165, 0.18242877757057036, 0.1488234764391495, 0.1536242337436382, 0.06240984495835302, 0.0768121168718191, 0.06240984495835302, 0.09121438878528518, 0.028804543826932164, 0.02400378652244347, 0.14414166009768817, 0.2227643837873363, 0.03931136184482405, 0.05241514912643207, 0.05241514912643207, 0.23586817106894428, 0.09172651097125611, 0.10483029825286413, 0.03931136184482405, 0.026207574563216034, 0.18682292387157987, 0.18682292387157987, 0.09341146193578993, 0.2802343858073698, 0.09341146193578993, 0.09341146193578993, 0.15932148254448758, 0.16787044014443567, 0.15388123679906604, 0.1150223386174837, 0.07694061839953302, 0.13600614363553817, 0.08238086414495455, 0.038081720217950685, 0.04896221170879374, 0.021760982981686106, 0.171782887115753, 0.1503100262262839, 0.1171246957607407, 0.12493300881145673, 0.1385975566502098, 0.11322053923538267, 0.05856234788037035, 0.05856234788037035, 0.04294572177893825, 0.021472860889469127, 0.26668938052263796, 0.26668938052263796, 0.26668938052263796, 0.11561340317543983, 0.11561340317543983, 0.11561340317543983, 0.3468402095263195, 0.11561340317543983, 0.11561340317543983, 0.3187596321267413, 0.15937981606337065, 0.15937981606337065, 0.15937981606337065, 0.15937981606337065, 0.15937981606337065, 0.15937981606337065, 0.09793398850032349, 0.2938019655009705, 0.06528932566688234, 0.06528932566688234, 0.09793398850032349, 0.19586797700064698, 0.09793398850032349, 0.06528932566688234, 0.03264466283344117, 0.03264466283344117, 0.15200674594791178, 0.05066891531597059, 0.10133783063194118, 0.20267566126388237, 0.10133783063194118, 0.05066891531597059, 0.10133783063194118, 0.10133783063194118, 0.05066891531597059, 0.05066891531597059, 0.1813023596550193, 0.1813023596550193, 0.09065117982750966, 0.271953539482529, 0.09065117982750966, 0.04532558991375483, 0.04532558991375483, 0.04532558991375483, 0.04532558991375483, 0.13167795853073241, 0.13167795853073241, 0.13167795853073241, 0.13167795853073241, 0.13167795853073241, 0.13167795853073241, 0.13167795853073241, 0.13167795853073241, 0.26420084693616, 0.13210042346808, 0.13210042346808, 0.13210042346808, 0.13210042346808, 0.2191978170439839, 0.2191978170439839, 0.2191978170439839, 0.10375960103209142, 0.31127880309627426, 0.10375960103209142, 0.10375960103209142, 0.10375960103209142, 0.10375960103209142, 0.10375960103209142, 0.10375960103209142, 0.37217559637212216, 0.10303532461932435, 0.10303532461932435, 0.4121412984772974, 0.10303532461932435, 0.10303532461932435, 0.10303532461932435, 0.10303532461932435, 0.10303532461932435, 0.14688445401970762, 0.14688445401970762, 0.14688445401970762, 0.14688445401970762, 0.29376890803941524, 0.14688445401970762, 0.17192801458528445, 0.17192801458528445, 0.3438560291705689, 0.17192801458528445, 0.24820538294789932, 0.10408612833299004, 0.10408612833299004, 0.1281060041021416, 0.17614575564044468, 0.0800662525638385, 0.05604637679468694, 0.05604637679468694, 0.024019875769151548, 0.024019875769151548, 0.26675490880513186, 0.10003309080192446, 0.11114787866880495, 0.12226266653568545, 0.16672181800320743, 0.07780351506816346, 0.04445915146752198, 0.07780351506816346, 0.02222957573376099, 0.02222957573376099, 0.1336256431943563, 0.06681282159717815, 0.10690051455548504, 0.12026307887492067, 0.253888722069277, 0.08017538591661379, 0.12026307887492067, 0.05345025727774252, 0.04008769295830689, 0.02672512863887126, 0.19288396618404624, 0.11573037971042775, 0.03857679323680925, 0.34719113913128324, 0.11573037971042775, 0.03857679323680925, 0.0771535864736185, 0.03857679323680925, 0.03857679323680925, 0.10427342532799182, 0.05213671266399591, 0.13034178165998977, 0.13034178165998977, 0.2867519196519775, 0.07820506899599386, 0.10427342532799182, 0.05213671266399591, 0.026068356331997954, 0.026068356331997954, 0.18831470090318445, 0.11926597723868348, 0.1393528786683565, 0.1795266815277025, 0.1217768399173926, 0.08913562509417397, 0.07030415500385552, 0.04268466553805514, 0.03013035214450951, 0.020086901429673006, 0.136474888906305, 0.14665958210826804, 0.14258570482748284, 0.1588812139506237, 0.15073345938905328, 0.09369917745806014, 0.052960404650207904, 0.05499734329060052, 0.0346279568866744, 0.028517140965496564, 0.17241529878842485, 0.1568979218974666, 0.15172546293381386, 0.12413901512766588, 0.15344961592169812, 0.08620764939421242, 0.0534487426244117, 0.0534487426244117, 0.02413814183037948, 0.02241398884249523, 0.20062440634450163, 0.20062440634450163, 0.10031220317225081, 0.10031220317225081, 0.10031220317225081, 0.10031220317225081, 0.10031220317225081, 0.10031220317225081, 0.13868599671833354, 0.16524288970695059, 0.15934135793170237, 0.14458752849358178, 0.10622757195446823, 0.10622757195446823, 0.0649168495277306, 0.05016302008961, 0.03540919065148941, 0.0324584247638653, 0.4311687725920017, 0.0958152827982226, 0.0958152827982226, 0.0958152827982226, 0.0958152827982226, 0.0479076413991113, 0.0479076413991113, 0.0479076413991113, 0.10108113644610887, 0.10108113644610887, 0.1516217046691633, 0.17689198878069054, 0.10108113644610887, 0.20216227289221775, 0.05054056822305444, 0.07581085233458165, 0.02527028411152722, 0.02527028411152722, 0.29977306875175963, 0.29977306875175963, 0.19516231380346627, 0.13940165271676164, 0.0743475481156062, 0.17657542677456473, 0.10222787865895852, 0.05576066108670465, 0.13940165271676164, 0.046467217572253874, 0.027880330543352327, 0.0371737740578031, 0.14042726490799318, 0.1199482887755775, 0.19308748924849062, 0.11702272075666098, 0.14042726490799318, 0.12579942481341055, 0.07313920047291311, 0.029255680189165245, 0.035106816226998294, 0.02047897613241567, 0.20838747711619315, 0.15862330347650525, 0.14929252091906375, 0.14307199921410277, 0.06842573875457089, 0.10885912983681732, 0.0590949561971294, 0.04354365193472693, 0.034212869377285446, 0.027992347672324454, 0.10812949198453384, 0.32438847595360154, 0.10812949198453384, 0.10812949198453384, 0.10812949198453384, 0.10812949198453384, 0.10812949198453384, 0.15333640160783962, 0.2044485354771195, 0.10222426773855975, 0.051112133869279874, 0.10222426773855975, 0.10222426773855975, 0.051112133869279874, 0.051112133869279874, 0.051112133869279874, 0.14289343002806162, 0.15802332261926813, 0.15129892591206523, 0.1546611242656667, 0.09582265307764132, 0.0739683637792319, 0.09918485143124277, 0.06556286789522828, 0.030259785182413047, 0.028578686005612322, 0.11473347570631665, 0.11473347570631665, 0.11473347570631665, 0.11473347570631665, 0.11473347570631665, 0.11473347570631665, 0.11473347570631665, 0.11473347570631665, 0.10817762813970833, 0.09787499688830753, 0.17514473127381347, 0.1390855218939107, 0.11848025939110912, 0.16484210002241267, 0.07211841875980556, 0.05666447188270436, 0.04121052500560317, 0.030907893754202376, 0.1483436181860532, 0.15452460227713877, 0.123619681821711, 0.1977914909147376, 0.08035279318411215, 0.11743869773062546, 0.06799082500194105, 0.0618098409108555, 0.0247239363643422, 0.0247239363643422, 0.1972226340841721, 0.09861131704208605, 0.09861131704208605, 0.09861131704208605, 0.09861131704208605, 0.09861131704208605, 0.09861131704208605, 0.09861131704208605, 0.09861131704208605, 0.29789338696832723, 0.29789338696832723, 0.15754995766511354, 0.1688035260697645, 0.14754678574986824, 0.13629321734521727, 0.11128528755710401, 0.11128528755710401, 0.06752141042790581, 0.05126625606563219, 0.02750872276692459, 0.021256740319896272, 0.2121455379989928, 0.2121455379989928, 0.2121455379989928, 0.18869009142254722, 0.18869009142254722, 0.37738018284509445, 0.06637998720091368, 0.17701329920243647, 0.11063331200152281, 0.13275997440182735, 0.11063331200152281, 0.08850664960121823, 0.1548866368021319, 0.11063331200152281, 0.04425332480060912, 0.02212666240030456, 0.1608878243766496, 0.1608878243766496, 0.1608878243766496, 0.1608878243766496, 0.1608878243766496, 0.1608878243766496, 0.1583134539377891, 0.16074904553683197, 0.16074904553683197, 0.14369990434353164, 0.11934398835310253, 0.1120372135559738, 0.058454198377029816, 0.03653387398564363, 0.026791507589471998, 0.024355915990429088, 0.1410534093971527, 0.19931460023510708, 0.1410534093971527, 0.1349206524668417, 0.11958876014106425, 0.0950577324198203, 0.05519481237279888, 0.049062055442487894, 0.024531027721243947, 0.03986292004702141, 0.18508093151100066, 0.061693643837000225, 0.12338728767400045, 0.2467745753480009, 0.18508093151100066, 0.061693643837000225, 0.061693643837000225, 0.030846821918500113, 0.030846821918500113, 0.030846821918500113, 0.12641966366118815, 0.2528393273223763, 0.12641966366118815, 0.12641966366118815, 0.06320983183059407, 0.18962949549178224, 0.06320983183059407, 0.06320983183059407, 0.20925587555824263, 0.20925587555824263, 0.20925587555824263, 0.20925587555824263, 0.20925587555824263, 0.22315542407311864, 0.13732641481422686, 0.13732641481422686, 0.13732641481422686, 0.06866320740711343, 0.08582900925889178, 0.051497405555335066, 0.10299481111067013, 0.017165801851778358, 0.034331603703556715, 0.16262119146435894, 0.2106683616697377, 0.11087808508933564, 0.11457402125898017, 0.10718214891969112, 0.09609434041075755, 0.08131059573217947, 0.0517431063750233, 0.02956748935715617, 0.02956748935715617, 0.10485592182662143, 0.13481475663422757, 0.20971184365324286, 0.12882298967270633, 0.11384357226890326, 0.09586827138433959, 0.09586827138433959, 0.047934135692169796, 0.03894648524988796, 0.02696295132684551, 0.13961079284916114, 0.12887150109153336, 0.11276256345509168, 0.11813220933390559, 0.1020232716974639, 0.17719831400085836, 0.06980539642458057, 0.06980539642458057, 0.0536964587881389, 0.03221787527288334, 0.11937024141493531, 0.059685120707467655, 0.059685120707467655, 0.11937024141493531, 0.11937024141493531, 0.23874048282987062, 0.059685120707467655, 0.11937024141493531, 0.059685120707467655, 0.059685120707467655, 0.1823052831435871, 0.09115264157179355, 0.09115264157179355, 0.09115264157179355, 0.09115264157179355, 0.27345792471538066, 0.09115264157179355, 0.09115264157179355, 0.09115264157179355, 0.1508467625681072, 0.08799394483139586, 0.1508467625681072, 0.17808298358734878, 0.1466565747189931, 0.11942035369975153, 0.06913809951038247, 0.03980678456658384, 0.03142640886835567, 0.02304603317012749, 0.3900010061384423, 0.12173075037618333, 0.14607690045142, 0.09738460030094667, 0.14607690045142, 0.048692300150473336, 0.19476920060189334, 0.048692300150473336, 0.07303845022571, 0.07303845022571, 0.048692300150473336, 0.18798728796386463, 0.15038983037109172, 0.11279237277831879, 0.07519491518554586, 0.11279237277831879, 0.15038983037109172, 0.03759745759277293, 0.07519491518554586, 0.07519491518554586, 0.03759745759277293, 0.16137220295258367, 0.1008576268453648, 0.12102915221443776, 0.24205830442887552, 0.12102915221443776, 0.08068610147629184, 0.06051457610721888, 0.06051457610721888, 0.02017152536907296, 0.02017152536907296, 0.12982711812516365, 0.04868516929693637, 0.08114194882822728, 0.2271974567190364, 0.08114194882822728, 0.1135987283595182, 0.09737033859387274, 0.1135987283595182, 0.03245677953129091, 0.06491355906258182, 0.1524963634337197, 0.07624818171685985, 0.10166424228914646, 0.22874454515057954, 0.07624818171685985, 0.10166424228914646, 0.08895621200300315, 0.07624818171685985, 0.038124090858429924, 0.05083212114457323, 0.12388953538121239, 0.20648255896868734, 0.12388953538121239, 0.08259302358747493, 0.08259302358747493, 0.12388953538121239, 0.041296511793737466, 0.12388953538121239, 0.041296511793737466, 0.041296511793737466, 0.13982232704817077, 0.15380455975298785, 0.12933565251955798, 0.1258400943433537, 0.15380455975298785, 0.08738895440510673, 0.08738895440510673, 0.06292004717167685, 0.03495558176204269, 0.03495558176204269, 0.24496371341620551, 0.11431639959422923, 0.1469782280497233, 0.11431639959422923, 0.1388127709358498, 0.07348911402486165, 0.040827285569367586, 0.06532365691098814, 0.03266182845549407, 0.02449637134162055, 0.13698698547441954, 0.09132465698294635, 0.11415582122868294, 0.2739739709488391, 0.09132465698294635, 0.09132465698294635, 0.04566232849147318, 0.06849349273720977, 0.04566232849147318, 0.02283116424573659, 0.22245328061177524, 0.22245328061177524, 0.14830218707451684, 0.03707554676862921, 0.11122664030588762, 0.07415109353725842, 0.07415109353725842, 0.03707554676862921, 0.07415109353725842, 0.21070506932628108, 0.42141013865256216, 0.33794181299330806, 0.14460284596763925, 0.14460284596763925, 0.14460284596763925, 0.14460284596763925, 0.0964018973117595, 0.14460284596763925, 0.0964018973117595, 0.04820094865587975, 0.0964018973117595, 0.04820094865587975, 0.18096773932565102, 0.20107526591739003, 0.10053763295869501, 0.08043010636695601, 0.12064515955043402, 0.12064515955043402, 0.06032257977521701, 0.040215053183478004, 0.08043010636695601, 0.020107526591739002, 0.15007421841874263, 0.18008906210249118, 0.12506184868228554, 0.13006432262957696, 0.12005937473499412, 0.09004453105124559, 0.06002968736749706, 0.08003958315666275, 0.040019791578331375, 0.025012369736457107, 0.1910742461710533, 0.1467671745951569, 0.11353687091323457, 0.1495363665686504, 0.11353687091323457, 0.09138333512528636, 0.06922979933733815, 0.0636914153903511, 0.030461111708428787, 0.030461111708428787, 0.16417853867038396, 0.19179735825979435, 0.14423161341136534, 0.11968155155411167, 0.1120096572237199, 0.08745959536646622, 0.07365018557176103, 0.05063450258058571, 0.03529071391980216, 0.021481304125096967, 0.20932374995621716, 0.16820658478624595, 0.11961357140355267, 0.13082734372263574, 0.11961357140355267, 0.09718602676538654, 0.06354470980813735, 0.04111716516997123, 0.03737924106361021, 0.018689620531805104, 0.18879091530421072, 0.09439545765210536, 0.18879091530421072, 0.09439545765210536, 0.09439545765210536, 0.09439545765210536, 0.09439545765210536, 0.09439545765210536, 0.19473197509516124, 0.19473197509516124, 0.19473197509516124, 0.19473197509516124, 0.19473197509516124, 0.11402707277417358, 0.34208121832252075, 0.11402707277417358, 0.11402707277417358, 0.11402707277417358, 0.11402707277417358, 0.11608398288390963, 0.34825194865172887, 0.11608398288390963, 0.11608398288390963, 0.058041991441954814, 0.058041991441954814, 0.058041991441954814, 0.058041991441954814, 0.058041991441954814, 0.12669914376499902, 0.12669914376499902, 0.12669914376499902, 0.12669914376499902, 0.12669914376499902, 0.12669914376499902, 0.12669914376499902, 0.19586839621950491, 0.07834735848780197, 0.11752103773170296, 0.15669471697560394, 0.039173679243900986, 0.19586839621950491, 0.07834735848780197, 0.039173679243900986, 0.039173679243900986, 0.1705327707430462, 0.0852663853715231, 0.0852663853715231, 0.0852663853715231, 0.0852663853715231, 0.0852663853715231, 0.0852663853715231, 0.1705327707430462, 0.0852663853715231, 0.1826552656905182, 0.1826552656905182, 0.1826552656905182, 0.1826552656905182, 0.1826552656905182, 0.12193904425289138, 0.12193904425289138, 0.0844193383289248, 0.07503941184793315, 0.3189175003537159, 0.06565948536694151, 0.05627955888594987, 0.10317919129090809, 0.01875985296198329, 0.028139779442974935, 0.26048986089989185, 0.26048986089989185, 0.15549595466575444, 0.13110443236524394, 0.17988747696626495, 0.12805549207768013, 0.13110443236524394, 0.08841926833935057, 0.07927244747665914, 0.048783044601021, 0.030489402875638127, 0.027440462588074314, 0.13755793021795654, 0.24072637788142395, 0.12036318894071198, 0.1031684476634674, 0.06877896510897827, 0.08597370638622284, 0.0515842238317337, 0.08597370638622284, 0.017194741277244567, 0.06877896510897827, 0.0720823928994282, 0.1874142215385133, 0.20183070011839893, 0.15377577151878014, 0.0913043643392757, 0.11052633577912323, 0.0720823928994282, 0.05286042145958067, 0.024027464299809398, 0.038443942879695034, 0.14692648427321442, 0.14692648427321442, 0.14692648427321442, 0.07346324213660721, 0.07346324213660721, 0.07346324213660721, 0.07346324213660721, 0.22038972640982163, 0.09788179417994547, 0.11745815301593455, 0.19086949865089367, 0.19086949865089367, 0.1272463324339291, 0.11256406330693729, 0.06851725592596182, 0.039152717671978185, 0.03425862796298091, 0.019576358835989092, 0.3770171520092934, 0.12947905090850362, 0.12947905090850362, 0.12947905090850362, 0.12947905090850362, 0.12947905090850362, 0.12947905090850362, 0.12947905090850362, 0.12947905090850362, 0.18637709145684803, 0.1035428285871378, 0.1035428285871378, 0.08283426286971024, 0.24850278860913072, 0.08283426286971024, 0.06212569715228268, 0.08283426286971024, 0.02070856571742756, 0.04141713143485512, 0.06144677707732776, 0.12289355415465553, 0.1843403312319833, 0.3072338853866388, 0.12289355415465553, 0.06144677707732776, 0.06144677707732776, 0.06144677707732776, 0.06144677707732776, 0.11728945513922584, 0.11728945513922584, 0.11728945513922584, 0.11728945513922584, 0.05864472756961292, 0.11728945513922584, 0.11728945513922584, 0.17593418270883876, 0.05864472756961292, 0.21148549319869597, 0.21148549319869597, 0.21148549319869597, 0.21148549319869597, 0.21148549319869597, 0.21148549319869597, 0.15621712719160682, 0.0923101206132222, 0.071007785087094, 0.0923101206132222, 0.071007785087094, 0.13491479166547862, 0.14911634868289741, 0.17041868420902562, 0.0142015570174188, 0.042604671052256404, 0.11757038069694979, 0.2622723877085803, 0.1266142561351767, 0.14470200701163052, 0.07235100350581526, 0.1266142561351767, 0.05426325262936144, 0.05426325262936144, 0.02713162631468072, 0.018087750876453815, 0.13183052681860052, 0.29002715900092113, 0.13183052681860052, 0.10546442145488041, 0.10546442145488041, 0.10546442145488041, 0.05273221072744021, 0.05273221072744021, 0.026366105363720103, 0.026366105363720103, 0.13231990816555758, 0.13231990816555758, 0.14788695618503495, 0.1245363841558189, 0.11675286014608022, 0.09340228811686418, 0.10896933613634154, 0.06226819207790945, 0.031134096038954725, 0.031134096038954725, 0.13777340677203967, 0.27554681354407934, 0.11021872541763172, 0.11021872541763172, 0.0826640440632238, 0.0826640440632238, 0.0826640440632238, 0.02755468135440793, 0.02755468135440793, 0.02755468135440793, 0.12587445507397943, 0.1573430688424743, 0.14835203633719002, 0.10339687381076881, 0.1573430688424743, 0.0809192925475582, 0.08541480880020032, 0.06293722753698971, 0.04495516252642122, 0.03146861376849486, 0.12545117292090888, 0.2648413650552521, 0.15332921134777752, 0.09757313449404023, 0.09757313449404023, 0.11151215370747455, 0.0696950960671716, 0.041817057640302954, 0.02787803842686864, 0.01393901921343432, 0.17666564310285313, 0.14752491846733096, 0.15845269020565178, 0.14388232788789068, 0.11109901267292825, 0.0983499456448873, 0.06374533514020474, 0.045532382243003384, 0.029140724635522166, 0.025498134056081895, 0.22830230562714376, 0.22830230562714376, 0.22830230562714376, 0.29612885899939634, 0.29612885899939634, 0.15283799691343564, 0.3056759938268713, 0.15283799691343564, 0.15283799691343564, 0.15283799691343564, 0.15283799691343564, 0.35772500114001843, 0.11219703841197988, 0.16829555761796983, 0.16829555761796983, 0.16829555761796983, 0.11219703841197988, 0.11219703841197988, 0.11219703841197988, 0.05609851920598994, 0.05609851920598994, 0.13380926316209385, 0.10035694737157037, 0.1672615789526173, 0.13380926316209385, 0.10035694737157037, 0.10035694737157037, 0.10035694737157037, 0.06690463158104692, 0.03345231579052346, 0.06690463158104692, 0.19970044818054733, 0.09985022409027366, 0.15690749499900147, 0.08558590636309171, 0.11411454181745562, 0.15690749499900147, 0.05705727090872781, 0.05705727090872781, 0.05705727090872781, 0.028528635454363904, 0.13369136711456905, 0.13369136711456905, 0.08912757807637937, 0.13369136711456905, 0.08912757807637937, 0.13369136711456905, 0.08912757807637937, 0.13369136711456905, 0.04456378903818969, 0.04456378903818969, 0.1434859881102583, 0.1434859881102583, 0.09565732540683888, 0.09565732540683888, 0.09565732540683888, 0.1434859881102583, 0.09565732540683888, 0.1434859881102583, 0.04782866270341944, 0.04782866270341944, 0.15846681800342546, 0.15846681800342546, 0.15846681800342546, 0.15846681800342546, 0.15846681800342546, 0.15846681800342546, 0.28831530202849665, 0.28831530202849665, 0.2580791171297555, 0.2580791171297555, 0.2580791171297555, 0.20441823025777608, 0.08760781296761831, 0.05840520864507888, 0.11681041729015776, 0.2920260432253944, 0.05840520864507888, 0.02920260432253944, 0.11681041729015776, 0.02920260432253944, 0.02920260432253944, 0.17783049667501258, 0.1429617718367748, 0.2161860939970741, 0.1150667919661846, 0.09414555706324194, 0.1011193020308895, 0.055789959741180416, 0.04881621477353286, 0.03486872483823776, 0.01743436241911888, 0.12859366737961547, 0.12859366737961547, 0.12859366737961547, 0.12859366737961547, 0.12859366737961547, 0.12859366737961547, 0.12859366737961547, 0.12859366737961547, 0.09332953865999416, 0.2799886159799825, 0.09332953865999416, 0.09332953865999416, 0.09332953865999416, 0.18665907731998832, 0.09332953865999416, 0.3388824773527754, 0.14770420238744944, 0.08862252143246967, 0.2117093567553442, 0.14278072897453445, 0.08369904801955469, 0.11323988849704457, 0.08862252143246967, 0.04923473412914981, 0.03938778730331985, 0.03938778730331985, 0.1252280563723274, 0.1893692559776658, 0.1313367420490263, 0.15577148475582187, 0.10384765650388124, 0.1130106850189296, 0.06414119960533841, 0.05497817109029007, 0.03359777122184393, 0.030543428383494484, 0.15933119817263752, 0.07966559908631876, 0.07966559908631876, 0.07966559908631876, 0.07966559908631876, 0.31866239634527505, 0.07966559908631876, 0.07966559908631876, 0.07966559908631876, 0.07601207005596351, 0.07601207005596351, 0.07601207005596351, 0.22803621016789055, 0.30404828022385405, 0.15202414011192703, 0.07601207005596351, 0.07601207005596351, 0.1835518108911173, 0.09177590544555865, 0.13766385816833798, 0.09177590544555865, 0.045887952722779325, 0.09177590544555865, 0.09177590544555865, 0.1835518108911173, 0.045887952722779325, 0.045887952722779325, 0.1766578917117818, 0.1766578917117818, 0.0883289458558909, 0.0883289458558909, 0.0883289458558909, 0.0883289458558909, 0.0883289458558909, 0.1766578917117818, 0.11879927168042359, 0.11879927168042359, 0.11879927168042359, 0.35639781504127077, 0.11879927168042359, 0.18008104708074085, 0.13506078531055565, 0.11255065442546304, 0.09004052354037043, 0.13506078531055565, 0.09004052354037043, 0.07878545809782413, 0.10129558898291674, 0.03376519632763891, 0.03376519632763891, 0.17010094889162114, 0.17010094889162114, 0.17010094889162114, 0.17010094889162114, 0.17010094889162114, 0.394112072655234, 0.11977915933639464, 0.10432378393815017, 0.08114072084078347, 0.13523453473463912, 0.05795765774341676, 0.042502282345172296, 0.03091075079648894, 0.023183063097366705, 0.011591531548683353, 0.1699773743164971, 0.14670666235650046, 0.09409461792520374, 0.12748303073737283, 0.11938887005563487, 0.15075374269736944, 0.07992983673216232, 0.06070620511303467, 0.031364872641734584, 0.02023540170434489, 0.2297517841149781, 0.27283024363653646, 0.11487589205748905, 0.08615691904311679, 0.057437946028744524, 0.08615691904311679, 0.07179743253593066, 0.04307845952155839, 0.014359486507186131, 0.014359486507186131, 0.36323953357580585, 0.09080988339395146, 0.18161976678790293, 0.09080988339395146, 0.09080988339395146, 0.09080988339395146, 0.09080988339395146, 0.30890345276954123, 0.15445172638477062, 0.07722586319238531, 0.15445172638477062, 0.11583879478857796, 0.07722586319238531, 0.038612931596192654, 0.038612931596192654, 0.038612931596192654, 0.32377207245020007, 0.21584804830013338, 0.10792402415006669, 0.10792402415006669, 0.10792402415006669, 0.21211271510072327, 0.16261974824388784, 0.14847890057050628, 0.12373241714208857, 0.09545072179532547, 0.07423945028525314, 0.06716902644856236, 0.04595775493849004, 0.03181690726510849, 0.03888733110179927, 0.15483053881486927, 0.11529933741532818, 0.17459613951463981, 0.12847640454850853, 0.11200507063203309, 0.13177067133180365, 0.06917960244919691, 0.046119734966131275, 0.036236934616246, 0.03294266783295091, 0.16584549165110635, 0.0995072949906638, 0.3316909833022127, 0.0995072949906638, 0.06633819666044254, 0.06633819666044254, 0.0995072949906638, 0.03316909833022127, 0.03316909833022127, 0.03316909833022127, 0.16031705959271864, 0.08015852979635932, 0.40079264898179656, 0.08015852979635932, 0.053439019864239545, 0.053439019864239545, 0.053439019864239545, 0.053439019864239545, 0.026719509932119773, 0.026719509932119773, 0.1741560162678352, 0.1412075807577042, 0.1412075807577042, 0.12237990332334364, 0.10825914524757323, 0.10825914524757323, 0.05648303230308168, 0.0706037903788521, 0.05177611294449154, 0.0235345967929507, 0.12967742575304322, 0.12626486191743683, 0.18086588328713926, 0.10920204273940483, 0.14674024493107524, 0.09896435123258562, 0.07848896821894723, 0.04777589369848961, 0.03753820219167041, 0.04436332986288321, 0.15392744838284567, 0.11638416828946868, 0.13891013634549487, 0.14266446435483257, 0.16519043241085876, 0.11638416828946868, 0.06006924814940319, 0.06006924814940319, 0.026280296065363896, 0.022525968056026198, 0.23301598524278863, 0.23301598524278863, 0.23301598524278863, 0.23301598524278863, 0.1656001395821672, 0.3312002791643344, 0.3312002791643344, 0.1656001395821672, 0.2069472805650407, 0.07760523021189025, 0.1552104604237805, 0.12934205035315044, 0.12934205035315044, 0.07760523021189025, 0.07760523021189025, 0.05173682014126017, 0.07760523021189025, 0.025868410070630087, 0.23126507884285202, 0.11563253942142601, 0.06937952365285562, 0.25439158672713724, 0.0925060315371408, 0.0462530157685704, 0.11563253942142601, 0.0231265078842852, 0.0231265078842852, 0.0231265078842852, 0.14538731873664948, 0.09912771732044283, 0.09912771732044283, 0.15199583322467902, 0.25112355054512187, 0.11234474629650187, 0.0594766303922657, 0.039651086928177134, 0.02643405795211809, 0.013217028976059045, 0.3218055450800689, 0.07313762388183385, 0.1462752477636677, 0.08776514865820062, 0.1462752477636677, 0.1023926734345674, 0.05851009910546708, 0.02925504955273354, 0.02925504955273354, 0.01462752477636677, 0.14492888195753317, 0.15269292920525818, 0.14492888195753317, 0.13457681896056653, 0.1294007874620832, 0.10352062996966654, 0.07505245672800825, 0.05176031498483327, 0.038820236238624954, 0.023292141743174975, 0.11722943948721967, 0.13854388303035053, 0.14920110480191595, 0.20781582454552577, 0.10124360682987153, 0.11722943948721967, 0.05328610885782712, 0.042628887086261696, 0.04795749797204441, 0.02664305442891356, 0.11509481735310181, 0.12228824343767068, 0.23738306079077248, 0.10790139126853295, 0.12228824343767068, 0.09351453909939522, 0.07193426084568863, 0.06474083476111978, 0.028773704338275453, 0.035967130422844315, 0.11779239795375421, 0.11779239795375421, 0.23558479590750842, 0.11779239795375421, 0.11779239795375421, 0.23558479590750842, 0.11779239795375421, 0.09677583673235453, 0.09677583673235453, 0.29032751019706354, 0.09677583673235453, 0.09677583673235453, 0.09677583673235453, 0.09677583673235453, 0.09677583673235453, 0.2814199502778561, 0.2814199502778561, 0.2814199502778561, 0.16833558868213694, 0.16833558868213694, 0.16833558868213694, 0.16833558868213694, 0.16833558868213694, 0.1929955298952064, 0.13670516700910454, 0.17155158212907237, 0.08845628453530294, 0.13134418006757104, 0.08845628453530294, 0.08041480412300267, 0.04556838900303485, 0.040207402061501335, 0.021443947766134046, 0.3611866513734529, 0.18059332568672645, 0.06019777522890882, 0.12039555045781763, 0.12039555045781763, 0.06019777522890882, 0.06019777522890882, 0.06019777522890882, 0.26250145995645635, 0.19402281822868514, 0.07989174868239976, 0.11413106954628538, 0.07989174868239976, 0.10271796259165684, 0.05706553477314269, 0.05706553477314269, 0.022826213909257077, 0.022826213909257077, 0.16485273010354526, 0.14653576009204022, 0.160273487600669, 0.10074333506327765, 0.12363954757765894, 0.11448106257190642, 0.06868863754314386, 0.041213182525886315, 0.036633940023010056, 0.041213182525886315, 0.24560857043811724, 0.11825597835909347, 0.12735259207902375, 0.11825597835909347, 0.12735259207902375, 0.0818695234793724, 0.06367629603951187, 0.04548306859965134, 0.027289841159790804, 0.03638645487972107, 0.1195718420448176, 0.16880730641621308, 0.11253820427747539, 0.10550456651013318, 0.09847092874279097, 0.16177366864887088, 0.08440365320810654, 0.08440365320810654, 0.03516818883671106, 0.028134551069368848, 0.1176414098856, 0.2352828197712, 0.0392138032952, 0.0784276065904, 0.0784276065904, 0.196069016476, 0.0784276065904, 0.0784276065904, 0.0392138032952, 0.0392138032952, 0.11597434025815882, 0.11597434025815882, 0.11597434025815882, 0.11597434025815882, 0.11597434025815882, 0.23194868051631765, 0.11597434025815882, 0.11597434025815882, 0.11597434025815882, 0.11491008587731283, 0.11491008587731283, 0.11491008587731283, 0.28727521469328204, 0.057455042938656414, 0.057455042938656414, 0.11491008587731283, 0.057455042938656414, 0.057455042938656414, 0.1615509050540685, 0.1615509050540685, 0.1615509050540685, 0.1615509050540685, 0.1615509050540685, 0.11507053263501432, 0.11890621705618147, 0.16493443011018719, 0.17644148337368862, 0.1304132703196829, 0.10739916379268004, 0.07287800400217574, 0.04602821305400573, 0.03835684421167144, 0.02684979094817001, 0.1005692163865555, 0.30170764915966647, 0.1005692163865555, 0.1005692163865555, 0.1005692163865555, 0.1005692163865555, 0.1005692163865555, 0.11797709301187462, 0.11797709301187462, 0.11797709301187462, 0.15730279068249947, 0.07865139534124974, 0.15730279068249947, 0.03932569767062487, 0.03932569767062487, 0.07865139534124974, 0.03932569767062487, 0.20621281596687902, 0.10310640798343951, 0.10310640798343951, 0.051553203991719754, 0.25776601995859877, 0.051553203991719754, 0.051553203991719754, 0.15465961197515926, 0.051553203991719754, 0.051553203991719754, 0.17152482008765482, 0.17152482008765482, 0.17152482008765482, 0.17152482008765482, 0.17152482008765482, 0.17152482008765482, 0.17152482008765482, 0.1606479051382587, 0.1606479051382587, 0.3212958102765174, 0.1606479051382587, 0.18738155516214275, 0.09369077758107137, 0.09369077758107137, 0.09369077758107137, 0.09369077758107137, 0.28107233274321414, 0.09369077758107137, 0.09369077758107137, 0.09369077758107137, 0.18444902955987108, 0.09222451477993554, 0.09222451477993554, 0.09222451477993554, 0.2766735443398066, 0.09222451477993554, 0.09222451477993554, 0.09222451477993554, 0.15280340526761768, 0.15280340526761768, 0.15280340526761768, 0.30560681053523536, 0.15280340526761768, 0.12001907770706734, 0.048007631082826935, 0.0720114466242404, 0.1440228932484808, 0.0720114466242404, 0.1440228932484808, 0.1440228932484808, 0.1440228932484808, 0.024003815541413467, 0.09601526216565387, 0.18336270778415956, 0.1339958249191935, 0.218624766973421, 0.10578617756778436, 0.09168135389207978, 0.0775765302163752, 0.08462894205422748, 0.04936688286496604, 0.03526205918926145, 0.028209647351409163, 0.1423330297284771, 0.1423330297284771, 0.1423330297284771, 0.2846660594569542, 0.1423330297284771, 0.2468289140316766, 0.2468289140316766, 0.2468289140316766, 0.10617676514588814, 0.16238799375253482, 0.11242245721329333, 0.10617676514588814, 0.21235353029177628, 0.15614230168512963, 0.04996553653924148, 0.04996553653924148, 0.02498276826962074, 0.018737076202215555, 0.08695181787614839, 0.21737954469037096, 0.13042772681422257, 0.13042772681422257, 0.13042772681422257, 0.043475908938074195, 0.13042772681422257, 0.043475908938074195, 0.043475908938074195, 0.21231109569913384, 0.21231109569913384, 0.4246221913982677, 0.11623230266269734, 0.3169971890800836, 0.13736544860136957, 0.08453258375468897, 0.07396601078535285, 0.12679887563203346, 0.05283286484668061, 0.031699718908008365, 0.031699718908008365, 0.031699718908008365, 0.1080049308155033, 0.3240147924465099, 0.1080049308155033, 0.1080049308155033, 0.05400246540775165, 0.1080049308155033, 0.05400246540775165, 0.05400246540775165, 0.05400246540775165, 0.10787972484581593, 0.1294556698149791, 0.10787972484581593, 0.1294556698149791, 0.10787972484581593, 0.1510316147841423, 0.08630377987665275, 0.043151889938326374, 0.08630377987665275, 0.043151889938326374, 0.10954426373456896, 0.1482069450526521, 0.16109450549201318, 0.11598804395424948, 0.13531938461329107, 0.09021292307552738, 0.10310048351488843, 0.0708815824164858, 0.025775120878722108, 0.032218901098402634, 0.2858822141824967, 0.16320887600717163, 0.10880591733811441, 0.10560574329875812, 0.11093936669768528, 0.08107107566369309, 0.059736582067984385, 0.03520191443291937, 0.026668116994635886, 0.02346794295527958, 0.3484238234592811, 0.11783969865425646, 0.05891984932712823, 0.11783969865425646, 0.17675954798138468, 0.29459924663564113, 0.05891984932712823, 0.11783969865425646, 0.05891984932712823, 0.05891984932712823, 0.2581034433663674, 0.2860470088357348, 0.1430235044178674, 0.1430235044178674, 0.1430235044178674, 0.1430235044178674, 0.1430235044178674, 0.1067227224278161, 0.167707135243711, 0.1524610320397373, 0.12196882563178982, 0.07623051601986865, 0.18295323844768474, 0.045738309611921185, 0.06098441281589491, 0.045738309611921185, 0.030492206407947454, 0.14023448414722137, 0.13854491204906205, 0.17064678191408864, 0.13685533995090277, 0.14023448414722137, 0.09292646539876113, 0.07265160022084961, 0.055755879239256685, 0.032101869865026576, 0.020274865177911522, 0.18695373211983726, 0.18695373211983726, 0.18695373211983726, 0.18695373211983726, 0.18695373211983726, 0.18695373211983726, 0.18695373211983726, 0.2064359667605548, 0.11796340957745989, 0.058981704788729945, 0.29490852394364975, 0.11796340957745989, 0.029490852394364973, 0.08847255718309492, 0.029490852394364973, 0.029490852394364973, 0.06158390930309706, 0.06158390930309706, 0.24633563721238824, 0.13548460046681352, 0.09853425488495529, 0.2832859827942465, 0.07390069116371648, 0.024633563721238823, 0.012316781860619411, 0.012316781860619411, 0.1395487662282755, 0.0951468860647333, 0.12686251475297775, 0.07611750885178664, 0.22200940081771103, 0.0951468860647333, 0.05708813163883998, 0.10783313754003108, 0.031715628688244436, 0.0507450059011911, 0.33693424622871454, 0.10108027386861436, 0.13477369849148582, 0.06738684924574291, 0.13477369849148582, 0.06738684924574291, 0.06738684924574291, 0.033693424622871455, 0.42802412523124983, 0.061146303604464255, 0.12229260720892851, 0.061146303604464255, 0.12229260720892851, 0.061146303604464255, 0.061146303604464255, 0.061146303604464255, 0.33489853341643483, 0.11163284447214493, 0.14884379262952657, 0.07442189631476329, 0.11163284447214493, 0.11163284447214493, 0.03721094815738164, 0.03721094815738164, 0.03721094815738164, 0.03721094815738164, 0.15560818153749728, 0.1333784413178548, 0.2222974021964247, 0.11114870109821234, 0.08336152582365926, 0.11114870109821234, 0.0666892206589274, 0.050016915494195555, 0.0333446103294637, 0.027787175274553086, 0.12393733335217891, 0.12393733335217891, 0.12393733335217891, 0.12393733335217891, 0.12393733335217891, 0.24787466670435782, 0.12393733335217891, 0.12393733335217891, 0.37093155608501427, 0.06182192601416904, 0.12364385202833808, 0.06182192601416904, 0.1545548150354226, 0.06182192601416904, 0.06182192601416904, 0.03091096300708452, 0.03091096300708452, 0.03091096300708452, 0.16008114972827558, 0.14229435531402274, 0.14970551965329476, 0.14970551965329476, 0.10820299935337147, 0.11116746508908026, 0.06966494478915697, 0.051878150374904126, 0.031126890224942475, 0.026680191621379263, 0.21080316364749244, 0.10540158182374622, 0.10540158182374622, 0.10540158182374622, 0.10540158182374622, 0.10540158182374622, 0.10540158182374622, 0.10540158182374622, 0.1441329488952298, 0.11296906805301793, 0.15192391910578273, 0.1402374637899533, 0.07011873189497665, 0.1830877999479946, 0.06622324678970017, 0.05453679147387073, 0.05064130636859425, 0.02337291063165888, 0.15344397650717487, 0.07672198825358743, 0.30688795301434973, 0.07672198825358743, 0.15344397650717487, 0.07672198825358743, 0.07672198825358743, 0.07672198825358743, 0.13260903346031608, 0.14414025376121314, 0.1787339146639043, 0.14414025376121314, 0.09417163245732592, 0.11146846290867149, 0.08456228220657838, 0.0442030111534387, 0.04035927105313968, 0.02498431065194361, 0.1307132378605477, 0.1307132378605477, 0.2614264757210954, 0.1307132378605477, 0.1307132378605477, 0.1307132378605477, 0.1307132378605477, 0.1691899104159508, 0.09298987442708745, 0.14723396784288847, 0.15240007197772668, 0.12398649923611661, 0.1369017595732121, 0.06328477565176785, 0.06328477565176785, 0.030996624809029153, 0.01937289050564322, 0.28221820133781106, 0.20696001431439479, 0.16933092080268666, 0.05644364026756222, 0.07525818702341629, 0.07525818702341629, 0.05644364026756222, 0.037629093511708143, 0.018814546755854072, 0.018814546755854072, 0.2148689112323589, 0.2148689112323589, 0.2148689112323589, 0.2148689112323589, 0.1709204180312906, 0.273472668850065, 0.1367363344250325, 0.06836816721251625, 0.10255225081877437, 0.10255225081877437, 0.06836816721251625, 0.034184083606258124, 0.034184083606258124, 0.034184083606258124, 0.18891316762938717, 0.16353677197767846, 0.15225837391025235, 0.13252117729225668, 0.11560358019111752, 0.08458798550569575, 0.0676703884045566, 0.03947439323599135, 0.036654793719134826, 0.0225567961348522, 0.13118763483446158, 0.13118763483446158, 0.08745842322297438, 0.13118763483446158, 0.08745842322297438, 0.08745842322297438, 0.08745842322297438, 0.21864605805743598, 0.04372921161148719, 0.04372921161148719, 0.13169469608972129, 0.18741168289691107, 0.12662951547088586, 0.14435764763680986, 0.10636879299554411, 0.11649915423321498, 0.07344511897311379, 0.055716986807189776, 0.03292367402243032, 0.02532590309417717, 0.17634893374511568, 0.05878297791503856, 0.11756595583007712, 0.11756595583007712, 0.05878297791503856, 0.2939148895751928, 0.05878297791503856, 0.05878297791503856, 0.05878297791503856, 0.17650705433390662, 0.08825352716695331, 0.35301410866781324, 0.08825352716695331, 0.044126763583476655, 0.044126763583476655, 0.044126763583476655, 0.044126763583476655, 0.044126763583476655, 0.044126763583476655, 0.11485072690590518, 0.2385361251122646, 0.10601605560545094, 0.14135474080726793, 0.15018941210772216, 0.07067737040363396, 0.07067737040363396, 0.05300802780272547, 0.026504013901362734, 0.026504013901362734, 0.09750624773007005, 0.06500416515338003, 0.06500416515338003, 0.2275145780368301, 0.06500416515338003, 0.16251041288345008, 0.09750624773007005, 0.13000833030676007, 0.03250208257669002, 0.410809026640807, 0.07249571058367182, 0.16915665802856758, 0.048330473722447885, 0.09666094744489577, 0.07249571058367182, 0.048330473722447885, 0.024165236861223943, 0.024165236861223943, 0.024165236861223943, 0.28132362469176186, 0.28132362469176186, 0.34682007592027647, 0.08670501898006912, 0.08670501898006912, 0.08670501898006912, 0.08670501898006912, 0.08670501898006912, 0.08670501898006912, 0.18794271262449633, 0.18794271262449633, 0.18794271262449633, 0.18794271262449633, 0.18794271262449633, 0.18794271262449633, 0.20028758865777221, 0.20028758865777221, 0.2804026241208811, 0.08011503546310889, 0.08011503546310889, 0.04005751773155444, 0.04005751773155444, 0.04005751773155444, 0.04005751773155444, 0.2206846671169633, 0.2206846671169633, 0.2942462228226177, 0.07356155570565442, 0.07356155570565442, 0.03678077785282721, 0.03678077785282721, 0.03678077785282721, 0.03678077785282721, 0.17731908679075709, 0.35463817358151417, 0.17731908679075709, 0.17731908679075709, 0.1682471765838684, 0.1682471765838684, 0.3364943531677368, 0.1682471765838684, 0.2563039000628522, 0.10984452859836523, 0.09153710716530436, 0.15256184527550726, 0.11594700240938552, 0.09153710716530436, 0.08543463335428407, 0.054922264299182615, 0.024409895244081162, 0.024409895244081162, 0.13843848300513048, 0.12261694209025842, 0.13843848300513048, 0.18985849097846466, 0.09097386026051431, 0.13448309777641246, 0.08306308980307829, 0.05537539320205219, 0.027687696601026095, 0.019776926143590067, 0.15143905409111078, 0.19855342647501192, 0.10432468170720965, 0.17163092796992555, 0.10432468170720965, 0.09759405708093806, 0.07740218320212329, 0.03701843544449375, 0.02692249850508636, 0.03028781081822216, 0.12765224008213744, 0.12765224008213744, 0.12765224008213744, 0.12765224008213744, 0.12765224008213744, 0.12765224008213744, 0.12765224008213744, 0.12765224008213744, 0.39430992956118044, 0.19715496478059022, 0.19715496478059022, 0.19715496478059022, 0.20533369110999242, 0.20533369110999242, 0.41066738221998483, 0.20370007628009587, 0.20370007628009587, 0.40740015256019174, 0.17307242763339709, 0.1638006904387508, 0.12362316259528364, 0.12980432072504783, 0.14525721604945827, 0.08962679288158064, 0.06181158129764182, 0.052539844102995545, 0.027815211583938817, 0.037086948778585094, 0.13301888081732752, 0.13301888081732752, 0.13301888081732752, 0.26603776163465503, 0.13301888081732752, 0.13301888081732752, 0.13301888081732752, 0.16809766919959057, 0.18971022666810936, 0.1320767400853926, 0.15849208810247112, 0.07444325350267583, 0.10806278734259395, 0.03842232438847785, 0.06964046295411609, 0.02641534801707852, 0.033619533839918114, 0.3378261214344638, 0.07788918998575006, 0.35606486850628605, 0.11127027140821438, 0.10014324426739295, 0.06676216284492863, 0.11127027140821438, 0.022254054281642878, 0.06676216284492863, 0.011127027140821439, 0.06676216284492863, 0.09831281228847374, 0.3440948430096581, 0.09831281228847374, 0.09831281228847374, 0.09831281228847374, 0.09831281228847374, 0.04915640614423687, 0.04915640614423687, 0.09831281228847374, 0.2698719991083508, 0.17986638688691234, 0.17986638688691234, 0.17986638688691234, 0.17986638688691234, 0.17986638688691234, 0.09843246471133668, 0.29529739413401007, 0.09843246471133668, 0.09843246471133668, 0.09843246471133668, 0.09843246471133668, 0.09843246471133668, 0.09843246471133668, 0.10716070024794778, 0.21432140049589557, 0.10716070024794778, 0.10716070024794778, 0.10716070024794778, 0.21432140049589557, 0.10716070024794778, 0.10716070024794778, 0.14356147211072529, 0.14356147211072529, 0.14356147211072529, 0.14356147211072529, 0.09570764807381685, 0.09570764807381685, 0.09570764807381685, 0.047853824036908424, 0.047853824036908424, 0.047853824036908424, 0.2118082602290303, 0.2118082602290303, 0.2118082602290303, 0.2118082602290303, 0.0817487441307408, 0.1226231161961112, 0.1634974882614816, 0.1226231161961112, 0.1226231161961112, 0.0817487441307408, 0.1634974882614816, 0.0817487441307408, 0.0408743720653704, 0.0408743720653704], \"Term\": [\"abundances\", \"abundances\", \"abundances\", \"abundances\", \"abundances\", \"abundances\", \"abundances\", \"abundances\", \"acceptance\", \"acceptance\", \"acceptance\", \"acceptance\", \"acceptance\", \"acceptance\", \"acceptance\", \"acceptance\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"actions\", \"actions\", \"actions\", \"actions\", \"actions\", \"actions\", \"actions\", \"actions\", \"actions\", \"actions\", \"admm\", \"admm\", \"admm\", \"admm\", \"advance\", \"advance\", \"advance\", \"advance\", \"advance\", \"advance\", \"advance\", \"aggressive\", \"aji\", \"aji\", \"aji\", \"aji\", \"aji\", \"aji\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"algorithms\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"ambiguities\", \"ambiguities\", \"ambiguities\", \"amo\", \"amo\", \"amo\", \"amo\", \"amo\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"animate\", \"apg\", \"apg\", \"apg\", \"apg\", \"apg\", \"apg\", \"apg\", \"apg\", \"apg\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approximation\", \"approximation\", \"approximation\", \"approximation\", \"approximation\", \"approximation\", \"approximation\", \"approximation\", \"approximation\", \"approximation\", \"argyriou\", \"articles\", \"articles\", \"articles\", \"articles\", \"articles\", \"articles\", \"articles\", \"articles\", \"audio\", \"audio\", \"audio\", \"audio\", \"audio\", \"audio\", \"audio\", \"audio\", \"audio\", \"audio\", \"austria\", \"autoregressive\", \"autoregressive\", \"autoregressive\", \"autoregressive\", \"autoregressive\", \"autoregressive\", \"autoregressive\", \"autoregressive\", \"background\", \"background\", \"background\", \"background\", \"background\", \"background\", \"background\", \"background\", \"background\", \"background\", \"bandits\", \"bandits\", \"bandits\", \"bandits\", \"bandits\", \"bandits\", \"bandits\", \"bandits\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"basic\", \"basic\", \"basic\", \"basic\", \"basic\", \"basic\", \"basic\", \"basic\", \"basic\", \"basic\", \"bcd\", \"bcd\", \"bcd\", \"bcd\", \"bcd\", \"bcd\", \"bcd\", \"bcd\", \"bcd\", \"behrman\", \"behrman\", \"behrman\", \"behrman\", \"behrman\", \"berger\", \"berger\", \"berger\", \"berger\", \"bernasconi\", \"beygelzimer\", \"beygelzimer\", \"beygelzimer\", \"beygelzimer\", \"beygelzimer\", \"beygelzimer\", \"beygelzimer\", \"beygelzimer\", \"bichromatic\", \"bichromatic\", \"bichromatic\", \"bichromatic\", \"bichromaticity\", \"bichromaticity\", \"bichromaticity\", \"bichromaticity\", \"bichromaticity\", \"bigram\", \"bigram\", \"bigram\", \"bigram\", \"bigram\", \"bigram\", \"bigram\", \"bigram\", \"binary\", \"binary\", \"binary\", \"binary\", \"binary\", \"binary\", \"binary\", \"binary\", \"binary\", \"binary\", \"bji\", \"bji\", \"bji\", \"bji\", \"bji\", \"bji\", \"blind\", \"blind\", \"blind\", \"blind\", \"blind\", \"blind\", \"blind\", \"blind\", \"blind\", \"blur\", \"blur\", \"blur\", \"blur\", \"blur\", \"blur\", \"blur\", \"blur\", \"blur\", \"blur\", \"bn\", \"bn\", \"bn\", \"bn\", \"bn\", \"bn\", \"bn\", \"bn\", \"bn\", \"bn\", \"boltzmann\", \"boltzmann\", \"boltzmann\", \"boltzmann\", \"boltzmann\", \"boltzmann\", \"boltzmann\", \"boltzmann\", \"boltzmann\", \"botelho\", \"botelho\", \"botelho\", \"botelho\", \"botelho\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"break\", \"break\", \"break\", \"break\", \"break\", \"break\", \"break\", \"break\", \"breakeven\", \"breakeven\", \"breakeven\", \"breakeven\", \"bri\", \"bri\", \"bri\", \"bri\", \"bri\", \"bri\", \"bri\", \"bri\", \"brief\", \"brief\", \"brief\", \"brief\", \"brief\", \"brief\", \"brief\", \"brief\", \"brief\", \"brief\", \"brightness\", \"brightness\", \"brightness\", \"brightness\", \"brightness\", \"brightness\", \"brightness\", \"broom\", \"bsb\", \"bsb\", \"callahan\", \"caltech\", \"caltech\", \"caltech\", \"caltech\", \"caltech\", \"caltech\", \"caltech\", \"caltech\", \"caltech\", \"cascades\", \"cascades\", \"cascades\", \"cascades\", \"cascades\", \"cascades\", \"cascades\", \"cascades\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"caste\", \"caste\", \"caste\", \"caste\", \"caste\", \"caste\", \"casual\", \"casual\", \"categorical\", \"categorical\", \"categorical\", \"categorical\", \"categorical\", \"centralized\", \"centralized\", \"chain\", \"chain\", \"chain\", \"chain\", \"chain\", \"chain\", \"chain\", \"chain\", \"chain\", \"chain\", \"chan\", \"chan\", \"chan\", \"chromatic\", \"chromatic\", \"chromatic\", \"chromatic\", \"chromatic\", \"chromatic\", \"chromatic\", \"chromatic\", \"city\", \"city\", \"city\", \"city\", \"city\", \"city\", \"city\", \"city\", \"city\", \"city\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"claudio\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"coding\", \"coding\", \"coding\", \"coding\", \"coding\", \"coding\", \"coding\", \"coding\", \"coding\", \"coding\", \"coherence\", \"coherence\", \"coherence\", \"coherence\", \"coherence\", \"coherence\", \"coherence\", \"coherence\", \"compatible\", \"compatible\", \"compatible\", \"compatible\", \"compatible\", \"compatible\", \"compatible\", \"competitor\", \"competitor\", \"competitor\", \"competitor\", \"competitor\", \"competitor\", \"competitor\", \"competitor\", \"competitor\", \"composite\", \"composite\", \"composite\", \"composite\", \"composite\", \"composite\", \"composite\", \"composite\", \"composite\", \"composite\", \"conditional\", \"conditional\", \"conditional\", \"conditional\", \"conditional\", \"conditional\", \"conditional\", \"conditional\", \"conditional\", \"conditional\", \"connie\", \"connie\", \"connie\", \"connie\", \"connie\", \"connie\", \"connie\", \"connie\", \"contagion\", \"contagion\", \"contagion\", \"contagion\", \"contextual\", \"contextual\", \"contextual\", \"contextual\", \"contextual\", \"contextual\", \"contextual\", \"contextual\", \"contextual\", \"contextual\", \"contour\", \"contour\", \"contour\", \"contour\", \"contour\", \"contour\", \"contour\", \"contour\", \"contour\", \"contour\", \"contracts\", \"contracts\", \"contracts\", \"contracts\", \"contracts\", \"contracts\", \"contracts\", \"contracts\", \"convex\", \"convex\", \"convex\", \"convex\", \"convex\", \"convex\", \"convex\", \"convex\", \"convex\", \"convex\", \"copied\", \"copied\", \"correctness\", \"correlation\", \"correlation\", \"correlation\", \"correlation\", \"correlation\", \"correlation\", \"correlation\", \"correlation\", \"correlation\", \"correlation\", \"cover\", \"cover\", \"cover\", \"cover\", \"cover\", \"cover\", \"cover\", \"cover\", \"cover\", \"cover\", \"critical\", \"critical\", \"critical\", \"critical\", \"critical\", \"critical\", \"critical\", \"critical\", \"critical\", \"critical\", \"crp\", \"crp\", \"crp\", \"crp\", \"cuprite\", \"cybenko\", \"cyclic\", \"cyclic\", \"cyclic\", \"cyclic\", \"cyclic\", \"cyclic\", \"cyclic\", \"cyclic\", \"cyclic\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"deblurring\", \"deblurring\", \"deblurring\", \"deblurring\", \"deblurring\", \"deblurring\", \"deblurring\", \"deblurring\", \"deblurring\", \"deblurring\", \"declared\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep\", \"define\", \"define\", \"define\", \"define\", \"define\", \"define\", \"define\", \"define\", \"define\", \"define\", \"descend\", \"descend\", \"descended\", \"descended\", \"descends\", \"descends\", \"descends\", \"descends\", \"destination\", \"destination\", \"destination\", \"destination\", \"destination\", \"destination\", \"detection\", \"detection\", \"detection\", \"detection\", \"detection\", \"detection\", \"detection\", \"detection\", \"detection\", \"detection\", \"devavrat\", \"dfc\", \"dfc\", \"dfc\", \"dfc\", \"dfc\", \"dfc\", \"dfc\", \"dfc\", \"dfc\", \"dfc\", \"dhh\", \"dhh\", \"dhh\", \"dhh\", \"dictionary\", \"dictionary\", \"dictionary\", \"dictionary\", \"dictionary\", \"dictionary\", \"dictionary\", \"dictionary\", \"dictionary\", \"dictionary\", \"differences\", \"differences\", \"differences\", \"differences\", \"differences\", \"differences\", \"differences\", \"differences\", \"differences\", \"differences\", \"differencing\", \"differencing\", \"differencing\", \"differencing\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"diffusions\", \"diffusions\", \"diffusions\", \"diffusions\", \"dishes\", \"dishes\", \"dishes\", \"dishes\", \"dishes\", \"dishes\", \"dishes\", \"dishes\", \"dishes\", \"dishes\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"divide\", \"divide\", \"divide\", \"divide\", \"divide\", \"divide\", \"divide\", \"divide\", \"divide\", \"divide\", \"documents\", \"documents\", \"documents\", \"documents\", \"documents\", \"documents\", \"documents\", \"documents\", \"documents\", \"documents\", \"dog\", \"dog\", \"dog\", \"dog\", \"dog\", \"dog\", \"dog\", \"dog\", \"dog\", \"dog\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"dudk\", \"dudk\", \"dwell\", \"dwell\", \"dyer\", \"dyer\", \"dyer\", \"dyer\", \"dyer\", \"dyer\", \"dyer\", \"dyer\", \"dyer\", \"edge\", \"edge\", \"edge\", \"edge\", \"edge\", \"edge\", \"edge\", \"edge\", \"edge\", \"edge\", \"eels\", \"eels\", \"eels\", \"eels\", \"eels\", \"eels\", \"eels\", \"eels\", \"eg\", \"eg\", \"eg\", \"eg\", \"eg\", \"eg\", \"eg\", \"eg\", \"eg\", \"eg\", \"eigenmaps\", \"eigenmaps\", \"eigenmaps\", \"eigenmaps\", \"eigenmaps\", \"eigenmaps\", \"eigenmaps\", \"ekf\", \"ekf\", \"ekf\", \"ekf\", \"ekf\", \"ekf\", \"elizabeth\", \"elizabeth\", \"elizabeth\", \"elizabeth\", \"elizabeth\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"endmember\", \"endmember\", \"endmember\", \"endmember\", \"endmember\", \"endmembers\", \"endmembers\", \"endmembers\", \"endmembers\", \"endmembers\", \"endmembers\", \"endmembers\", \"endmembers\", \"endmembers\", \"ep\", \"ep\", \"ep\", \"ep\", \"ep\", \"ep\", \"ep\", \"ep\", \"ep\", \"ep\", \"equilibrium\", \"equilibrium\", \"equilibrium\", \"equilibrium\", \"equilibrium\", \"equilibrium\", \"equilibrium\", \"equilibrium\", \"equilibrium\", \"equilibrium\", \"ergodic\", \"ergodic\", \"ergodic\", \"ergodic\", \"ergodic\", \"ergodic\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"estimation\", \"estimation\", \"estimation\", \"estimation\", \"estimation\", \"estimation\", \"estimation\", \"estimation\", \"estimation\", \"estimation\", \"et\", \"et\", \"et\", \"et\", \"et\", \"et\", \"et\", \"et\", \"et\", \"et\", \"etal\", \"etal\", \"etal\", \"etal\", \"etal\", \"etal\", \"ethernet\", \"exchangeable\", \"exchangeable\", \"exchangeable\", \"exchangeable\", \"exchangeable\", \"exchangeable\", \"exchangeable\", \"exchangeable\", \"exemplar\", \"exemplar\", \"exemplar\", \"exemplar\", \"exemplar\", \"exposure\", \"exposure\", \"exposure\", \"exposure\", \"exposure\", \"exposure\", \"exposure\", \"exposure\", \"facesync\", \"facesync\", \"facesync\", \"facesync\", \"facesync\", \"facesync\", \"facesync\", \"facesync\", \"facesync\", \"factorial\", \"factorial\", \"factorial\", \"factorial\", \"factorial\", \"factorial\", \"factorial\", \"factorial\", \"factorial\", \"factorial\", \"factorization\", \"factorization\", \"factorization\", \"factorization\", \"factorization\", \"factorization\", \"factorization\", \"factorization\", \"factorization\", \"factorization\", \"false\", \"false\", \"false\", \"false\", \"false\", \"false\", \"false\", \"false\", \"false\", \"false\", \"fault\", \"fault\", \"fault\", \"fault\", \"fault\", \"fault\", \"fault\", \"fault\", \"fault\", \"faults\", \"faults\", \"faults\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"features\", \"features\", \"features\", \"features\", \"features\", \"features\", \"features\", \"features\", \"features\", \"features\", \"ffull\", \"ffull\", \"ffull\", \"ffull\", \"ffull\", \"ffull\", \"fiduciary\", \"fiduciary\", \"fiduciary\", \"fiduciary\", \"fiduciary\", \"field\", \"field\", \"field\", \"field\", \"field\", \"field\", \"field\", \"field\", \"field\", \"field\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"filter\", \"filter\", \"filter\", \"filter\", \"filter\", \"filter\", \"filter\", \"filter\", \"filter\", \"filter\", \"filters\", \"filters\", \"filters\", \"filters\", \"filters\", \"filters\", \"filters\", \"filters\", \"filters\", \"filters\", \"findallnn\", \"findallnn\", \"findallnn\", \"findallnn\", \"findallnn\", \"findallnn\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"fmm\", \"following\", \"following\", \"following\", \"following\", \"following\", \"following\", \"following\", \"following\", \"following\", \"following\", \"forecast\", \"forecast\", \"forecast\", \"forecast\", \"forecast\", \"forecast\", \"forecast\", \"forecast\", \"forecast\", \"forecast\", \"forgeries\", \"forgeries\", \"forgeries\", \"forgeries\", \"forgeries\", \"forgeries\", \"forgeries\", \"forgeries\", \"forgeries\", \"forgeries\", \"forgery\", \"forgery\", \"forgery\", \"fpr\", \"fpr\", \"fpr\", \"fpr\", \"fpr\", \"fukishima\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"functions\", \"functions\", \"functions\", \"functions\", \"functions\", \"functions\", \"functions\", \"functions\", \"functions\", \"functions\", \"gain\", \"gain\", \"gain\", \"gain\", \"gain\", \"gain\", \"gain\", \"gain\", \"gain\", \"gain\", \"garzon\", \"general\", \"general\", \"general\", \"general\", \"general\", \"general\", \"general\", \"general\", \"general\", \"general\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"glas\", \"glas\", \"glas\", \"glas\", \"glas\", \"glas\", \"glas\", \"glas\", \"glas\", \"glas\", \"growth\", \"growth\", \"hamming\", \"hamming\", \"hamming\", \"hamming\", \"hamming\", \"hamming\", \"hamming\", \"hamming\", \"hamming\", \"hamming\", \"handwriting\", \"handwriting\", \"handwriting\", \"harmonium\", \"harmonium\", \"harmonium\", \"harmonium\", \"harmonium\", \"harmonium\", \"harmonium\", \"harmonium\", \"harmonium\", \"hidden\", \"hidden\", \"hidden\", \"hidden\", \"hidden\", \"hidden\", \"hidden\", \"hidden\", \"hidden\", \"hidden\", \"hippocampus\", \"hippocampus\", \"hippocampus\", \"hippocampus\", \"hippocampus\", \"hippocampus\", \"hippocampus\", \"hippocampus\", \"hippocampus\", \"hippocampus\", \"hit\", \"hit\", \"hit\", \"hit\", \"hit\", \"hit\", \"hit\", \"hit\", \"hit\", \"hit\", \"hli\", \"hli\", \"hli\", \"hli\", \"hli\", \"hli\", \"hli\", \"hli\", \"hli\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"hpnn\", \"hpnn\", \"hpnn\", \"hpnn\", \"hpnn\", \"hpnn\", \"hpnn\", \"hpnn\", \"hpnn\", \"hpnn\", \"hxi\", \"hxi\", \"hyperspectral\", \"hyperspectral\", \"hyperspectral\", \"hyperspectral\", \"hyperspectral\", \"hyperspectral\", \"hyperspectral\", \"hyperspectral\", \"ic\", \"ic\", \"ic\", \"ic\", \"ic\", \"ic\", \"ic\", \"ic\", \"ie\", \"ie\", \"ie\", \"ie\", \"ie\", \"ie\", \"ie\", \"ie\", \"ie\", \"ie\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"images\", \"images\", \"images\", \"images\", \"images\", \"images\", \"images\", \"images\", \"images\", \"images\", \"infected\", \"infected\", \"infected\", \"infected\", \"infected\", \"infected\", \"infected\", \"infected\", \"infected\", \"infection\", \"infection\", \"infection\", \"infection\", \"infection\", \"infection\", \"infection\", \"infection\", \"infection\", \"infections\", \"infections\", \"inferences\", \"inferences\", \"inferences\", \"inferences\", \"inferences\", \"inferences\", \"inferencing\", \"inferencing\", \"infinite\", \"infinite\", \"infinite\", \"infinite\", \"infinite\", \"infinite\", \"infinite\", \"infinite\", \"infinite\", \"infinite\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"initials\", \"initials\", \"initials\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"irradiance\", \"isoluminant\", \"isoluminant\", \"isoluminant\", \"judge\", \"judge\", \"judge\", \"judge\", \"judge\", \"judge\", \"judge\", \"kale\", \"kale\", \"kale\", \"kale\", \"kale\", \"kale\", \"kale\", \"karger\", \"karger\", \"kasiviswanathan\", \"kb\", \"kb\", \"kb\", \"kb\", \"kb\", \"kb\", \"kb\", \"kb\", \"kb\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernels\", \"kernels\", \"kernels\", \"kernels\", \"kernels\", \"kernels\", \"kernels\", \"kernels\", \"kernels\", \"kernels\", \"kernelsum\", \"kernelsum\", \"kernelsum\", \"kernelsum\", \"kernelsum\", \"kernelsum\", \"kernelsum\", \"kerr\", \"kerr\", \"kerr\", \"kerr\", \"kerr\", \"kerr\", \"kerr\", \"kerr\", \"kerr\", \"kh\", \"kh\", \"kh\", \"kh\", \"kh\", \"kh\", \"kh\", \"kh\", \"kh\", \"kh\", \"klt\", \"klt\", \"klt\", \"klt\", \"klt\", \"klt\", \"klt\", \"klt\", \"krauthgamer\", \"ktr\", \"ktr\", \"ktr\", \"ktr\", \"ktr\", \"kwktr\", \"kwktr\", \"kwktr\", \"kwktr\", \"kwktr\", \"kwktr\", \"kxri\", \"kxri\", \"kxri\", \"lagrangian\", \"lagrangian\", \"lagrangian\", \"lagrangian\", \"lagrangian\", \"lagrangian\", \"lagrangian\", \"lagrangian\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language\", \"latent\", \"latent\", \"latent\", \"latent\", \"latent\", \"latent\", \"latent\", \"latent\", \"latent\", \"latent\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"layers\", \"layers\", \"layers\", \"layers\", \"layers\", \"layers\", \"layers\", \"layers\", \"layers\", \"layers\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"leisch\", \"leisch\", \"lens\", \"lens\", \"lens\", \"lens\", \"leskovec\", \"leskovec\", \"leskovec\", \"leskovec\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"lim\", \"lim\", \"lim\", \"lim\", \"lim\", \"lim\", \"lim\", \"lim\", \"lim\", \"lincoln\", \"lincoln\", \"lincoln\", \"lincoln\", \"lincoln\", \"lincoln\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"link\", \"link\", \"link\", \"link\", \"link\", \"link\", \"link\", \"link\", \"link\", \"linucb\", \"linucb\", \"linucb\", \"linucb\", \"linucb\", \"linucb\", \"lippman\", \"llm\", \"llm\", \"lmin\", \"lmin\", \"lmin\", \"lmin\", \"lmin\", \"lmin\", \"lmin\", \"lmin\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"lof\", \"lof\", \"lof\", \"lof\", \"lof\", \"lof\", \"lof\", \"lof\", \"lof\", \"lof\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"logn\", \"logn\", \"logn\", \"logn\", \"logn\", \"logn\", \"logn\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"losses\", \"losses\", \"losses\", \"losses\", \"losses\", \"losses\", \"losses\", \"losses\", \"losses\", \"losses\", \"lstm\", \"lstm\", \"lstm\", \"lstm\", \"lstm\", \"lstm\", \"lstm\", \"lstm\", \"lstm\", \"lstm\", \"lwe\", \"lwe\", \"lwe\", \"lwe\", \"macbeth\", \"macbeth\", \"macbeth\", \"macbeth\", \"macbeth\", \"macbeth\", \"macbeth\", \"macbeth\", \"macbeth\", \"majority\", \"majority\", \"majority\", \"majority\", \"majority\", \"majority\", \"majority\", \"majority\", \"majority\", \"majority\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"man\", \"mandelbrot\", \"mandelbrot\", \"manners\", \"margin\", \"margin\", \"margin\", \"margin\", \"margin\", \"margin\", \"margin\", \"margin\", \"margin\", \"margin\", \"marginal\", \"marginal\", \"marginal\", \"marginal\", \"marginal\", \"marginal\", \"marginal\", \"marginal\", \"marginal\", \"marginal\", \"matrices\", \"matrices\", \"matrices\", \"matrices\", \"matrices\", \"matrices\", \"matrices\", \"matrices\", \"matrices\", \"matrices\", \"matricization\", \"matricization\", \"matricization\", \"matricization\", \"matricization\", \"matricization\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"max\", \"max\", \"max\", \"max\", \"max\", \"max\", \"max\", \"max\", \"max\", \"max\", \"maxim\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"melville\", \"membrane\", \"membrane\", \"membrane\", \"membrane\", \"membrane\", \"membrane\", \"membrane\", \"membrane\", \"membrane\", \"membrane\", \"memory\", \"memory\", \"memory\", \"memory\", \"memory\", \"memory\", \"memory\", \"memory\", \"memory\", \"memory\", \"metabolic\", \"metabolic\", \"metabolic\", \"metabolic\", \"metabolic\", \"metabolic\", \"metabolic\", \"metabolic\", \"metabolic\", \"metabolic\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"methods\", \"micchelli\", \"min\", \"min\", \"min\", \"min\", \"min\", \"min\", \"min\", \"min\", \"min\", \"min\", \"mixability\", \"mixability\", \"mixability\", \"mixability\", \"mixability\", \"mixability\", \"mixability\", \"mixability\", \"mixability\", \"mixability\", \"mle\", \"mle\", \"mle\", \"mle\", \"mle\", \"mle\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"models\", \"models\", \"models\", \"models\", \"models\", \"models\", \"models\", \"models\", \"models\", \"models\", \"modulated\", \"modulated\", \"modulated\", \"monochromatic\", \"monochromatic\", \"monochromatic\", \"monochromatic\", \"monochromatic\", \"monochromatic\", \"monthly\", \"monthly\", \"monthly\", \"monthly\", \"monthly\", \"monthly\", \"monthly\", \"morton\", \"morton\", \"morton\", \"morton\", \"morton\", \"morton\", \"morton\", \"morton\", \"morton\", \"morton\", \"mpot\", \"mpot\", \"mpot\", \"mpot\", \"mpot\", \"mpot\", \"mpot\", \"mpot\", \"mpot\", \"mpot\", \"mse\", \"mse\", \"mse\", \"mse\", \"mse\", \"mse\", \"mse\", \"mse\", \"mse\", \"mslr\", \"mslr\", \"mslr\", \"mslr\", \"mslr\", \"mslr\", \"mslr\", \"mslr\", \"mstd\", \"mstd\", \"mstd\", \"mstd\", \"mstd\", \"multiagent\", \"multiagent\", \"multiagent\", \"multipliers\", \"multipliers\", \"multipliers\", \"multipliers\", \"multipliers\", \"multipliers\", \"multipliers\", \"multipliers\", \"multipole\", \"muscovite\", \"muscovite\", \"muscovite\", \"muscovite\", \"muscovite\", \"muscovite\", \"muscovite\", \"muscovite\", \"musical\", \"musical\", \"musical\", \"musical\", \"musical\", \"musical\", \"nadaraya\", \"nadaraya\", \"nadaraya\", \"nadaraya\", \"nearest\", \"nearest\", \"nearest\", \"nearest\", \"nearest\", \"nearest\", \"nearest\", \"nearest\", \"nearest\", \"nearest\", \"neighbor\", \"neighbor\", \"neighbor\", \"neighbor\", \"neighbor\", \"neighbor\", \"neighbor\", \"neighbor\", \"neighbor\", \"neighbor\", \"net\", \"net\", \"net\", \"net\", \"net\", \"net\", \"net\", \"net\", \"net\", \"net\", \"netinf\", \"netinf\", \"netinf\", \"netinf\", \"netinf\", \"netinf\", \"netinf\", \"netinf\", \"netinf\", \"nets\", \"nets\", \"nets\", \"nets\", \"nets\", \"nets\", \"nets\", \"nets\", \"nets\", \"nets\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"networks\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neutral\", \"neutral\", \"neutral\", \"neutral\", \"neutral\", \"neutral\", \"neutral\", \"neutral\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"news\", \"news\", \"news\", \"news\", \"news\", \"news\", \"news\", \"news\", \"ni\", \"ni\", \"ni\", \"ni\", \"ni\", \"ni\", \"ni\", \"ni\", \"ni\", \"ni\", \"nlp\", \"nlp\", \"nodes\", \"nodes\", \"nodes\", \"nodes\", \"nodes\", \"nodes\", \"nodes\", \"nodes\", \"nodes\", \"nodes\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"nonlocal\", \"nonlocal\", \"nonlocal\", \"nonlocal\", \"nonlocal\", \"nonlocal\", \"nonlocal\", \"noun\", \"noun\", \"noun\", \"noun\", \"noun\", \"noun\", \"noun\", \"noun\", \"noun\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"nys\", \"nys\", \"nys\", \"nys\", \"nys\", \"nys\", \"nys\", \"nys\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"objective\", \"objective\", \"objective\", \"objective\", \"objective\", \"objective\", \"objective\", \"objective\", \"objective\", \"objective\", \"oblivious\", \"oblivious\", \"oblivious\", \"oblivious\", \"oblivious\", \"oblivious\", \"oblivious\", \"oblivious\", \"oblivious\", \"ocain\", \"ocain\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"oonference\", \"oonference\", \"oonference\", \"opposition\", \"opposition\", \"opposition\", \"optical\", \"optical\", \"optical\", \"optical\", \"optical\", \"optical\", \"optical\", \"optical\", \"optical\", \"optical\", \"optically\", \"optically\", \"optically\", \"optically\", \"optically\", \"optically\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimizers\", \"optimizers\", \"optimizers\", \"optimizers\", \"optimizers\", \"optimizers\", \"optimizers\", \"optimizers\", \"optimizers\", \"optimizers\", \"options\", \"options\", \"options\", \"options\", \"options\", \"options\", \"options\", \"options\", \"optscale\", \"optscale\", \"optscale\", \"optscale\", \"optscale\", \"oracle\", \"oracle\", \"oracle\", \"oracle\", \"oracle\", \"oracle\", \"oracle\", \"oracle\", \"oracle\", \"oracle\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"pdp\", \"pdp\", \"pdp\", \"pdp\", \"pdp\", \"pdp\", \"pdp\", \"pdp\", \"pdp\", \"pdp\", \"pds\", \"pds\", \"pds\", \"pds\", \"pds\", \"pds\", \"pds\", \"pds\", \"pds\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performmusic\", \"pf\", \"pf\", \"pf\", \"pf\", \"pf\", \"pf\", \"pf\", \"pf\", \"pf\", \"pf\", \"phrase\", \"phrase\", \"phrase\", \"phrase\", \"phrase\", \"phrase\", \"phrase\", \"phrase\", \"phrase\", \"phrase\", \"pk\", \"pk\", \"pk\", \"pk\", \"pk\", \"pk\", \"pk\", \"pk\", \"pk\", \"pk\", \"player\", \"player\", \"player\", \"player\", \"player\", \"player\", \"player\", \"player\", \"player\", \"player\", \"players\", \"players\", \"players\", \"players\", \"players\", \"players\", \"players\", \"players\", \"players\", \"players\", \"pmin\", \"pmin\", \"pmin\", \"pmin\", \"pmin\", \"pmin\", \"pmin\", \"pmin\", \"pmin\", \"pmin\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"pooling\", \"pooling\", \"pooling\", \"pooling\", \"pooling\", \"pooling\", \"pooling\", \"pooling\", \"pooling\", \"pooling\", \"pos\", \"pos\", \"pos\", \"pos\", \"pos\", \"pos\", \"pos\", \"pos\", \"pos\", \"positivity\", \"positivity\", \"posts\", \"predicate\", \"predicate\", \"predicate\", \"predicate\", \"predicate\", \"predicate\", \"predicate\", \"predicate\", \"predicate\", \"predicate\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price\", \"prior\", \"prior\", \"prior\", \"prior\", \"prior\", \"prior\", \"prior\", \"prior\", \"prior\", \"prior\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"proj\", \"proj\", \"proj\", \"proj\", \"proj\", \"proj\", \"proj\", \"proj\", \"projens\", \"projens\", \"projens\", \"projens\", \"projens\", \"prox\", \"prox\", \"prox\", \"prox\", \"prox\", \"prox\", \"psca\", \"psca\", \"psca\", \"psca\", \"psca\", \"psca\", \"psca\", \"psca\", \"psca\", \"pti\", \"pti\", \"pti\", \"pti\", \"pti\", \"pti\", \"pti\", \"pursuit\", \"pursuit\", \"pursuit\", \"pursuit\", \"pursuit\", \"pursuit\", \"pursuit\", \"pursuit\", \"pursuit\", \"qt\", \"qt\", \"qt\", \"qt\", \"qt\", \"qt\", \"qt\", \"qt\", \"qt\", \"qtk\", \"qtk\", \"qtk\", \"qtk\", \"qtk\", \"query\", \"query\", \"query\", \"query\", \"query\", \"query\", \"query\", \"query\", \"query\", \"query\", \"radar\", \"radar\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"random\", \"randomized\", \"randomized\", \"randomized\", \"randomized\", \"randomized\", \"randomized\", \"randomized\", \"randomized\", \"randomized\", \"randomized\", \"rank\", \"rank\", \"rank\", \"rank\", \"rank\", \"rank\", \"rank\", \"rank\", \"rank\", \"rank\", \"rankings\", \"rankings\", \"rankings\", \"rankings\", \"rankings\", \"rankings\", \"rankings\", \"rankings\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"recognition\", \"redefined\", \"reedy\", \"reedy\", \"reedy\", \"reedy\", \"reedy\", \"reedy\", \"reedy\", \"reedy\", \"reference\", \"reference\", \"reference\", \"reference\", \"reference\", \"reference\", \"reference\", \"reference\", \"reference\", \"reference\", \"refinement\", \"refinement\", \"refinement\", \"refinement\", \"refinement\", \"refinement\", \"refinement\", \"refinement\", \"refinement\", \"reg\", \"reg\", \"reg\", \"reg\", \"reg\", \"reg\", \"reg\", \"reg\", \"reg\", \"regk\", \"regk\", \"regk\", \"regk\", \"regk\", \"regk\", \"regret\", \"regret\", \"regret\", \"regret\", \"regret\", \"regret\", \"regret\", \"regret\", \"regret\", \"regret\", \"regularization\", \"regularization\", \"regularization\", \"regularization\", \"regularization\", \"regularization\", \"regularization\", \"regularization\", \"regularization\", \"regularization\", \"regularizer\", \"regularizer\", \"regularizer\", \"regularizer\", \"regularizer\", \"regularizer\", \"regularizer\", \"regularizer\", \"regularizer\", \"regularizer\", \"related\", \"related\", \"related\", \"related\", \"related\", \"related\", \"related\", \"related\", \"related\", \"related\", \"replay\", \"replay\", \"replay\", \"replay\", \"replay\", \"replay\", \"replay\", \"replay\", \"replay\", \"replay\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"responses\", \"responses\", \"responses\", \"responses\", \"responses\", \"responses\", \"responses\", \"responses\", \"responses\", \"responses\", \"results\", \"results\", \"results\", \"results\", \"results\", \"results\", \"results\", \"results\", \"results\", \"results\", \"retraining\", \"retraining\", \"retraining\", \"reyzin\", \"reyzin\", \"richtarik\", \"richtarik\", \"richtarik\", \"richtarik\", \"richtarik\", \"richtarik\", \"rir\", \"rmf\", \"rmf\", \"rmf\", \"rmf\", \"rmf\", \"rmf\", \"rmf\", \"rmf\", \"rmf\", \"roj\", \"roj\", \"roj\", \"roj\", \"roj\", \"roj\", \"roj\", \"roj\", \"roj\", \"roj\", \"role\", \"role\", \"role\", \"role\", \"role\", \"role\", \"role\", \"role\", \"role\", \"role\", \"round\", \"round\", \"round\", \"round\", \"round\", \"round\", \"round\", \"round\", \"round\", \"round\", \"rounds\", \"rounds\", \"rounds\", \"rounds\", \"rounds\", \"rounds\", \"rounds\", \"rounds\", \"rounds\", \"rounds\", \"rtj\", \"rtj\", \"rtj\", \"rtj\", \"rtj\", \"rtj\", \"ruhl\", \"ruhl\", \"rumor\", \"rumor\", \"rumor\", \"runtime\", \"runtime\", \"runtime\", \"runtime\", \"runtime\", \"runtime\", \"runtime\", \"runtime\", \"runtime\", \"runtime\", \"scale\", \"scale\", \"scale\", \"scale\", \"scale\", \"scale\", \"scale\", \"scale\", \"scale\", \"scale\", \"schapire\", \"schapire\", \"schapire\", \"schapire\", \"schapire\", \"schapire\", \"schapire\", \"schapire\", \"scholes\", \"scholes\", \"scholes\", \"scholes\", \"scholes\", \"scholes\", \"scholes\", \"schulmans\", \"search\", \"search\", \"search\", \"search\", \"search\", \"search\", \"search\", \"search\", \"search\", \"search\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"selector\", \"selector\", \"selector\", \"selector\", \"selector\", \"selector\", \"selector\", \"selector\", \"selector\", \"semg\", \"semg\", \"semg\", \"semg\", \"semg\", \"semg\", \"semg\", \"semg\", \"semibandit\", \"semibandit\", \"semibandit\", \"semibandit\", \"semibandit\", \"semibandit\", \"semibandit\", \"semibandit\", \"semibandit\", \"semibandit\", \"semibandits\", \"semibandits\", \"semibandits\", \"semibandits\", \"semibandits\", \"semibandits\", \"semibandits\", \"semibandits\", \"sentiment\", \"sentiment\", \"sentiment\", \"sentiment\", \"sentiment\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequencing\", \"sequencing\", \"sequencing\", \"sequencing\", \"sequencing\", \"series\", \"series\", \"series\", \"series\", \"series\", \"series\", \"series\", \"series\", \"series\", \"series\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"sh\", \"sh\", \"sh\", \"sh\", \"sh\", \"sh\", \"sh\", \"sh\", \"sh\", \"sh\", \"shifts\", \"shifts\", \"shifts\", \"shifts\", \"shifts\", \"shifts\", \"shifts\", \"shortcut\", \"shortcut\", \"shortcut\", \"shortcut\", \"shortcut\", \"shortcut\", \"shortcut\", \"shortcut\", \"shortcut\", \"shortcuts\", \"shortcuts\", \"shortcuts\", \"shortcuts\", \"shortcuts\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"shown\", \"shown\", \"shown\", \"shown\", \"shown\", \"shown\", \"shown\", \"shown\", \"shown\", \"shown\", \"signature\", \"signature\", \"signature\", \"signature\", \"signature\", \"signature\", \"signature\", \"signature\", \"signature\", \"signature\", \"signatures\", \"signatures\", \"signatures\", \"signatures\", \"signatures\", \"signatures\", \"signatures\", \"signatures\", \"signatures\", \"signatures\", \"simple\", \"simple\", \"simple\", \"simple\", \"simple\", \"simple\", \"simple\", \"simple\", \"simple\", \"simple\", \"since\", \"since\", \"since\", \"since\", \"since\", \"since\", \"since\", \"since\", \"since\", \"since\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"skinner\", \"skinner\", \"skinner\", \"skinner\", \"sll\", \"sll\", \"sll\", \"sll\", \"snr\", \"snr\", \"snr\", \"snr\", \"snr\", \"snr\", \"snr\", \"snr\", \"snr\", \"snr\", \"social\", \"social\", \"social\", \"social\", \"social\", \"social\", \"social\", \"social\", \"social\", \"social\", \"source\", \"source\", \"source\", \"source\", \"source\", \"source\", \"source\", \"source\", \"source\", \"source\", \"sources\", \"sources\", \"sources\", \"sources\", \"sources\", \"sources\", \"sources\", \"sources\", \"sources\", \"sources\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"sparse\", \"sparse\", \"sparse\", \"sparse\", \"sparse\", \"sparse\", \"sparse\", \"sparse\", \"sparse\", \"sparse\", \"spectral\", \"spectral\", \"spectral\", \"spectral\", \"spectral\", \"spectral\", \"spectral\", \"spectral\", \"spectral\", \"spectral\", \"specular\", \"specular\", \"specular\", \"specular\", \"specular\", \"specular\", \"specular\", \"spence\", \"spence\", \"spence\", \"spence\", \"spence\", \"spence\", \"spence\", \"spence\", \"spheres\", \"spheres\", \"spheres\", \"spiral\", \"spiral\", \"spiral\", \"spiral\", \"spiral\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"stationarity\", \"stationarity\", \"stationarity\", \"stationarity\", \"stationarity\", \"stationarity\", \"stationarity\", \"stationarity\", \"stationary\", \"stationary\", \"stationary\", \"stationary\", \"stationary\", \"stationary\", \"stationary\", \"stationary\", \"stationary\", \"stationary\", \"step\", \"step\", \"step\", \"step\", \"step\", \"step\", \"step\", \"step\", \"step\", \"step\", \"steps\", \"steps\", \"steps\", \"steps\", \"steps\", \"steps\", \"steps\", \"steps\", \"steps\", \"steps\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastically\", \"stochastically\", \"stochastically\", \"stochastically\", \"stochastically\", \"stochastically\", \"stochastically\", \"stochastically\", \"stochastically\", \"stochastically\", \"stores\", \"stores\", \"stores\", \"stores\", \"stores\", \"stores\", \"stores\", \"stores\", \"stores\", \"strategic\", \"strategic\", \"strategic\", \"strategic\", \"strategic\", \"strategic\", \"strategic\", \"strategic\", \"strategic\", \"string\", \"string\", \"string\", \"string\", \"string\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"subgradient\", \"subgradient\", \"subgradient\", \"subgradient\", \"subgradient\", \"subgradient\", \"subgradient\", \"subjective\", \"subjective\", \"subjective\", \"subjective\", \"subjective\", \"subjective\", \"subjective\", \"subjective\", \"subjective\", \"subjective\", \"subroutine\", \"subroutine\", \"subroutine\", \"subroutine\", \"subroutine\", \"subroutine\", \"subroutine\", \"subroutine\", \"subroutine\", \"subroutine\", \"subroutines\", \"subroutines\", \"subroutines\", \"subroutines\", \"subroutines\", \"subroutines\", \"subroutines\", \"subtree\", \"subtree\", \"subtree\", \"subtree\", \"sumida\", \"sumida\", \"sumida\", \"sumida\", \"sumida\", \"sumida\", \"sumida\", \"sumida\", \"sumida\", \"summations\", \"summations\", \"summations\", \"summations\", \"summations\", \"summations\", \"summations\", \"summations\", \"supplemental\", \"supplemental\", \"supplemental\", \"supplemental\", \"supplemental\", \"swap\", \"swap\", \"swap\", \"swap\", \"swap\", \"swap\", \"swap\", \"swap\", \"swap\", \"swap\", \"synaptic\", \"synaptic\", \"synaptic\", \"synaptic\", \"synaptic\", \"synaptic\", \"synaptic\", \"synaptic\", \"synaptic\", \"synaptic\", \"synergy\", \"synergy\", \"synergy\", \"synergy\", \"synergy\", \"t_p\", \"t_p\", \"t_p\", \"target\", \"target\", \"target\", \"target\", \"target\", \"target\", \"target\", \"target\", \"target\", \"target\", \"tau\", \"tau\", \"tau\", \"tau\", \"tau\", \"tau\", \"tau\", \"tau\", \"tau\", \"tca\", \"tca\", \"tca\", \"tensor\", \"tensor\", \"tensor\", \"tensor\", \"tensor\", \"tensor\", \"tensor\", \"tensor\", \"tensor\", \"tensor\", \"tensors\", \"tensors\", \"tensors\", \"tensors\", \"tensors\", \"tensors\", \"tensors\", \"tensors\", \"tensors\", \"theories\", \"theories\", \"theories\", \"theories\", \"theories\", \"theories\", \"theories\", \"theories\", \"theories\", \"theories\", \"three\", \"three\", \"three\", \"three\", \"three\", \"three\", \"three\", \"three\", \"three\", \"three\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"tolat\", \"tolerance\", \"tolerance\", \"tolerance\", \"tolerance\", \"tolerance\", \"tolerance\", \"tolerance\", \"tolerance\", \"tolerance\", \"tomioka\", \"tpr\", \"tpr\", \"tpr\", \"tpr\", \"tpr\", \"tpr\", \"tr\", \"tr\", \"tr\", \"tr\", \"tr\", \"tr\", \"tr\", \"tr\", \"tr\", \"tr\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"transducer\", \"transducer\", \"transducer\", \"transducer\", \"transducer\", \"transducer\", \"transducer\", \"transmission\", \"transmission\", \"transmission\", \"transmission\", \"transmission\", \"transmission\", \"transmission\", \"transmission\", \"transmission\", \"transparent\", \"transparent\", \"transparent\", \"transparent\", \"transparent\", \"transparent\", \"transparent\", \"transparent\", \"transparent\", \"transparent\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"trend\", \"trend\", \"trend\", \"trend\", \"trend\", \"trend\", \"trend\", \"trend\", \"trending\", \"trending\", \"trending\", \"trending\", \"trending\", \"trending\", \"trending\", \"trending\", \"trends\", \"trends\", \"trends\", \"trends\", \"trends\", \"trends\", \"trends\", \"trends\", \"trends\", \"trends\", \"true\", \"true\", \"true\", \"true\", \"true\", \"true\", \"true\", \"true\", \"true\", \"true\", \"tsybakov\", \"tsybakov\", \"tsybakov\", \"tsybakov\", \"tsybakov\", \"tsybakov\", \"tsybakov\", \"tsybakov\", \"twitter\", \"twitter\", \"twitter\", \"twitter\", \"twitter\", \"twitter\", \"twitter\", \"twitter\", \"twitter\", \"twitter\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"two\", \"ucb\", \"ucb\", \"ucb\", \"ucb\", \"ucb\", \"ucb\", \"ucb\", \"ucb\", \"units\", \"units\", \"units\", \"units\", \"units\", \"units\", \"units\", \"units\", \"units\", \"units\", \"unmixing\", \"unmixing\", \"unmixing\", \"unmixing\", \"unmixing\", \"unmixing\", \"unmixing\", \"unmixing\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"used\", \"usgs\", \"usgs\", \"usgs\", \"usgs\", \"usgs\", \"usgs\", \"usgs\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"using\", \"ut\", \"ut\", \"ut\", \"ut\", \"ut\", \"ut\", \"ut\", \"ut\", \"ut\", \"ut\", \"uxlx\", \"uxlx\", \"uxlx\", \"uxlx\", \"valuation\", \"valuation\", \"valuation\", \"valuation\", \"valuation\", \"valuation\", \"valuation\", \"valuation\", \"valuation\", \"valuation\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"vcee\", \"vcee\", \"vcee\", \"vcee\", \"vcee\", \"vcee\", \"vcee\", \"vcee\", \"vcee\", \"vcee\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"verb\", \"verb\", \"verb\", \"verb\", \"verb\", \"verb\", \"verb\", \"verb\", \"verb\", \"verification\", \"verification\", \"verification\", \"verification\", \"verification\", \"verification\", \"verification\", \"verification\", \"verification\", \"verification\", \"vision\", \"vision\", \"vision\", \"vision\", \"vision\", \"vision\", \"vision\", \"vision\", \"vision\", \"vision\", \"vocabularies\", \"vocabularies\", \"vocabularies\", \"vocabularies\", \"vocabularies\", \"vocabularies\", \"vocabularies\", \"vocabularies\", \"vocabularies\", \"voting\", \"voting\", \"voting\", \"voting\", \"voting\", \"voting\", \"voting\", \"voting\", \"voting\", \"voting\", \"vovk\", \"vovk\", \"wafer\", \"wafer\", \"wafer\", \"wafer\", \"wafer\", \"wafer\", \"wafer\", \"wave\", \"wave\", \"wave\", \"wave\", \"wave\", \"wave\", \"wavelet\", \"wavelet\", \"wavelet\", \"wavelet\", \"wavelet\", \"wavelet\", \"wavelet\", \"wavelet\", \"wavelet\", \"wavelets\", \"wavelets\", \"wavelets\", \"wavelets\", \"wavelets\", \"wavelets\", \"wavelets\", \"wavelets\", \"wavelets\", \"waxi\", \"waxi\", \"waxi\", \"waxi\", \"weibull\", \"weibull\", \"weibull\", \"weibull\", \"weighted\", \"weighted\", \"weighted\", \"weighted\", \"weighted\", \"weighted\", \"weighted\", \"weighted\", \"weighted\", \"weighted\", \"weights\", \"weights\", \"weights\", \"weights\", \"weights\", \"weights\", \"weights\", \"weights\", \"weights\", \"weights\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"wichita\", \"wichita\", \"wichita\", \"wichita\", \"wichita\", \"wichita\", \"wichita\", \"wichita\", \"wien\", \"wien\", \"wien\", \"wien\", \"wilkinson\", \"wilkinson\", \"wilkinson\", \"wjc\", \"wjc\", \"wjc\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"xdx\", \"xdx\", \"xdx\", \"xdx\", \"xdx\", \"xdx\", \"xdx\", \"xi\", \"xi\", \"xi\", \"xi\", \"xi\", \"xi\", \"xi\", \"xi\", \"xi\", \"xi\", \"xiaoyue\", \"xr\", \"xr\", \"xr\", \"xr\", \"xr\", \"xr\", \"xr\", \"xr\", \"xr\", \"xr\", \"xri\", \"xri\", \"xri\", \"xri\", \"xri\", \"xri\", \"xri\", \"xri\", \"xri\", \"xrj\", \"xtk\", \"xtk\", \"xtk\", \"xtk\", \"xtk\", \"yeir\", \"yeir\", \"yeir\", \"yeir\", \"yeir\", \"yeir\", \"yeir\", \"yeir\", \"yf\", \"yf\", \"yf\", \"yf\", \"yf\", \"yf\", \"yf\", \"yf\", \"ys\", \"ys\", \"ys\", \"ys\", \"ys\", \"ys\", \"ys\", \"ys\", \"ys\", \"ys\", \"ziehe\", \"ziehe\", \"ziehe\", \"ziehe\", \"zik\", \"zik\", \"zik\", \"zik\", \"zik\", \"zik\", \"zik\", \"zik\", \"zik\", \"zik\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [5, 1, 3, 10, 2, 8, 9, 4, 7, 6]};\n",
              "\n",
              "function LDAvis_load_lib(url, callback){\n",
              "  var s = document.createElement('script');\n",
              "  s.src = url;\n",
              "  s.async = true;\n",
              "  s.onreadystatechange = s.onload = callback;\n",
              "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
              "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "}\n",
              "\n",
              "if(typeof(LDAvis) !== \"undefined\"){\n",
              "   // already loaded: just create the visualization\n",
              "   !function(LDAvis){\n",
              "       new LDAvis(\"#\" + \"ldavis_el19311351084006676003312548861\", ldavis_el19311351084006676003312548861_data);\n",
              "   }(LDAvis);\n",
              "}else if(typeof define === \"function\" && define.amd){\n",
              "   // require.js is available: use it to load d3/LDAvis\n",
              "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
              "   require([\"d3\"], function(d3){\n",
              "      window.d3 = d3;\n",
              "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "        new LDAvis(\"#\" + \"ldavis_el19311351084006676003312548861\", ldavis_el19311351084006676003312548861_data);\n",
              "      });\n",
              "    });\n",
              "}else{\n",
              "    // require.js not available: dynamically load d3 & LDAvis\n",
              "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
              "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "                 new LDAvis(\"#\" + \"ldavis_el19311351084006676003312548861\", ldavis_el19311351084006676003312548861_data);\n",
              "            })\n",
              "         });\n",
              "}\n",
              "</script>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "import pyLDAvis.gensim_models as gensimvis\n",
        "import pickle\n",
        "import pyLDAvis\n",
        "\n",
        "# Visualize the topics\n",
        "pyLDAvis.enable_notebook()\n",
        "\n",
        "LDAvis_data_filepath = os.path.join('./results/ldavis_prepared_'+str(num_topics))\n",
        "\n",
        "# # this is a bit time consuming - make the if statement True\n",
        "# # if you want to execute visualization prep yourself\n",
        "if 1 == 1:\n",
        "    LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2word)\n",
        "    with open(LDAvis_data_filepath, 'wb') as f:\n",
        "        pickle.dump(LDAvis_prepared, f)\n",
        "\n",
        "# load the pre-prepared pyLDAvis data from disk\n",
        "with open(LDAvis_data_filepath, 'rb') as f:\n",
        "    LDAvis_prepared = pickle.load(f)\n",
        "\n",
        "pyLDAvis.save_html(LDAvis_prepared, './results/ldavis_prepared_'+ str(num_topics) +'.html')\n",
        "\n",
        "LDAvis_prepared"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gOQZcX3waID"
      },
      "source": [
        "** **\n",
        "#### Closing Notes\n",
        "Machine learning has become increasingly popular over the past decade, and recent advances in computational availability have led to exponential growth to people looking for ways how new methods can be incorporated to advance the field of Natural Language Processing.\n",
        "\n",
        "Often, we treat topic models as black-box algorithms, but hopefully, this article addressed to shed light on the underlying math, and intuitions behind it, and high-level code to get you started with any textual data.\n",
        "\n",
        "In the next article, we’ll go one step deeper into understanding how you can evaluate the performance of topic models, tune its hyper-parameters to get more intuitive and reliable results.\n",
        "\n",
        "** **\n",
        "#### References:\n",
        "1. Topic model — Wikipedia. https://en.wikipedia.org/wiki/Topic_model\n",
        "2. Distributed Strategies for Topic Modeling. https://www.ideals.illinois.edu/bitstream/handle/2142/46405/ParallelTopicModels.pdf?sequence=2&isAllowed=y\n",
        "3. Topic Mapping — Software — Resources — Amaral Lab. https://amaral.northwestern.edu/resources/software/topic-mapping\n",
        "4. A Survey of Topic Modeling in Text Mining. https://thesai.org/Downloads/Volume6No1/Paper_21-A_Survey_of_Topic_Modeling_in_Text_Mining.pdf\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}